% This file was created with JabRef 2.9b2.
% Encoding: UTF-8

@INPROCEEDINGS{Bengio2004,
  author = {Bengio, Yoshua and Grandvalet, Yves},
  title = {No Unbiased Estimator of the Variance of K-Fold Cross-Validation},
  year = {2004},
  address = {Cambridge, MA},
  publisher = {MIT Press},
  abstract = {Most machine learning researchers perform quantitative experiments
	to estimate generalization error and compare algorithm performances.
	In order to draw statistically convincing conclusions, it is important
	to estimate the uncertainty of such estimates. This paper studies
	the estimation of uncertainty around the K-fold cross-validation
	estimator. The main theorem shows that there exists no universal
	unbiased estimator of the variance of K-fold cross-validation. An
	analysis based on the eigendecomposition of the covariance matrix
	of errors helps to better understand the nature of the problem and
	shows that naive estimators may grossly underestimate variance, as
	confirmed by numerical experiments.},
  cat = {C},
  crossref = {NIPS16},
  keywords = {cross validation, error bars, generalization error inference, k-fold
	cross-validation, model selection, statistical comparison of algorithms,
	variance estimate},
  topics = {Comparative},
  url = {http://www.iro.umontreal.ca/~lisa/pointeurs/var-kfold-part1-nips.pdf}
}

@INPROCEEDINGS{Chen2011,
  author = {Jixu Chen and Qiang Ji},
  title = {A hierarchical framework for simultaneous facial activity tracking},
  booktitle = {FG},
  year = {2011},
  pages = {679-686},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/fgr/2011},
  ee = {http://dx.doi.org/10.1109/FG.2011.5771330}
}

@INPROCEEDINGS{Lee07,
  author = {Cheongjae Lee and Gary Geunbae Lee},
  title = {Emotion Recognition for Affective User Interfaces using Natural Language
	Dialogs},
  booktitle = {Proceedings of the 16th IEEE International Symposium on Robot and
	Human interactive Communication},
  year = {2007},
  pages = {798-801},
  abstract = {In a real world, emotion plays a significant role in rational actions
	in human communication. Given the potential and importance of emotions,
	in recent years, there has been growing interest in the study of
	emotions to improve the capabilities of current human-robot interaction.
	The emotion recognition from text modality is a necessary step to
	develop affective conversational interfaces. In this paper, we present
	an effective hybrid approach to improve the performance of emotion
	recognition from text by combining linguistic, pragmatic, and keyword
	spotting features.},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/ro-man/2007},
  ee = {http://dx.doi.org/10.1109/ROMAN.2007.4415194},
  file = {:Lee07.pdf:PDF},
  keywords = {emotion feature-generation labels-emotion audio-features word-spotting-features},
  review = {Multimodal recognition. Uses key words to map to emotions. Language
	word features to assist emotion recognition, along with the usual
	audio features.}
}

@INPROCEEDINGS{Monte1996,
  author = {Enric Monte and Javier Hernando Pericas and Xavier Mir{\'o} and A.
	Adolf},
  title = {Text independent speaker identification on noisy environments by
	means of self organizing maps},
  booktitle = {Proceedings of the International Conference on Spoken Language Processing
	(Interspeech)},
  year = {1996},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/interspeech/1996},
  ee = {http://www.isca-speech.org/archive/icslp_1996/i96_1804.html},
  file = {Monte1996.pdf:Monte1996.pdf:PDF},
  keywords = {cluster, occupancy, TI-database, codebook, audio},
  review = {Paper uses cluster occupancy time, which I dabbled with for multiple
	instance learning.}
}

@INPROCEEDINGS{Taheri2011,
  author = {Sima Taheri and Pavan K. Turaga and Rama Chellappa},
  title = {Towards view-invariant expression analysis using analytic shape manifolds},
  booktitle = {Proceedings of the IEEE International Conference on Automatic Face
	and Gesture Recognition},
  year = {2011},
  pages = {306-313},
  abstract = {Facial expression analysis is one of the important components for
	effective human-computer interaction. However, to develop robust
	and generalizable models for expression analysis one needs to break
	the dependence of the models on the choice of the coordinate frame
	of the camera i.e. expression models should generalize across facial
	poses. To perform this systematically, one needs to understand the
	space of observed images subject to projective transformations. However,
	since the projective shape-space is cumbersome to work with, we address
	this problem by deriving models for expressions on the affine shape-space
	as an approximation to the projective shape-space by using a Riemannian
	interpretation of deformations that facial expressions cause on different
	parts of the face. We use landmark configurations to represent facial
	deformations and exploit the fact that the affine shape-space can
	be studied using the Grassmann manifold. This representation enables
	us to perform various expression analysis and recognition algorithms
	without the need for the normalization as a preprocessing step. We
	extend some of the available approaches for expression analysis to
	the Grassmann manifold and experimentally show promising results,
	paving the way for a more general theory of view-invariant expression
	analysis.},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/fgr/2011},
  ee = {http://dx.doi.org/10.1109/FG.2011.5771415},
  file = {:Taheri2011.pdf:PDF},
  keywords = {face alignment tracker-features corpus-cohn-kanade corpus-Bosphorus
	labels-facs classification-svm feature-generation multi-angle}
}

@INPROCEEDINGS{Tariq2011,
  author = {Usman Tariq and Kai-Hsiang Lin and Zhen Li and Xi Zhou and Zhaowen
	Wang and Vuong Le and Thomas S. Huang and Xutao Lv and Tony X. Han},
  title = {Emotion recognition from an ensemble of features},
  booktitle = {Proceedings of the IEEE International Conference on Automatic Face
	and Gesture Recognition},
  year = {2011},
  pages = {872-877},
  abstract = {This work details the authors' efforts to push the baseline of expression
	recognition performance on a realistic database. Both subject-dependent
	and subject-independent emotion recognition scenarios are addressed
	in this work. These two happen frequently in real life settings.
	The approach towards solving this problem involves face detection,
	followed by key point identification, then feature generation and
	then finally classification. An ensemble of features comprising of
	Hierarchial Gaussianization (HG), Scale Invariant Feature Transform
	(SIFT) and Optic Flow have been incorporated. In the classification
	stage we used SVMs. The classification task has been divided into
	person specific and person independent emotion recognition. Both
	manual labels and automatic algorithms for person verification have
	been attempted. They both give similar performance.},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/fgr/2011},
  ee = {http://dx.doi.org/10.1109/FG.2011.5771365},
  file = {:Tariq2011.pdf:PDF},
  keywords = {fera corpus posed-data situation-acted labels-emotion detection alignment
	tracker-features aam-features texture-features optical-flow-features
	classification-svm}
}

@INPROCEEDINGS{Wang05,
  author = {Bin Wang and Wenkai Lu},
  title = {An In-depth Comparasion on {FastICA}, {CuBICA} and {IC-FastICA}},
  booktitle = {Proceedings of the Advances in Natural Computation},
  year = {2005},
  series = {Lecture Notes in Computer Science},
  pages = {410-414},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/icnc/2005-2},
  ee = {http://dx.doi.org/10.1007/11539117_60},
  keywords = {ica comparison dimensional-reduction}
}

@ARTICLE{Aaron1997,
  author = {Aaron and Ben-Ze'ev},
  title = {The affective realm},
  journal = {New Ideas in Psychology},
  year = {1997},
  volume = {15},
  pages = {247 - 259},
  number = {3},
  abstract = {A major difficulty in understanding emotions is the confusion concerning
	the differences between emotions and related affective phenomena
	such as feelings, moods, affective disorders, sentiments, and affective
	traits. In light of such confusion, in many disputes concerning affective
	phenomena each side is actually referring to a different set of phenomena.
	This paper clarifies the differences between the various phenomena
	and thereby supplies a novel systematic description of the affective
	realm. The suggested classification is not arbitrary: it is done
	in light of the two basic characteristics of affective phenomena:
	the intentional nature of the evaluative stand, and the occurrent
	or dispositional nature of the feeling dimension. These characteristics
	express the two basic mental dimensions of intentionality and feeling.
	The suggested classification has significant implications for understanding
	emotions and other related phenomena.},
  doi = {10.1016/S0732-118X(97)10011-3},
  issn = {0732-118X},
  keywords = {emotion-definition},
  url = {http://www.sciencedirect.com/science/article/pii/S0732118X97100113}
}

@INPROCEEDINGS{Abrilian2006,
  author = {Abrilian, S. and Devillers, L. and Martin, J.},
  title = {Annotation of Emotions in Real-Life Video Interviews: Variability
	between Coders},
  booktitle = {Proceedings of the 5th International Conference on Language Resources
	and Evaluation (LREC)},
  year = {2006},
  address = {Genoa, Italy},
  file = {Abrilian2006.pdf:Abrilian2006.pdf:PDF},
  keywords = {annotation human-performance perception gender-difference natural-data
	labels-emotion corpus-emotv annotation}
}

@INPROCEEDINGS{Afzal2009,
  author = {Afzal, S. and Robinson, P.},
  title = {Natural Affect Data - Collection \& Annotation in a Learning Context},
  booktitle = {Proceedings of the International Conference on Affective Computing
	\& Intelligent Interaction (ACII)},
  year = {2009},
  address = {Amsterdam},
  month = {Sept},
  abstract = {Automatic inference of affect relies on representative data. For viable
	applications of such technology the use of naturalistic over posed
	data has been increasingly emphasised. Creating a repository of naturalistic
	data is however a massively challenging task. We report results from
	a data collection exercise in one of the most significant application
	areas of affective computing, namely computer-based learning environments.
	The conceptual and methodological issues encountered during the process
	are discussed, and problems with labelling and annotation are identified.
	A comparison of the compiled database with some standard databases
	is also presented.},
  file = {Afzal2009.pdf:Afzal2009.pdf:PDF},
  keywords = {situation-learning natural-important annotation context-important
	labels-emotion corpus personal-differences},
  owner = {ts00051},
  timestamp = {2009.10.26}
}

@INPROCEEDINGS{Afzal2009b,
  author = {Afzal, S. and Sezgin, T.M. and Gao, G. and Robinson, P.},
  title = {Perception of the Emotional Expressions in Different Representations
	Using Facial Feature Points},
  booktitle = {Proceedings of the International Conference on Affective Computing
	\& Intelligent Interaction (ACII)},
  year = {2009},
  address = {Amsterdam},
  month = {Sept},
  abstract = {Facial expression recognition is an enabling technology for affective
	computing. Many existing facial expression analysis systems rely
	on automatically tracked facial feature points. Although psychologists
	have studied emotion perception from manually specified or marker-based
	point-light displays, no formal study exists on the amount of emotional
	information conveyed through automatically tracked feature points.
	We assess the utility of automatically extracted feature points in
	conveying emotions for posed and naturalistic data and present results
	from an experiment that compared human raters’ judgements of emotional
	expressions between actual video clips and three automatically generated
	representations of them. The implications for optimal face representation
	and creation of realistic animations are discussed.},
  file = {Afzal2009b.pdf:Afzal2009b.pdf:PDF},
  keywords = {tracker-features human perception human-performance naturalistic natural-data
	natural-important labels-emotion},
  owner = {ts00051},
  timestamp = {2009.10.26}
}

@INPROCEEDINGS{Aggarwal2005,
  author = {Aggarwal, Gaurav and Veeraraghavan, Ashok and Chellappa, Rama},
  title = {3{D} Facial Pose Tracking in Uncalibrated Videos},
  booktitle = {Proceedings of the International Conference on Pattern Recognition
	and Machine Intelligence},
  year = {2005},
  volume = {3776},
  pages = {515-520},
  abstract = {This paper presents a method to recover the 3D configuration of a
	face in each frame of a video. The 3D configuration consists of the
	three translational parameters and the three orientation parameters
	which correspond to the yaw, pitch and roll of the face. Such information
	is important for applications like face modeling, recognition, expression
	analysis, etc. which require head stabilization. The approach combines
	the structural advantages of geometric modeling with the statistical
	advantages of a particle-filter based inference. The face is modeled
	as the curved surface of a cylinder which is free to translate and
	rotate arbitrarily. The geometric modeling takes care of pose and
	self-occlusion while the statistical modeling handles moderate occlusion
	and illumination variations. Experimental results on multiple datasets
	are provided to show the efficacy of the approach. The insensitivity
	of our approach to calibration parameters (focal length) is also
	shown.},
  file = {Aggarwal2005.pdf:Aggarwal2005.pdf:PDF},
  keywords = {head-model head-pose estimation},
  optabstract = {"This paper presents a method to recover the 3D configuration of a
	face in each frame of a video. The 3D configuration consists of the
	3 translational parameters and the 3 orientation parameters which
	correspond to the yaw, pitch and roll of the face, which is important
	for applications like face modeling, recognition, expression analysis,
	etc. The approach combines the structural advantages of geometric
	modeling with the statistical advantages of a particle-filter based
	inference. The face is modeled as the curved surface of a cylinder
	which is free to translate and rotate arbitrarily. The geometric
	modeling takes care of pose and self-occlusion while the statistical
	modeling handles moderate occlusion and illumination variations.
	Experimental results on multiple datasets are provided to show the
	efficacy of the approach. The insensitivity of our approach to calibration
	parameters (focal length) is also shown"},
  optkey = {2005},
  owner = {ts00051},
  timestamp = {2009.10.26}
}

@ARTICLE{Aggarwal1999,
  author = {J. K. Aggarwal and Q. Cai},
  title = {Human Motion Analysis: A Review},
  journal = {Computer Vision and Image Understanding: CVIU},
  year = {1999},
  volume = {73},
  pages = {428-440},
  number = {3},
  file = {Aggarwal1999.pdf:Aggarwal1999.pdf:PDF},
  keywords = {survey-paper motion human behaviour},
  review = {Splits human motion models into body structure analysis, tracking
	and recognition (by templates, etc)},
  url = {citeseer.ist.psu.edu/aggarwal99human.html}
}

@TECHREPORT{Ahlberg2001,
  author = {Ahlberg, J.},
  title = {CANDIDE-3 -- an updated parameterized face},
  institution = {Dept. of Electrical Engineering, Linköping University},
  year = {2001},
  address = {Sweden},
  note = {Report No. LiTH-ISY-R-2326},
  owner = {tim},
  timestamp = {2012.10.25}
}

@ARTICLE{Akakin2011,
  author = {Akak{\i}n, H. \c{C}. and Sankur, B.},
  title = {Robust classification of face and head gestures in video},
  journal = {Image and Vision Computing},
  year = {2011},
  volume = {29},
  pages = {470--483},
  month = {June},
  acmid = {1994154},
  address = {Newton, MA, USA},
  doi = {http://dx.doi.org/10.1016/j.imavis.2011.03.001},
  file = {Akakin2011.pdf:Akakin2011.pdf:PDF},
  issn = {0262-8856},
  issue = {7},
  issue_date = {June, 2011},
  keywords = {Face and head gesture classification, Facial landmark tracking, Fusion
	of classifiers, Time series analysis, Twotalk corpus classification-hmms
	classification-crf label-emotions texture-feature tracker-features
	corpus-BUHMAP corpus-twotalk classification-nn temporal feature-generation},
  numpages = {14},
  publisher = {Butterworth-Heinemann},
  url = {http://dx.doi.org/10.1016/j.imavis.2011.03.001}
}

@ARTICLE{Alsius05,
  author = {Alsius, Agnes and Navarra, Jordi and Campbell, Ruth and Soto-Faraco,
	Salvador},
  title = {Audiovisual Integration of Speech Falters under High Attention Demands},
  journal = {Current Biology},
  year = {2005},
  volume = {15},
  pages = {839--843},
  number = {9},
  month = {May},
  abstract = {One of the most commonly cited examples of human multisensory integration
	occurs during exposure to natural speech, when the vocal and the
	visual aspects of the signal are integrated in a unitary percept.
	Audiovisual association of facial gestures and vocal sounds has been
	demonstrated in nonhuman primates [1] and in prelinguistic children
	[2], arguing for a general basis for this capacity. One critical
	question, however, concerns the role of attention in such multisensory
	integration. Although both behavioral and neurophysiological studies
	have converged on a preattentive conceptualization of audiovisual
	speech integration [3, 4, 5, 6, 7 and 8], this mechanism has rarely
	been measured under conditions of high attentional load, when the
	observers' attention resources are depleted [9]. We tested the extent
	to which audiovisual integration was modulated by the amount of available
	attentional resources by measuring the observers' susceptibility
	to the classic McGurk illusion [3] in a dual-task paradigm [10].
	The proportion of visually influenced responses was severely, and
	selectively, reduced if participants were concurrently performing
	an unrelated visual or auditory task. In contrast with the assumption
	that crossmodal speech integration is automatic, our results suggest
	that these multisensory binding processes are subject to attentional
	demands.},
  citeulike-article-id = {582359},
  doi = {10.1016/j.cub.2005.03.046},
  file = {Alsius05.pdf:Alsius05.pdf:PDF},
  keywords = {attention, audiovisual, speech, mcgurk, perception},
  posted-at = {2006-04-11 21:46:07},
  priority = {2},
  review = {Human subjects experence less McGurk effect when given attentional
	diverting second tasks. This may indicate fusion of sensory data
	(from spoken works and lip movements) requires attentional resources.},
  url = {http://dx.doi.org/10.1016/j.cub.2005.03.046}
}

@INPROCEEDINGS{Amar2001,
  author = {Robert A. Amar and Daniel R. Dooly and Sally A. Goldman and Qi Zhang},
  title = {Multiple-Instance Learning of Real-Valued Data},
  booktitle = {Proceedings of the 18th International Conference on Machine Learning},
  year = {2001},
  pages = {3--10},
  publisher = {Morgan Kaufmann, San Francisco, CA},
  abstract = {The multiple-instance learning model has received much attention recently
	with a primary application area being that of drug activity prediction.
	Most prior work on multiple-instance learning has been for concept
	learning, yet for drug activity prediction, the label is a real-valued
	affinity measurement giving the binding strength. We present extensions
	of k-nearest neighbors (k-NN), Citation-kNN, and the diverse density
	algorithm for the real-valued setting and study their performance
	on Boolean and real-valued data. We also provide a method for generating
	chemically realistic artificial data.},
  file = {Amar2001.pdf:Amar2001.pdf:PDF},
  keywords = {mil machine learning method supervised bag instance classification-method},
  url = {citeseer.ist.psu.edu/amar01multipleinstance.html}
}

@ARTICLE{Andersen1991,
  author = {Andersen, Peter A.},
  title = {When one cannot not communicate: A challenge to motley's traditional
	communication postulates},
  journal = {Communication Studies},
  year = {1991},
  volume = {42},
  pages = {309-325},
  number = {4},
  doi = {10.1080/10510979109368346},
  owner = {tim},
  timestamp = {2013.03.30}
}

@INPROCEEDINGS{Andrews2002,
  author = {Stuart Andrews and Ioannis Tsochantaridis and Thomas Hofmann},
  title = {Support Vector Machines for Multiple-Instance Learning},
  booktitle = {Proceedings of the Advances in Neural Information Processing Systems},
  year = {2002},
  pages = {561-568},
  publisher = {MIT Press},
  file = {Andrews2003.pdf:Andrews2003.pdf:PDF},
  keywords = {mil supervised machine learning classification method classification-method}
}

@INPROCEEDINGS{Anisetti2006,
  author = {Marco Anisetti and Valerio Bellandi and E. Damiani and Fabrizio Beverina},
  title = {3{D} Expressive face model-based tracking algorithm},
  booktitle = {Proceedings of the 24th IASTED international conference on Signal
	processing, pattern recognition, and applications},
  year = {2006},
  pages = {111-116},
  address = {Anaheim, CA, USA},
  publisher = {ACTA Press},
  abstract = {This paper presents a method for tracking a face on a video sequence,
	by recovering the full-motion and the expression deformation of the
	face using 3D expressive facial model. From some characteristic face
	points given on the first frame, an approximated 3D model of the
	face is reconstructed. Using a steepest descent image approach, the
	algorithm is able to extract simultaneously the parameters related
	to the face expression and to the 3D posture. The algorithm has been
	tested on the Kanade-Cohn database [1] and its precision has been
	compared with a standard multicamera system for the 3D tracking (ELITE2002
	System). The results in both cases are good. The proposed approach
	is part of a facial expression analysis system. Our aim is to detect
	the facial expressions in situations characterized by a moderate
	head motion in realistic experimental conditions (illumination from
	the ceiling, and subjects not in frontal pose).},
  isbn = {0-88986-580-9},
  keywords = {face tracking head-model},
  location = {Innsbruck, Austria}
}

@INPROCEEDINGS{Aran2007,
  author = {Aran, O. and Ar{i}, \.{I}. and G\"{u}vensan, M. A. and Haberdar,
	H. and Kurt, Z., T\"{u}rkmen and H. \.{I}., Uyar, A. and Akarun,
	L.},
  title = {A Database of Non-Manual Signs in {Turkish} Sign Language},
  booktitle = {Proceedings of the IEEE 15th Signal Processing and Communications
	Applications Conference},
  year = {2007},
  address = {Eski\c{s}ehir},
  abstract = {Sign languages are visual languages. The message is not only transferred
	via hand gestures (manual signs) but also head/body motion and facial
	expressions (non-manual signs). In this article, we present a database
	of non-manual signs in Turkish sign language (TSL). There are eight
	non-manual signs in the database, which are frequently used in TSL.
	The database contains the videos of these signs as well as a ground
	truth data of 60 manually landmarked points of the face.},
  keywords = {corpus-BUHMAP corpus posed-data corpus-announce},
  owner = {ts00051},
  review = {BUHMAP was used by Akakin before they moved to Twotalk for emotion/NVC
	recognition.},
  timestamp = {2012.01.30}
}

@INPROCEEDINGS{Aran2010,
  author = {Oya Aran and Hayley Hung and Daniel Gatica-Perez},
  title = {A Multimodal Corpus for Studying Dominance in Small Group Conversations},
  booktitle = {LREC workshop on Multimodal Corpora: Advances in Capturing, Coding
	and Analyzing Multimodality},
  year = {2010},
  address = {Malta},
  month = {May},
  owner = {tim},
  timestamp = {2012.10.31}
}

@ARTICLE{Archer1977,
  author = {Archer, D. and Akert, R. M.},
  title = {Words and everything else: Verbal and nonverbal cues in social interpretation},
  journal = {Journal of Personality and Social Psychology},
  year = {1977},
  volume = {35},
  pages = {443-449},
  abstract = {Examined the relative contributions of verbal cues and full-channel
	(verbal-plus-nonverbal) cues, using a new method for studying the
	process of interpretation. This method, the Social Interpretations
	Task (SIT), is a videotape of 20 unposed sequences of spontaneous
	behavior, which are paired with multiple-choice questions requiring
	interpretation. For each SIT question, there is an unambiguous criterion
	of accuracy (e.g., in one scene, 2 men discuss a game of basketball
	they have just played, and the viewer is asked to decide which man
	won the game). The SIT was presented to undergraduates in 2 communication
	conditions: (a) a verbal transcript version containing only a written
	record of what was actually said in each SIT scene (76 Ss), and (b)
	a full-channel version, containing all the verbal and nonverbal behaviors
	that occurred in each scene (370 Ss). The 2 conditions produced radically
	different levels of interpretation accuracy. Ss in the verbal transcript
	version actually did worse than chance; Ss in the full-channel version
	did strikingly and significantly better.},
  keywords = {NVC perception NVC-important},
  review = {Interpretation of either video or text transcript of video in various
	acted social situations. Accuracy of interpretation was much greater
	with video which contains non verbal information.}
}

@INBOOK{Argyle1975,
  chapter = {Non-verbal Communication in Human Social Interaction},
  title = {Non-Verbal Communication},
  publisher = {Cambridge University Press},
  year = {1975},
  editor = {Hind, Robert A.},
  author = {Argyle, Michael},
  keywords = {NVC research-methods naturalistic bi-directional mutual natural-data
	natural-important},
  review = {Overviews research approaches to NVC. It bashes some lab based studies,
	saying: "Social psycologists have developed a tradition of experimentation
	which consiss of very well-controlled studies, conducted under very
	artifical conditions. Subjects may sit in cubicles by themselves,
	watch flashing lights and press buttons; often there is no verbal
	communication, no NVC, no real motivation, and there are no situational
	rules."}
}

@BOOK{Argyle1994,
  title = {The Psychology of Interpersonal Behaviour},
  publisher = {Penguin},
  year = {1994},
  author = {Argyle, Michael},
  edition = {5th edition},
  abstract = {The first edition of this book outlined what amounted to a breakthrough
	in the analysis of social behaviour. Since then it has become widely
	used as an introductory textbook of social psychology. It is invaluable
	to anyone interested in the subject or whose work involves dealing
	with people, as well as anyone who wants to know how to make friends
	and influence people. For this new, fifth edition, Michael Argyle
	includes the latest research on non-verbal communication, social
	skills and happiness.},
  keywords = {nvc, nvc-definition, basic-emotions, gaze},
  owner = {ts00051},
  review = {It has the NVC sender, receiver diagram on page 24.},
  timestamp = {2010.03.03}
}

@BOOK{Argyle1976,
  title = {Gaze and Mutual Gaze},
  publisher = {Cambridge University Press},
  year = {1976},
  author = {Argyle, M. and Cook, M.},
  address = {New York},
  keywords = {gaze mutual},
  review = {Review at https://www.sciencemag.org/content/194/4260/54.extract}
}

@INPROCEEDINGS{Ashraf2007,
  author = {Ahmed Bilal Ashraf and Simon Lucey and Jeffrey F. Cohn and Tsuhan
	Chen and Zara Ambadar and Ken Prkachin and Patty Solomon and Barry
	J. Theobald},
  title = {The painful face: pain expression recognition using active appearance
	models},
  booktitle = {Proceedings of the 9th International Conference on Multimodal interfaces},
  year = {2007},
  pages = {9--14},
  address = {New York, NY, USA},
  publisher = {ACM},
  abstract = {Pain is typically assessed by patient self-report. Self-reported pain,
	however, is difficult to interpret and may be impaired or not even
	possible, as in young children or the severely ill. Behavioral scientists
	have identified reliable and valid facial indicators of pain. Until
	now they required manual measurement by highly skilled observers.
	We developed an approach that automatically recognizes acute pain.
	Adult patients with rotator cuff injury were video-recorded while
	a physiotherapist manipulated their affected and unaffected shoulder.
	Skilled observers rated pain expression from the video on a 5-point
	Likert-type scale. From these ratings, sequences were categorized
	as no-pain (rating of 0), pain (rating of 3, 4, or 5), and indeterminate
	(rating of 1 or 2). We explored machine learning approaches for pain-no
	pain classification. Active Appearance Models (AAM) were used to
	decouple shape and appearance parameters from the digitized face
	images. Support vector machines (SVM) were used with several representations
	from the AAM. Using a leave-one-out procedure, we achieved an equal
	error rate of 19% (hit rate = 81%) using canonical appearance and
	shape features. These findings suggest the feasibility of automatic
	pain detection from video.},
  doi = {http://doi.acm.org/10.1145/1322192.1322197},
  file = {Ashraf2007.pdf:Ashraf2007.pdf:PDF},
  isbn = {978-1-59593-817-6},
  keywords = {annotation labels-pain natural-data UNBC-McMaster corpus aam-features
	classification-svm temporal-fusion feature-generation},
  location = {Nagoya, Aichi, Japan},
  review = {Detection of spontaneous pain expressions using AAM and SVM classifier.
	The data set excluded any subject with facial hair or glasses. The
	ground truth data of pain levels was also only available the sequence
	level rather than at the frame level.}
}

@INPROCEEDINGS{Asthana2011,
  author = {Asthana, A. and Marks, T.K. and Jones, M.J. and Tieu, K.H. and Rohith,
	M.},
  title = {Fully automatic pose-invariant face recognition via 3D pose normalization},
  booktitle = {Computer Vision (ICCV), 2011 IEEE International Conference on},
  year = {2011},
  pages = {937 -944},
  month = {nov.},
  abstract = {An ideal approach to the problem of pose-invariant face recognition
	would handle continuous pose variations, would not be database specific,
	and would achieve high accuracy without any manual intervention.
	Most of the existing approaches fail to match one or more of these
	goals. In this paper, we present a fully automatic system for pose-invariant
	face recognition that not only meets these requirements but also
	outperforms other comparable methods. We propose a 3D pose normalization
	method that is completely automatic and leverages the accurate 2D
	facial feature points found by the system. The current system can
	handle 3D pose variation up to #x00B1;45 #x00B0; in yaw and #x00B1;30
	#x00B0; in pitch angles. Recognition experiments were conducted on
	the USF 3D, Multi-PIE, CMU-PIE, FERET, and FacePix databases. Our
	system not only shows excellent generalization by achieving high
	accuracy on all 5 databases but also outperforms other methods convincingly.},
  doi = {10.1109/ICCV.2011.6126336},
  issn = {1550-5499},
  keywords = {2D facial feature points;3D pose normalization method;3D pose variation;CMU-PIE
	databases;FERET databases;FacePix databases;USF 3D databases;automatic
	pose-invariant face recognition;continuous pose variations;fully
	automatic system;manual intervention;multiPIE databases;pitch angles;yaw;face
	recognition;pose estimation;visual databases;}
}

@INPROCEEDINGS{Auer2004,
  author = {Auer, P. and Ortner, R},
  title = {A Boosting Approach to Multiple Instance Learning},
  booktitle = {Proceedings of the 15th European Conference on Machine Learning},
  year = {2004},
  pages = {63–74},
  publisher = {Springer},
  abstract = {In this paper we present a boosting approach to multiple instance
	learning. As weak hypotheses we use balls (with respect to various
	metrics) centered at instances of positive bags. For the infin-norm
	these hypotheses can be modified into hyper-rectangles by a greedy
	algorithm. Our approach includes a stopping criterion for the algorithm
	based on estimates for the generalization error. These estimates
	can also be used to choose a preferable metric and data normalization.
	Compared to other approaches our algorithm delivers improved or at
	least competitive results on several multiple instance benchmark
	data sets.},
  file = {Auer2004.pdf:Auer2004.pdf:PDF},
  keywords = {mil supervised learning classification classification-method},
  owner = {ts00051},
  timestamp = {2010.03.04}
}

@INBOOK{Tellegen2008,
  chapter = {Exploring Personality Through Test Construction: Development of the
	Multidimensional Personality Questionnaire},
  pages = {261-293},
  title = {SAGE handbook of personality theory and assessment: Volume 2 — Personality
	measurement and testing.},
  publisher = {SAGE Publications Ltd},
  year = {2008},
  editor = {Gregory J. Boyle, Gerald Matthews, Donald H. Saklofske},
  author = {Auke Tellegen, Niels G. Waller},
  booktitle = {The SAGE Handbook of Personality Theory and Assessment: Volume 2
	— Personality Measurement and Testing},
  doi = {10.4135/9781849200479},
  url = {http://dx.doi.org/10.4135/9781849200479}
}

@ARTICLE{Avons1989,
  author = {Avons, S.E. and Leiser, R.G. and Carr, D.J.},
  title = {Paralanguage and human-computer interaction. Part 1: identification
	of recorded vocal segregates},
  journal = {Behaviour \& Information Technology},
  year = {1989},
  volume = {8},
  pages = {13-21},
  number = {1},
  abstract = { Abstract Vocal segregates are short non-lexical utterances such as
	‘mm-hmm’. They are frequently observed in natural dialogue, which
	they help to regulate and maintain. Twenty-five naïve subjects were
	asked to identify the meanings of isolated vocal segregates recorded
	by unfamiliar speakers. The segregates were recorded both with natural
	articulation, which enables the differentiation of vowel sounds,
	and with the speaker's mouth closed. Responses were made using a
	seven-alternative forced choice procedure. Mean identification accuracy
	was 69.4\% significantly above chance level, and performance was
	higher for vocal segregates which contained vowel sounds. The possible
	role of vocal segregates in human-computer interaction is discussed.
	},
  doi = {10.1080/01449298908914534},
  eprint = {http://www.tandfonline.com/doi/pdf/10.1080/01449298908914534},
  url = {http://www.tandfonline.com/doi/abs/10.1080/01449298908914534}
}

@INCOLLECTION{Awais2011,
  author = {Awais, Muhammad and Yan, Fei and Mikolajczyk, Krystian and Kittler,
	Josef},
  title = {Novel Fusion Methods for Pattern Recognition},
  booktitle = {Machine Learning and Knowledge Discovery in Databases},
  publisher = {Springer Berlin / Heidelberg},
  year = {2011},
  editor = {Gunopulos, Dimitrios and Hofmann, Thomas and Malerba, Donato and
	Vazirgiannis, Michalis},
  volume = {6911},
  series = {Lecture Notes in Computer Science},
  pages = {140-155},
  note = {10.1007/978-3-642-23780-5-19},
  abstract = {Over the last few years, several approaches have been proposed for
	information fusion including different variants of classifier level
	fusion (ensemble methods), stacking and multiple kernel learning
	(MKL). MKL has become a preferred choice for information fusion in
	object recognition. However, in the case of highly discriminative
	and complementary feature channels, it does not significantly improve
	upon its trivial baseline which averages the kernels. Alternative
	ways are stacking and classifier level fusion (CLF) which rely on
	a two phase approach. There is a significant amount of work on linear
	programming formulations of ensemble methods particularly in the
	case of binary classification. In this paper we propose a multiclass
	extension of binary ν -LPBoost, which learns the contribution of
	each class in each feature channel. The existing approaches of classifier
	fusion promote sparse features combinations, due to regularization
	based on ℓ 1 -norm, and lead to a selection of a subset of feature
	channels, which is not good in the case of informative channels.
	Therefore, we generalize existing classifier fusion formulations
	to arbitrary ℓ p -norm for binary and multiclass problems which results
	in more effective use of complementary information. We also extended
	stacking for both binary and multiclass datasets. We present an extensive
	evaluation of the fusion methods on four datasets involving kernels
	that are all informative and achieve state-of-the-art results on
	all of them.},
  affiliation = {Centre for Vision, Speech and Signal Processing (CVSSP), University
	of Surrey, UK},
  file = {:Awais2011.pdf:PDF},
  isbn = {978-3-642-23779-9},
  keyword = {Computer Science},
  url = {http://dx.doi.org/10.1007/978-3-642-23780-5_19}
}

@INPROCEEDINGS{Ayres02,
  author = {Jay Ayres and Johannes Gehrke and Tomi Yiu and Jason Flannick},
  title = {Sequential PAttern Mining using A Bitmap Representation},
  year = {2002},
  pages = {429--435},
  publisher = {ACM Press},
  abstract = {We introduce a new algorithm for mining sequential patterns. Our algorithm
	is especially efficient when the sequential patterns in the database
	are very long. We introduce a novel depth-first search strategy that
	integrates a depth-first traversal of the search space with effective
	pruning mechanisms.Our implementation of the search strategy combines
	a vertical bitmap representation of the database with efficient support
	counting. A salient feature of our algorithm is that it incrementally
	outputs new frequent itemsets in an online fashion. In a thorough
	experimental evaluation of our algorithm on standard benchmark data
	from the literature, our algorithm outperforms previous work up to
	an order of magnitude.},
  file = {Ayres02.pdf:Ayres02.pdf:PDF},
  keywords = {machine learning method supervised classification classification-sequential
	classification-method}
}

@ARTICLE{Azar2000,
  author = {Azar, B},
  title = {What's in a face?},
  journal = {Monitor on Psychology},
  year = {2000},
  volume = {31},
  number = {1},
  month = {Jan},
  file = {Azar2000.pdf:Azar2000.pdf:PDF},
  keywords = {emotion psychology emotion-definition},
  review = {Discusses to what extent facial expression indicates emotion. Fridlund,
	Nico Frijda considers facial expression as another channel in social
	situations. General consensus is facial expression is a good indicator
	in what someone will do next. Evolution of speech out of facial displays,
	vocalisation and mammalian sucking is discussed. Cross cultural data
	on facial paralanguage is scant.}
}

@INPROCEEDINGS{Bailly2009,
  author = {Bailly, Kevin and Milgram, Maurice},
  title = {{BISAR}: Boosted Input Selection Algorithm for Regression},
  booktitle = {Proceedings of International Joint Conference on Neural Networks},
  year = {2009},
  pages = {249-255},
  file = {Bailly2009.pdf:Bailly2009.pdf:PDF},
  keywords = {head-pose boost regression-method supervised},
  owner = {ts00051},
  timestamp = {2009.12.08}
}

@ARTICLE{Baker2004,
  author = {Simon Baker and Iain Matthews},
  title = {Lucas-Kanade 20 Years On: A Unifying Framework},
  journal = {International Journal of Computer Vision (IJCV)},
  year = {2004},
  volume = {56},
  pages = {221-255},
  number = {3},
  address = {Hingham, MA, USA},
  doi = {http://dx.doi.org/10.1023/B:VISI.0000011205.11775.fd},
  file = {Baker2004.pdf:Baker2004.pdf:PDF},
  issn = {0920-5691},
  keywords = {lk feature tracking},
  publisher = {Kluwer Academic Publishers}
}

@INCOLLECTION{Ball2000,
  author = {Ball, Gene and Breese, Jack},
  title = {Relating Personality and Behavior: Posture and Gestures},
  booktitle = {Affective Interactions},
  publisher = {Springer Berlin Heidelberg},
  year = {2000},
  editor = {Paiva, Ana},
  volume = {1814},
  series = {Lecture Notes in Computer Science},
  pages = {196-203},
  doi = {10.1007/10720296_14},
  isbn = {978-3-540-41520-6},
  url = {http://dx.doi.org/10.1007/10720296_14}
}

@INCOLLECTION{Ball2000b,
  author = {Ball, Gene and Breese, Jack},
  title = {Embodied conversational agents},
  publisher = {MIT Press},
  year = {2000},
  chapter = {Emotion and personality in a conversational agent},
  pages = {189--219},
  address = {Cambridge, MA, USA},
  acmid = {371564},
  isbn = {0-262-03278-3},
  numpages = {31},
  url = {http://dl.acm.org/citation.cfm?id=371552.371564}
}

@INPROCEEDINGS{Baltrusaitis2012,
  author = {Baltrusaitis, Tadas and Robinson, Peter and Morency, Louis-Philippe},
  title = {{3D} Constrained Local Model for Rigid and Non-Rigid Facial Tracking},
  booktitle = {Computer Vision and Pattern Recognition {(CVPR} 2012)},
  year = {2012},
  address = {Providence, {RI}},
  month = jun,
  abstract = {We present {3D} Constrained Local Model {(CLM-Z)} for robust facial
	feature tracking under varying pose. Our approach integrates both
	depth and intensity information in a common framework. We show the
	benefit of our {CLM-} Z method in both accuracy and convergence rates
	over regular {CLM} formulation through experiments on publicly available
	datasets. Additionally, we demonstrate a way to combine a rigid head
	pose tracker with {CLM-Z} that benefits rigid head tracking. We show
	better performance than the current state-of-the-art approaches in
	head pose tracking with our extension of the generalised adaptive
	view-based appearance model {(GAVAM).}},
  url = {http://ict.usc.edu/pubs/3D%20Constrained%20Local%20Model%20for%20Rigid%20and%20Non-Rigid%20Facial%20Tracking.pdf}
}

@ARTICLE{Barnett2001,
  author = {Barnett, Elizabeth and Michele Casper},
  title = {A Definition of “Social Environment”},
  journal = {American Journal of Public Health},
  year = {2001},
  volume = {91},
  number = {3},
  month = {March},
  owner = {tim},
  timestamp = {2012.10.04}
}

@INBOOK{Bartlett2010,
  chapter = {Automated facial expression measurement: Recent applications to basic
	research in human behavior, learning, and education},
  title = {Handbook of Face Perception},
  publisher = {Oxford University Press.},
  year = {2010},
  editor = {Andrew Calder and Gillian Rhodes and James V. Haxby and Mark H. Johnson},
  author = {Bartlett, M. and Whitehill, J.},
  file = {:Bartlett2010.pdf:PDF},
  keywords = {review-paper nvc-applications feature-generation},
  owner = {ts00051},
  timestamp = {2012.03.01}
}

@INPROCEEDINGS{Bartlett2006,
  author = {Marian Stewart Bartlett and Gwen Littlewort and Mark Frank and Claudia
	Lainscsek and Ian Fasel and Javier Movellan},
  title = {Fully Automatic Facial Action Recognition in Spontaneous Behavior},
  booktitle = {Proceedings of the 7th IEEE International Conference on Automatic
	Face and Gesture Recognition},
  year = {2006},
  pages = {223--230},
  address = {Washington, DC, USA},
  publisher = {IEEE Computer Society},
  doi = {http://dx.doi.org/10.1109/FGR.2006.55},
  file = {Bartlett2006.pdf:Bartlett2006.pdf:PDF},
  isbn = {0-7695-2503-2},
  keywords = {facial deformation recognition expression labels-facs face-detection
	texture-feature gabor feature-selection adaboost classification-svm
	posed-data corpus-cohn-kanade corpus-ekman-hager feature-generation},
  review = {Factual action units where detected using a gabor wavelet decomposition
	and classified into AUs by Adaboost and SVM independently (for comparison).
	The evolution and intensity of the AU over time is determined. No
	high level emotion detection was attempted. Video data uses spontaneous
	emotions.}
}

@INPROCEEDINGS{Vetter2006,
  author = {Curzio Basso and Pascal Paysan and Thomas Vetter},
  title = {Registration of Expressions Data using a 3{D} Morphable Model},
  booktitle = {Proceedings of the 7th IEEE International Conference on Automatic
	Face and Gesture Recognition},
  year = {2006},
  pages = {205-210},
  address = {Washington, DC, USA},
  publisher = {IEEE Computer Society},
  abstract = {The registration of 3D scans of faces is a key step for many applications,
	in particular for building 3D Morphable Models. Although a number
	of algorithms are already available for registering data with neutral
	expression, the registration of scans with arbitrary expressions
	is typically performed under the assumption of a known, fixed identity.
	We present a novel algorithm which breaks this restriction, allowing
	to register 3D scans of faces with arbitrary identity and expression.
	Furthermore, our algorithm can process incomplete data, yielding
	results which are both continuous and with low reconstruction error.
	Even in the case of complete, expression-less data, our method can
	yield better results than previous algorithms, due to an adaptive
	smoothing, which regularizes the results surface only where the estimated
	correspondence is unreliable.},
  doi = {http://dx.doi.org/10.1109/FGR.2006.89},
  file = {Vetter2006.pdf:Vetter2006.pdf:PDF},
  isbn = {0-7695-2503-2},
  keywords = {model facial head-model tracking},
  review = {Registration of face using morphable models. The identity and expression
	vectors are separately obtained.}
}

@ARTICLE{Baum1972,
  author = {Baum, L.E.},
  title = {An Inequality and Associated Maximization Technique in Statistical
	Estimation of Probabilistic Functions of a {Markov} Process},
  journal = {Inequalities},
  year = {1972},
  volume = {3},
  pages = {1-8},
  keywords = {hmms hidden markov models expectation-maximization supervised learning
	sequential classification-method classification-hmms},
  owner = {ts00051},
  timestamp = {2011.12.23}
}

@ARTICLE{Bauman95,
  author = {Bauman, Sara L. and Hambrecht, Georgia},
  title = {Analysis of View Angle Used in Speechreading Training of Sentences},
  journal = {American Journal of Audiology},
  year = {1995},
  volume = {4},
  pages = {67-70},
  number = {3},
  abstract = {This study examines the effects of sentence perception across three
	speaker viewing angles: front view (0{degrees}), quarter view (45{degrees}),
	and side view (90{degrees}). The performance of a female adult with
	postlingual hearing loss was measured for accuracy at each angle.
	The present study used a single-subject, alternating treatment design
	where three treatment angles were randomly presented in every session.
	The percentage accuracy levels for each session were compared to
	determine the most effective treatment viewing angle of the speaker.
	The results indicated that the side-view angle was most effective,
	as the percentage gain of improvement was greatest in combination
	with the consistent upward trend of the data points across treatment
	sessions. The performance at front-view and quarter-view angles was
	also successful. The results of this preliminary effort indicate
	the value of treatment for visual sentence perception at all three
	angles, including the nontraditionally targeted side view.},
  eprint = {http://aja.asha.org/cgi/reprint/4/3/67.pdf},
  keywords = {lipreading perception},
  url = {http://aja.asha.org/cgi/content/abstract/4/3/67}
}

@INCOLLECTION{Bavelas97,
  author = {Bavelas, J. B. and Chovil, N.},
  title = {Faces in Dialogue},
  booktitle = {The Psychology of Facial Expression},
  publisher = {Cambridge University Press},
  year = {1997},
  editor = {Russell, J. A. and Fernandez-Dols, J. M.},
  pages = {334-346},
  address = {Cambridge, UK},
  file = {:Bavelas97.pdf:PDF},
  keywords = {nvc-definition facial expression naturalistic-justification nvc-annotation
	nvc-taxonomy dialogue natural-important},
  review = {Emphasises dynamics in expression. Verbal and non-verbal communication
	are closely linked. Emphasises work should be in naturalistic situations.
	
	
	"It should be apparent that this is not a theory that can be investigated
	by physical decriptions of still photographs or by study of only
	the isolated video track of a videotape." "We analyse facial displays
	as they occur in actual social interaction, with the goal of understanding
	their meaning in context."
	
	
	"First, we assume that it is natural for people, including our analysts,
	to approach dialogue at the level of meaning. In our spontaneous
	interactions with others, we are attending to meaning and not to
	physical movements or configurations."}
}

@ARTICLE{Beaton2000,
  author = {Beaton, D. E. and Bombardier, C. and Guillemin, F. and Ferraz, M.
	B.},
  title = {Guidelines for the process of cross-cultural adaptation of self-report
	measures},
  journal = {Spine},
  year = {2000},
  volume = {25},
  pages = {3186-3191},
  number = {24},
  file = {:Beaton2000.pdf:PDF},
  keywords = {translation instruments},
  owner = {tim},
  timestamp = {2011.07.07}
}

@ARTICLE{Beattie1982b,
  author = {Beattie, G.},
  title = {Unnatural behaviour in the laboratory},
  journal = {New Scientist},
  year = {1982},
  volume = {96},
  pages = {181},
  keywords = {reactivity natural conversation natural-important},
  owner = {ts00051},
  timestamp = {2012.02.02}
}

@ARTICLE{BeckerAsano2011,
  author = {Becker-Asano, C.},
  title = {Invited Commentary: On Guiding the Design of an Ill-defined Phenomenon},
  journal = {International Journal of Synthetic Emotions},
  year = {2011},
  volume = {2},
  pages = {66-67},
  number = {2},
  comments = {http://www.becker-asano.de/Hudlicka_commentary.pdf},
  doi = {http://dx.doi.org/10.4018/jse.2011070104}
}

@INPROCEEDINGS{Berger1990,
  author = {Marie-Odile Berger},
  title = {Snake growing},
  booktitle = {Proceedings of the First European Conference on Computer Vision},
  year = {1990},
  pages = {570-572},
  address = {London, UK},
  publisher = {Springer-Verlag},
  isbn = {3-540-52522-X},
  keywords = {unclassified, model fitting}
}

@INPROCEEDINGS{Bernhardt07,
  author = {Daniel Bernhardt and Peter Robinson},
  title = {Detecting affect from non-stylised body motions},
  booktitle = {Proceedings of the International Conference on Affective Computing
	\& Intelligent Interaction (ACII)},
  year = {2007},
  pages = {59-70},
  address = {Lisbon, Portugal},
  month = {September},
  abstract = {In this paper we present a novel framework for analysing non-stylised
	motion in order to detect implicitly communicated affect. Our approach
	makes use of a segmentation technique which can divide complex motions
	into a set of automatically derived motion primitives. The parsed
	motion is then analysed in terms of dynamic features which are shown
	to encode affective information. In order to adapt our algorithm
	to personal movement idiosyncrasies we developed a new approach for
	deriving unbiased motion features. We have evaluated our approach
	using a comprehensive database of affectively performed motions.
	The results show that removing personal movement bias can have a
	significant benefit for automated affect recognition from body motion.
	The resulting recognition rate is similar to that of humans who took
	part in a comparable psychological experiment.},
  file = {Bernhardt07.pdf:Bernhardt07.pdf:PDF},
  keywords = {personal-differences whole-body human-performance posed-data energy-features}
}

@INBOOK{Bernieri1991,
  chapter = {Interpersonal coordination: Behavior matching and interactional synchrony},
  pages = {401-432},
  title = {Fundamentals of nonverbal behavior. Studies in emotion and social
	interaction},
  publisher = {Cambridge University Press},
  year = {1991},
  editor = {Feldman, R. and B. Rime},
  author = {Bernieri, F. and Rosenthal, R.},
  address = {New York},
  owner = {ts00051},
  timestamp = {2012.10.08}
}

@INPROCEEDINGS{Bickel2008,
  author = {Bickel, Bernd and Lang, Manuel and Botsch, Mario and Otaduy, Miguel
	A. and Gross, Markus},
  title = {Pose-Space Animation and Transfer of Facial Details},
  booktitle = {Proc. of the ACM SIGGRAPH / Eurographics Symposium on Computer Animation},
  year = {2008},
  pages = {57--66},
  month = {jul},
  url = {http://www.gmrv.es/Publications/2008/BLBOG08}
}

@ARTICLE{Blaschke2004,
  author = {Blaschke, T. and Wiskott, L.},
  title = {{CuBICA}: Independent Component Analysis by Simultaneous Third- and
	Fourth-Order Cumulant Diagonalization},
  journal = {IEEE Transactions on Signal Processing},
  year = {2004},
  volume = {52},
  pages = { 1250-1256},
  number = {5},
  month = {May},
  doi = {10.1109/TSP.2004.826173},
  file = {Blaschke2004.ps.gz:Blaschke2004.ps.gz:PostScript},
  issn = {1053-587X},
  keywords = {approximation theory, higher order statistics, independent component
	analysis, optimisation, signal processing Comon algorithm, CuBICA,
	approximation, data sets, fourth-order cumulant diagonalization,
	independent component analysis, optimization, third-order cumulant
	diagonalization, unsupervised learning machine method dimensional-reduction}
}

@INBOOK{Boker2009,
  chapter = {Effects of damping head movement and facial expression in dyadic
	conversation using real–time facial expression tracking and synthesized
	avatars},
  pages = {3485-3495},
  title = {Philosophical Transactions of Royal Society B},
  publisher = {Royal Society},
  year = {2009},
  author = {Boker, Steven M. Boker and Cohn, Jeffrey F. and Theobald, Barry-John
	and Matthews, Iain and Brick, Timothy R. and Spies, Jeffrey R},
  volume = {364},
  abstract = {When people speak with one another, they tend to adapt their head
	movements and facial expressions
	
	in response to each others’ headmovements and facial expressions.We
	present an experiment in which
	
	confederates’ head movements and facial expressions were motion tracked
	during videoconference
	
	conversations, an avatar face was reconstructed in real time, and
	naive participants spoke with the
	
	avatar face. No naive participant guessed that the computer generated
	face was not video. Confederates’
	
	facial expressions, vocal inflections and head movements were attenuated
	at 1 min intervals in a
	
	fully crossed experimental design. Attenuated head movements led to
	increased head nods and lateral
	
	head turns, and attenuated facial expressions led to increased head
	nodding in both naive participants
	
	and confederates. Together, these results are consistent with a hypothesis
	that the dynamics of head
	
	movements in dyadic conversation include a shared equilibrium. Although
	both conversational
	
	partners were blind to the manipulation, when apparent head movement
	of one conversant was
	
	attenuated, both partners responded by increasing the velocity of
	their head movements.},
  file = {Boker2009.pdf:Boker2009.pdf:PDF},
  keywords = {human perception behaviour dyadic head-pose},
  owner = {ts00051},
  review = {Really interesting experimental paradigm: modify the appearance of
	individuals in real time and see the effects on dyadic conversation.},
  timestamp = {2010.01.07}
}

@ARTICLE{Boom2010,
  author = {Bastiaan Johannes Boom and Luuk J. Spreeuwers and Raymond N. J. Veldhuis},
  title = {Subspace-Based Holistic Registration for Low-Resolution Facial Images},
  journal = {EURASIP J. Adv. Sig. Proc.},
  year = {2010},
  pages = {-1--1}
}

@ARTICLE{Bourgeois2008,
  author = {Patrick Bourgeois and Ursula Hess},
  title = {The impact of social context on mimicry},
  journal = {Biological Psychology},
  year = {2008},
  volume = {77},
  pages = {343 - 352},
  number = {3},
  abstract = {Facial mimicry, the tendency to imitate other's facial expressions,
	has frequently been described as a reflex-like mechanism that function
	independent of the relationship between expresser and observer. However,
	there is also evidence suggesting that it is a social cue regulating
	social interactions and that consequently mimicry varies as a function
	of social context and the type of emotion expression shown. Two studies
	were conducted to the assess impact of social group membership and
	type of expression on facial mimicry. Results suggest that the level
	of facial mimicry varies as a function of group membership. Moreover,
	mimicry levels were influenced by the kind of emotion displayed by
	the expresser. Although participants mimicked happiness displays
	regardless of the expresser's group membership, negative emotions
	were either not mimicked or only when shown by an ingroup member.},
  doi = {10.1016/j.biopsycho.2007.11.008},
  issn = {0301-0511},
  keywords = {Emotion},
  url = {http://www.sciencedirect.com/science/article/pii/S0301051107001949}
}

@INPROCEEDINGS{Bousmalis2009,
  author = {Bousmalis, K. and Mehu, M. and Pantic, M.},
  title = {Spotting Agreement and Disagreement: A Survey of Nonverbal Audiovisual
	Cues and Tools},
  booktitle = {Proceedings of the International Conference on Affective Computing
	\& Intelligent Interaction (ACII)},
  year = {2009},
  volume = {2},
  address = {Amsterdam, Netherlands},
  month = {September},
  abstract = {While detecting and interpreting temporal patterns of non–verbal behavioral
	cues in a given context is a natural and often unconscious process
	for humans, it remains a rather difficult task for computer systems.
	Nevertheless, it is an important one to achieve if the goal is to
	realise a naturalistic communication between humans and machines.
	Machines that are able to sense social attitudes like agreement and
	disagreement and respond to them in a meaningful way are likely to
	be welcomed by users due to the more natural, efficient and human–centered
	interaction they are bound to experience. This paper surveys the
	nonverbal cues that could be present during agreement and disagreement
	behavioural displays and lists a number of tools that could be useful
	in detecting them, as well as a few publicly available databases
	that could be used to train these tools for analysis of spontaneous,
	audiovisual instances of agreement and disagreement.},
  file = {Bousmalis2009.pdf:Bousmalis2009.pdf:PDF},
  keywords = {survey-paper nvc backchannel corpus-list back-channel},
  owner = {ts00051},
  timestamp = {2010.01.15}
}

@INPROCEEDINGS{Bousmalis2011,
  author = {Bousmalis, K. and L.-P. Morency and M. Pantic},
  title = {Modeling Hidden Dynamics of Multimodal Cues for Spontaneous Agreement
	and Disagreement Recognition},
  booktitle = {Proc. IEEE Int'l Conf. Automatic Face and Gesture Recognition},
  year = {2011},
  file = {:Bousmalis2011.pdf:PDF},
  owner = {ts00051},
  timestamp = {2012.10.15}
}

@ARTICLE{Bousmalis2012,
  author = {Bousmalis, K. and S. Zafeiriou and L.-P. Morency and M. Pantic},
  title = {Infinite Hidden Conditional Random Fields for Human Behavior Analysis},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  year = {2012},
  file = {:Bousmalis2012.pdf:PDF},
  owner = {ts00051},
  timestamp = {2012.10.15}
}

@ARTICLE{Brune2009,
  author = {Br\"{u}ne, Martin and Abdel-Hamid, Mona and Sonntag, Claudia and
	Lehmk\"{a}mper, Caroline and Langdon, Robyn},
  title = {Linking social cognition with social interaction: Non-verbal expressivity,
	social competence and "mentalising" in patients with schizophrenia
	spectrum disorders},
  journal = {Behavioral and Brain Functions},
  year = {2009},
  volume = {5},
  number = {6},
  doi = {10.1186/1744-9081-5-6},
  file = {Brune2009.pdf:Brune2009.pdf:PDF},
  keywords = {nvc expression psychological personal-differences},
  owner = {ts00051},
  timestamp = {2010.01.18}
}

@ARTICLE{Breiman2001,
  author = {Breiman, Leo},
  title = {Random Forests},
  journal = {Machine Learning},
  year = {2001},
  volume = {45},
  pages = {5–32},
  number = {1},
  abstract = {Random forests are a combination of tree predictors
	
	such that each tree depends on the values of a random
	
	vector sampled independently and with the same
	
	distribution for all trees in the forest. The
	
	generalization error for forests converges a.s. to a limit
	
	as the number of trees in the forest becomes large.
	
	The generalization error of a forest of tree classifiers
	
	depends on the strength of the individual trees in the
	
	forest and the correlation between them. Using a
	
	random selection of features to split each node yields
	
	error rates that compare favorably to Adaboost
	
	(Freund and Schapire[1996]), but are more robust with
	
	respect to noise. Internal estimates monitor error,
	
	strength, and correlation and these are used to show
	
	the response to increasing the number of features used
	
	in the splitting. Internal estimates are also used to
	
	measure variable importance. These ideas are also
	
	applicable to regression.},
  file = {Breiman2001.pdf:Breiman2001.pdf:PDF},
  keywords = {classification regression machine learning method supervised learning
	classification-method},
  owner = {ts00051},
  timestamp = {2010.02.22}
}

@BOOK{Breiman1984,
  title = {Classification and Regression Trees},
  publisher = {Wadsworth and Brooks},
  year = {1984},
  author = {L. Breiman and J. Friedman and R. Olshen and C. Stone},
  address = {Monterey, CA},
  remarks = {cited in \cite{cslu:esca98mm, cslu:icslp98cronk, cstr:unitsel97} for
	CART, clustering, and decision trees}
}

@ARTICLE{Brinton2007,
  author = {Brinton, Bonnie and Spackman, Matthew P. and Fujiki, Martin and Ricks,
	Jenny},
  title = {What Should Chris Say? The Ability of Children With Specific Language
	Impairment to Recognize the Need to Dissemble Emotions in Social
	Situations},
  journal = {J Speech Lang Hear Res},
  year = {2007},
  volume = {50},
  pages = {798-811},
  number = {3},
  abstract = {PurposeIn this study, the authors examined the ability of children
	with specific language impairment (SLI) and their typical peers to
	judge when an experienced emotion should be dissembled (hidden) in
	accord with social display rules. MethodParticipants included 19
	children with SLI and19 children with typical language skills, both
	groups ranging in age from 7;9 (years;months) to 10;10, with a mean
	age of 9;1. Children were presented with 10 hypothetical social situations
	in which a character, Chris, experienced an emotion that should be
	dissembled for social purposes. The participants' responses were
	categorized as to whether or not they dissembled or displayed the
	emotion. ResultsAlthough the task was difficult for many participants,
	children with SLI indicated that the experienced emotion should be
	dissembled significantly less often than did their typical peers.
	Children in the 2 groups did not significantly differ in their judgments
	of the social display rules governing these situations. ConclusionThese
	results suggested that the children with SLI did not understand the
	impact of displaying emotion on relationships in the same way as
	did their typical peers. In this respect, they seemed to lag behind
	the typical children in their developing emotion knowledge.},
  doi = {10.1044/1092-4388(2007/055)},
  eprint = {http://jslhr.asha.org/cgi/reprint/50/3/798.pdf},
  url = {http://jslhr.asha.org/cgi/content/abstract/50/3/798}
}

@BOOK{BritishDeafAssociation_Dictionary_1992,
  title = {Dictionary of British Sign Language/English},
  publisher = {Faber and Faber},
  year = {1992},
  author = {{British Deaf Association} }
}

@ARTICLE{Buciu2009,
  author = {Ioan Buciu and Costas Kotropoulos and Ioannis Pitas},
  title = {Comparison of ICA approaches for facial expression recognition},
  journal = {Signal, Image and Video Processing},
  year = {2009},
  volume = {3},
  pages = {345-361},
  number = {4},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  ee = {http://dx.doi.org/10.1007/s11760-008-0074-3}
}

@ARTICLE{Buck1979,
  author = {Buck, R},
  title = {Measuring individual differences in the nonverbal communication of
	affect: The slide-viewing paradigm},
  journal = {Human Communication Research},
  year = {1979},
  volume = {6},
  pages = {47-57},
  abstract = {This paper describes the slide-viewing paradigm for measuring nonverbal
	sending accuracy—the spontaneous tendency to display communicative
	nonverbal behavior— and nonverbal receiving ability—the ability to
	accurately decode such behaviors when they occur in others. Sending
	accuracy has been measured in adults, preschool children, and brain-damaged
	patients, and suggestive relationships have emerged between sending
	accuracy and gender, personality, autonomic responding, and presumed
	site of brain damage. The pattern of results suggests that sending
	accuracy may be related to both unlearned, temperamental factors
	and to gender-related social learning experiences. Receiving ability
	has been measured in adults and preschool children, and relationships
	with various measures have been demonstrated.},
  keywords = {nvc gender-difference human-performance},
  owner = {ts00051},
  review = {Sender is recorded viewing slides that provoke a reaction. A receiver
	attempts to identify which slide is being viewed. CARAT. What does
	accuracy depend on?},
  timestamp = {2011.12.09}
}

@BOOK{Burgoon1996,
  title = {Nonverbal communication: The unspoken dialogue},
  publisher = {Harper \& Row},
  year = {1996},
  author = {Burgoon, J. K., Buller, D. B., \& Woodall, W. G.},
  address = {New York},
  owner = {tim},
  timestamp = {2013.03.30}
}

@ARTICLE{Busso2007,
  author = {Busso, C. and Narayanan, S.S.},
  title = {Interrelation Between Speech and Facial Gestures in Emotional Utterances:
	A Single Subject Study},
  journal = {Audio, Speech, and Language Processing, IEEE Transactions on},
  year = {2007},
  volume = {15},
  pages = {2331 -2347},
  number = {8},
  month = {nov. },
  abstract = {The verbal and nonverbal channels of human communication are internally
	and intricately connected. As a result, gestures and speech present
	high levels of correlation and coordination. This relationship is
	greatly affected by the linguistic and emotional content of the message.
	The present paper investigates the influence of articulation and
	emotions on the interrelation between facial gestures and speech.
	The analyses are based on an audio-visual database recorded from
	an actress with markers attached to her face, who was asked to read
	semantically neutral sentences, expressing four emotion states (neutral,
	sadness, happiness, and anger). A multilinear regression framework
	is used to estimate facial features from acoustic speech parameters.
	The levels of coupling between the communication channels are quantified
	by using Pearson's correlation between the recorded and estimated
	facial features. The results show that facial and acoustic features
	are strongly interrelated, showing levels of correlation higher than
	r = 0.8 when the mapping is computed at sentence-level using spectral
	envelope speech features. The results reveal that the lower face
	region provides the highest activeness and correlation levels. Furthermore,
	the correlation levels present significant interemo- tional differences,
	which suggest that emotional content affect the relationship between
	facial gestures and speech. Principal component analysis (PCA) shows
	that the audiovisual mapping parameters are grouped in a smaller
	subspace, which suggests that there is an emotion-dependent structure
	that is preserved from across sentences. The results suggest that
	this internal structure seems to be easy to model when prosodic-features
	are used to estimate the audiovisual mapping. The results also reveal
	that the correlation levels within a sentence vary according to broad
	phonetic properties presented in the sentence. Consonants, especially
	unvoiced and fricative sounds, present the lowest correlation lev-
	els. Likewise, the results show that facial gestures are linked at
	different resolutions. While the orofacial area is locally connected
	with the speech, other facial gestures such as eyebrow motion are
	linked only at the sentence-level. The results presented here have
	important implications for applications such as facial animation
	and multimodal emotion recognition.},
  doi = {10.1109/TASL.2007.905145},
  file = {:Busso2007.pdf:PDF},
  issn = {1558-7916},
  keywords = {PCA;audio-visual database;correlation levels;emotional utterances;facial
	feature estimation;facial gestures;human communication;multilinear
	regression framework;nonverbal channels;principal component analysis;semantically
	neutral sentences;speech gestures;verbal channels;audio databases;audio-visual
	systems;correlation methods;emotion recognition;estimation theory;face
	recognition;feature extraction;principal component analysis;regression
	analysis;speech processing;visual databases;}
}

@INPROCEEDINGS{Campbell2010,
  author = {Campbell, N. and Tabeta, A.},
  title = {A software toolkit for viewing annotated multimodal data interactively
	over the web},
  booktitle = {Proceedings of the International Conference on Language Resources
	and Evaluation (LREC)},
  year = {2010},
  abstract = {This paper describes a software toolkit for the interactive display
	and analysis of automatically extracted or manually derived annotation
	features of visual and audio data. It has been extensively tested
	with material collected as part of the FreeTalk Multimodal Conversation
	Corpus. Both the corpus and the software are available for download
	from sites in Europe and Japan. The corpus consists of several hours
	of video and audio recordings from a variety of capture devices,
	and includes subjective annotations of the content, along with derived
	data obtained from image processing. Because of the large size of
	the corpus, it is unrealistic to expect researchers to download all
	the material before deciding whether it will be useful to them in
	their research. We have therefore devised a means for interactive
	browsing of the content and for viewing at different levels of granularity.
	This has resulted in a simple set of tools that can be added to any
	website to allow similar browsing of audio- video recordings and
	their related data and annotations.},
  file = {:Campbell2010.pdf:PDF},
  keywords = {FreeTalk corpus corpus-FreeTalk annotation tools},
  owner = {ts00051},
  timestamp = {2012.01.10}
}

@INCOLLECTION{Campbell96,
  author = {Campbell, Ruth},
  title = {Seeing Brains Reading Speech: A Review and Speculations},
  booktitle = {Speechreading by Man and Machine: Data, Models and Systems},
  publisher = {NATO/Springer-Verlag},
  year = {1996},
  editor = {Stork, David G. and Hennecke, Marcus E.},
  address = {New York, NY},
  keywords = {lipreading brain FMRI},
  review = {Description of visual, audio and cognitive cortical location used
	in audio/visual and silent speech reading. Case studies of patients
	are compared to speech reading ability. There appears to be no single
	brain area which is purely dedicated and required for speech reading.}
}

@ARTICLE{Carifio2007,
  author = {Carifio, James and Perla, Rocco J.},
  title = {Ten Common Misunderstandings, Misconceptions, Persistent Myths and
	Urban Legends about Likert Scales and Likert Response Formats and
	their Antidotes},
  journal = {Journal of Social Sciences},
  year = {2007},
  volume = {3},
  pages = {106-116},
  number = {3},
  doi = {10.3844/jssp.2007.106.116},
  owner = {tim},
  timestamp = {2012.11.01}
}

@ARTICLE{Carletta2007,
  author = {Carletta, J.},
  title = {Unleashing the killer corpus: experiences in creating the multi-everything
	{AMI} {M}eeting {C}orpus},
  journal = {Language Resources and Evaluation Journal},
  year = {2007},
  volume = {41},
  pages = {181-190},
  number = {2},
  file = {Carletta2007.pdf:Carletta2007.pdf:PDF},
  keywords = {corpus-ami corpus-announce situation-meeting},
  owner = {ts00051},
  timestamp = {2009.10.26}
}

@ARTICLE{Chantler1997,
  author = {Chantler, M.J. and Stoner, J.P.},
  title = {Automatic interpretation of sonar image sequences using temporal
	feature measures},
  journal = {Oceanic Engineering, IEEE Journal of},
  year = {1997},
  volume = {22},
  pages = {47 -56},
  number = {1},
  month = {jan},
  abstract = {This paper reports the development of a system for the automated interpretation
	of sector scan sonar data. It proposes the use of a new combination
	of feature measures derived from sequences of sonar scans to characterize
	the behaviour of targets' returns over time. Previous research used
	grey-scale and shape descriptors derived from single sonar scans.
	However, problems were experienced with targets whose return varied
	significantly over time (such as divers, UUV's, and ships' wakes).
	Hence a new set of temporal feature measures has been developed by
	combining existing one-dimensional temporal measures and two-dimensional
	object descriptors. These new features provide a quantitative description
	of the behaviour of a target's two-dimensional returns over a sequence
	of sonar scans. Experiments with a limited but real data set have
	shown that classification accuracy can be significantly improved
	by the use of these new features. The use of ldquo;static rdquo;
	feature measures (derived from a single scan) was observed to give
	classification errors of between 7% and 10% when they were applied
	to the data set. In contrast, the use of temporal measures reduced
	this error rate to 1% or 2% and in some cases reduced it to zero
	},
  doi = {10.1109/48.557539},
  issn = {0364-9059},
  keywords = {Error analysis;Helium;Image sequences;Monitoring;Sea measurements;Sea
	surface;Shape;Sonar measurements;Sonar navigation;Time measurement;feature
	extraction;filtering theory;image classification;image sequences;object
	detection;real-time systems;sonar imaging;classification accuracy;classification
	errors;object classification;one-dimensional temporal measures;quantitative
	description;sector scan sonar data;sonar image sequences;target returns;temporal
	feature measures;two-dimensional object descriptors;}
}

@ARTICLE{Chartrand1999,
  author = {Chartrand, Tanya L. and Bargh, John A.},
  title = {The chameleon effect: The perception-behavior link and social interaction},
  journal = {Journal of Personality and Social Psychology},
  year = {1999},
  volume = {76},
  pages = {893-910},
  number = {6},
  month = {Jun},
  abstract = {The chameleon effect refers to nonconscious mimicry of the postures,
	mannerisms, facial expressions, and other behaviors of one's interaction
	partners, such that one's behavior passively and unintentionally
	changes to match that of others in one's current social environment.
	The authors suggest that the mechanism involved is the perception-behavior
	link, the recently documented finding (e.g., J. A. Bargh, M. Chen,
	& L. Burrows, 1996) that the mere perception of another's behavior
	automatically increases the likelihood of engaging in that behavior
	oneself. Experiment 1 showed that the motor behavior of participants
	unintentionally matched that of strangers with whom they worked on
	a task. Experiment 2 had confederates mimic the posture and movements
	of participants and showed that mimicry facilitates the smoothness
	of interactions and increases liking between interaction partners.
	Experiment 3 showed that dispositionally empathic individuals exhibit
	the chameleon effect to a greater extent than do other people.},
  keywords = {human behaviour mimic perception NVC coupled},
  owner = {ts00051},
  timestamp = {2012.01.26}
}

@ARTICLE{Chew2012,
  author = {Chew, S.W. and Lucey, P. and Lucey, S. and Saragih, J. and Cohn,
	J.F. and Matthews, I. and Sridharan, S.},
  title = {In the Pursuit of Effective Affective Computing: The Relationship
	Between Features and Registration},
  journal = {Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions
	on},
  year = {2012},
  volume = {42},
  pages = {1006 -1016},
  number = {4},
  month = {aug. },
  abstract = {For facial expression recognition systems to be applicable in the
	real world, they need to be able to detect and track a previously
	unseen person's face and its facial movements accurately in realistic
	environments. A highly plausible solution involves performing a #x201C;dense
	#x201D; form of alignment, where 60-70 fiducial facial points are
	tracked with high accuracy. The problem is that, in practice, this
	type of dense alignment had so far been impossible to achieve in
	a generic sense, mainly due to poor reliability and robustness. Instead,
	many expression detection methods have opted for a #x201C;coarse
	#x201D; form of face alignment, followed by an application of a biologically
	inspired appearance descriptor such as the histogram of oriented
	gradients or Gabor magnitudes. Encouragingly, recent advances to
	a number of dense alignment algorithms have demonstrated both high
	reliability and accuracy for unseen subjects [e.g., constrained local
	models (CLMs)]. This begs the question: Aside from countering against
	illumination variation, what do these appearance descriptors do that
	standard pixel representations do not? In this paper, we show that,
	when close to perfect alignment is obtained, there is no real benefit
	in employing these different appearance-based representations (under
	consistent illumination conditions). In fact, when misalignment does
	occur, we show that these appearance descriptors do work well by
	encoding robustness to alignment error. For this work, we compared
	two popular methods for dense alignment-subject-dependent active
	appearance models versus subject-independent CLMs-on the task of
	action-unit detection. These comparisons were conducted through a
	battery of experiments across various publicly available data sets
	(i.e., CK+, Pain, M3, and GEMEP-FERA). We also report our performance
	in the recent 2011 Facial Expression Recognition and Analysis Challenge
	for the subject-independent task.},
  doi = {10.1109/TSMCB.2012.2194485},
  issn = {1083-4419},
  keywords = {CK+ data set;Facial Expression Recognition and Analysis Challenge;GEMEP-FERA
	data set;Gabor magnitudes;Histogram of Oriented Gradients;M3 data
	set;Pain data set;action-unit detection;alignment error;appearance-based
	representations;biologically-inspired appearance descriptor;constrained
	local models;dense alignment algorithms;face alignment;facial expression
	recognition systems;facial movements;feature extraction;fiducial
	facial point tracking;illumination variation;image registration;pixel
	representations;publicly available data sets;realistic environments;robustness
	encoding;subject-dependent active appearance models;subject-independent
	CLM;subject-independent task;unseen person face detection;unseen
	person face tracking;emotion recognition;face recognition;feature
	extraction;image registration;lighting;object tracking;realistic
	images;}
}

@INPROCEEDINGS{Chew2012b,
  author = {Chew, S.W. and Lucey, S. and Lucey, P. and Sridharan, S. and Conn,
	J.F.},
  title = {Improved facial expression recognition via uni-hyperplane classification},
  booktitle = {Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference
	on},
  year = {2012},
  pages = {2554-2561},
  abstract = {Large margin learning approaches, such as support vector machines
	(SVM), have been successfully applied to numerous classification
	tasks, especially for automatic facial expression recognition. The
	risk of such approaches however, is their sensitivity to large margin
	losses due to the influence from noisy training examples and outliers
	which is a common problem in the area of affective computing (i.e.,
	manual coding at the frame level is tedious so coarse labels are
	normally assigned). In this paper, we leverage the relaxation of
	the parallel-hyperplanes constraint and propose the use of modified
	correlation filters (MCF). The MCF is similar in spirit to SVMs and
	correlation filters, but with the key difference of optimizing only
	a single hyperplane. We demonstrate the superiority of MCF over current
	techniques on a battery of experiments.},
  doi = {10.1109/CVPR.2012.6247973},
  issn = {1063-6919},
  keywords = {computational geometry;correlation methods;emotion recognition;face
	recognition;filtering theory;image classification;learning (artificial
	intelligence);support vector machines;MCF;SVM;affective computing;automatic
	facial expression recognition;classification tasks;large margin losses;learning
	approaches;modified correlation filters;noisy training examples;parallel-hyperplanes
	constraint;support vector machines;unihyperplane classification;Correlation;Databases;Face;Face
	recognition;Noise measurement;Support vector machines;Training}
}

@INPROCEEDINGS{Cho2007,
  author = {Heeryon Cho and Toru Ishida and Naomi Yamashita and Rieko Inaba and
	Yumiko Mori and Tomoko Koda},
  title = {Culturally-Situated Pictogram Retrieval},
  booktitle = {Proceedings of the 1st International Conference on Intercultural
	Collaboration},
  year = {2007},
  pages = {221-235},
  abstract = {This paper studies the patterns of cultural differences observed in
	pictogram interpretation. We conducted a 14-month online survey in
	the U.S. and Japan to ask the meaning of 120 pictograms used in a
	pictogram communication system. A total of 935 respondents in the
	U.S. and 543 respondents in Japan participated in the survey to submit
	pictogram interpretations which added up to compose an average of
	147 English interpretations and 97 Japanese interpretations per pictogram.
	Three human judges independently analyzed the English-Japanese pictogram
	interpretation words, and as a result, 19 pictograms were found to
	have culturally different interpretations by two or more judges.
	The following patterns of cultural differences in pictogram interpretation
	were observed: (1) two cultures share the same underlying concept,
	but have different perspectives on the concept, (2) two cultures
	only partially share the same underlying concept, and (3) two cultures
	do not share any common underlying concept.},
  keywords = {cross-cultural translation human-performance perception culture}
}

@ARTICLE{Choi2008,
  author = {Choi, Sukwon and Kim, Daijin},
  title = {Robust head tracking using 3{D} ellipsoidal head model in particle
	filter},
  journal = {Pattern Recognition},
  year = {2008},
  volume = {41},
  pages = {2901--2915},
  number = {9},
  abstract = {This paper proposes a real-time 3D head tracking method that can handle
	large rotation and translation. To achieve this goal, we incorporate
	the following three approaches into the particle filter. First, we
	take the 3D ellipsoidal head model to handle the large head rotation
	more effectively, especially the large rotation around the x-axis
	(pitch). Second, we take the online appearance model (OAM) that can
	adapt both the short-term and long-term appearance changes in the
	appearance model image effectively. Third, we take the adaptive state
	transition model to track the fast moving 3D heads, where the most
	plausible state for the next time is estimated by using the motion
	history model and the particles are distributed near the estimated
	state. This enables the real-time 3D head tracking by reducing the
	required number of particles greatly. The experimental results show
	that (1) the tracking accuracy of the 3D ellipsoidal head model is
	more precise than that of the 3D cylindrical head model by 15%, (2)
	the OAM provides more stable tracking than the wandering model, and
	(3) the adaptive state transition model can track faster moving heads
	than the zero-velocity model.},
  address = {New York, NY, USA},
  doizzz = {http://dx.doi.org/10.1016/j.patcog.2008.02.002},
  file = {Choi2008.pdf:Choi2008.pdf:PDF},
  issn = {0031-3203},
  keywords = {head pose tracker},
  owner = {ts00051},
  publisher = {Elsevier Science Inc.},
  timestamp = {2009.10.26}
}

@ARTICLE{Cohen2000,
  author = {Ira Cohen and Ashutosh Garg and Thomas S. Huang},
  title = {Emotion Recognition from Facial Expressions using Multilevel {HMM}},
  journal = {Neural Information Processing Systems (NIPS)},
  year = {2000},
  abstract = {Human-computer intelligent interaction (HCII) is an emerging field
	of science aimed at providing natural ways for humans to use computers
	as aids. It is argued that for the computer to be able to interact
	with humans, it needs to have the communication skills of humans.
	One of these skills is the ability to understand the emotional state
	of the person. The most expressive way humans display emotions is
	through facial expressions. This work focuses on automatic facial
	expression recognition from live video input using temporal cues.
	Methods for using temporal information have been extensively explored
	for speech recognition applications. Among these methods are template
	matching using dynamic programming methods and hidden Markov models
	(HMM). This work exploits existing methods and proposes a new architecture
	of HMMs for automatically segmenting and recognizing human facial
	expression from video sequences. The novelty of this architecture
	is that both segmentation and recognition of the facial expressions
	are done automatically using a multilevel HMM architecture while
	increasing the discrimination power between the different classes.
	In the work we explore person-dependent and person-independent recognition
	of expressions.},
  file = {:/home/ts00051/Dropbox/docs/papers/Cohen2000.pdf:PDF},
  keywords = {labels-emotion basic-emotions classification-hmms nvc-applications
	facial tracking au-features head-model},
  owner = {ts00051},
  timestamp = {2011.12.12}
}

@INPROCEEDINGS{Cohn2004b,
  author = {Cohn, J.F. and Reed, L.I. and Ambadar, Z. and Jing Xiao and Moriyama,
	T.},
  title = {Automatic analysis and recognition of brow actions and head motion
	in spontaneous facial behavior},
  booktitle = {Systems, Man and Cybernetics, 2004 IEEE International Conference
	on},
  year = {2004},
  volume = {1},
  pages = { 610 - 616 vol.1},
  month = {oct.},
  abstract = { Previous efforts in automatic facial expression recognition have
	been limited to posed facial behavior under well-controlled conditions
	(e.g., frontal pose and minimal out-of-plane head motion). The CMU/Pitt
	automated facial image analysis system (AFA) accommodates varied
	pose, moderate out-of-plane head motion, and occlusion. AFA was tested
	in video of two-person interviews originally collected to answer
	substantive questions in psychology, and represent a substantial
	challenge to automatic recognition of facial expression. This report
	focuses on two action units, brow raising and brow lowering because
	of their importance to emotion expression and paralinguistic communication.
	For two-state recognition, AFA achieved 89% accuracy. For three-state
	recognition (brow raising, brow lowering, and no brow action), accuracy
	was 76%. Brow and head motion were temporally coordinated. These
	findings demonstrate the feasibility of action unit recognition in
	spontaneous facial behavior.},
  doi = {10.1109/ICSMC.2004.1398367},
  issn = {1062-922X},
  keywords = { brow actions recognition; facial expression recognition; head motion;
	multimodal coordination; spontaneous facial behavior; emotion recognition;
	face recognition;}
}

@ARTICLE{Cohn2004,
  author = {Jeffrey Cohn and Karen Schmidt},
  title = {The Timing of Facial Motion in Posed and Spontaneous Smiles},
  journal = {International Journal of Wavelets, Multiresolution and Information
	Processing},
  year = {2004},
  volume = {2},
  pages = {1 - 12},
  month = {March},
  abstract = {Almost all work in automatic facial expression analysis has focused
	on recognition of prototypic expressions rather than dynamic changes
	in appearance over time. To investigate the relative contribution
	of dynamic features to expression recognition, we used automatic
	feature tracking to measure the relation between amplitude and duration
	of smile onsets in spontaneous and deliberate smiles of 81 young
	adults of Euro- and African-American background. Spontaneous smiles
	were of smaller amplitude and had a larger and more consistent relation
	between amplitude and duration than deliberate smiles. A linear discriminant
	classifier using timing and amplitude measures of smile onsets achieved
	a 93% recognition rate. Using timing measures alone, recognition
	rate declined only marginally to 89%. These findings suggest that
	by extracting and representing dynamic as well as morphological features,
	automatic facial expression analysis can begin to discriminate among
	the message values of morphologically similar expressions.},
  file = {Cohn2004.pdf:Cohn2004.pdf:PDF},
  keywords = {duchenne smiles temporal natural-important emotion natural-data posed-data
	nvc-difficult},
  review = {Spontaneous and deliberate smiles vary in time and intensity. For
	deliberate smiles, amplitude and duration are correlated, while in
	spontaneous smiles they are not.}
}

@INCOLLECTION{Condon1971,
  author = {Condon, W. S. and Ogston, W. D.},
  title = {Speech and body motion synchrony of the speaker-hearer},
  booktitle = {The Perception of Language},
  publisher = {Charles E. Merrill},
  year = {1971},
  editor = {Horton, D. L. and Jenkins, J. J.},
  pages = {224-56},
  address = {Colombus, Ohio},
  abstract = {The use of sound films and slow motion projection techniques provides
	a method for the storage and examination of a wide variety of subject
	matters. It can be used to provide a visual and auditory microscope
	of the study of behavioural process. Certain aspects of human, interactional
	behaviour have been examined by the present investigators at 24,
	48, 64, and, presently, at 125 frames per second. The present paper
	will indicate some of the technical, methodological and conceptual
	difficulties, the attempt at resolution, and some of the findings,
	particularly speech and body motion parallelism.},
  keywords = {NVC mutual annotation-difficulty},
  review = {Argyle et al says this paper finds eye blinks occur mostly at the
	beginning of words.},
  url = {http://www.eric.ed.gov/ERICWebPortal/contentdelivery/servlet/ERICServlet?accno=ED034425}
}

@ARTICLE{Cooper2010,
  author = {Cooper, Seth and Firas Khatib and Adrien Treuille and Janos Barbero
	and Jeehyung Lee and Michael Beenen and Andrew Leaver-Fay and David
	Baker and Zoran Popovi\'{c} and Foldit players},
  title = {Predicting protein structures with a multiplayer online game},
  journal = {Nature},
  year = {2010},
  volume = {466},
  pages = {756-760},
  keywords = {crowdsourcing annotation tools},
  owner = {ts00051},
  timestamp = {2012.01.13}
}

@INPROCEEDINGS{Cootes1992,
  author = {Cootes, T. and Taylor, C.},
  title = {Active Shape Models - Smart Snakes},
  booktitle = {Proceedings of the British Machine Vision Conference},
  year = {1992},
  pages = {266-275},
  address = {Leeds, UK},
  month = {September},
  publisher = {Springer Verlag},
  abstract = {We describe ctive Shape Models' which iteratively adapt to refine
	estimates of the pose, scale and shape of models of image objects.
	The method uses flexible models derived from sets of training examples.},
  file = {Cootes1992.pdf:Cootes1992.pdf:PDF},
  keywords = {tracking, contour, model fitting}
}

@ARTICLE{Cortes1995,
  author = {Corinna Cortes and Vladimir Vapnik},
  title = {Support-Vector Networks},
  journal = {Machine Learning},
  year = {1995},
  pages = {273-297},
  abstract = {The support-vector network is a new learning machine for two-group
	classification problems. The machine conceptually implements the
	following idea: input vectors are non-linearly mapped to a very high-dimension
	feature space. In this feature space a linear decision surface is
	constructed. Special properties of the decision surface ensures high
	generalization ability of the learning machine. The idea behind the
	supportvector network was previously implemented for the restricted
	case where the training data can be separated without errors. We
	here extend this result to non-separable training data.},
  file = {Cortes1995.pdf:Cortes1995.pdf:PDF},
  keywords = {svm machine learning supervised kernel seminal classification-svm}
}

@INPROCEEDINGS{Cosker2010,
  author = {Cosker, Darren and Krumhuber, Eva and Hilton, Adrian},
  title = {A FACS validated 3D human facial model},
  booktitle = {Proceedings of the SSPNET 2nd International Symposium on Facial Analysis
	and Animation},
  year = {2010},
  series = {FAA '10},
  pages = {12--12},
  address = {New York, NY, USA},
  publisher = {ACM},
  acmid = {1924039},
  doi = {10.1145/1924035.1924039},
  isbn = {978-1-4503-0388-0},
  location = {Edinburgh, United Kingdom},
  numpages = {1},
  url = {http://doi.acm.org/10.1145/1924035.1924039}
}

@ARTICLE{Cover1967,
  author = {Cover, Thomas M. and Hart, Peter E.},
  title = {Nearest neighbor pattern classification},
  journal = {IEEE Transactions on Information Theory},
  year = {1967},
  volume = {13},
  pages = {21-27},
  number = {1},
  file = {Cover1967.pdf:Cover1967.pdf:PDF},
  keywords = {machine-learning classification-nn classification-method},
  owner = {ts00051},
  timestamp = {2010.02.22}
}

@INBOOK{Cowie2009b,
  chapter = {Perceiving emotion: towards a realistic understanding of the task},
  pages = {3515-3525},
  title = {Philosophical Transactions of Royal Society B},
  publisher = {Royal Society},
  year = {2009},
  editor = {Robinson, Peter and el Kaliouby, Rana},
  author = {Cowie, Roddy},
  volume = {364},
  abstract = {A decade ago, perceiving emotion was generally equated with taking
	a sample (a still photograph or a few seconds of speech) that unquestionably
	signified an archetypal emotional state, and attaching the appropriate
	label. Computational research has shifted that paradigm in multiple
	ways. Concern with realism is key. Emotion generally colours ongoing
	action and interaction: describing that colouring is a different
	problem from categorizing brief episodes of relatively pure emotion.
	Multiple challenges flow from that. Describing emotional colouring
	is a challenge in itself. One approach is to use everyday categories
	describing states that are partly emotional and partly cognitive.
	Another approach is to use dimensions. Both approaches need ways
	to deal with gradual changes over time and mixed emotions. Attaching
	target descriptions to a sample poses problems of both procedure
	and validation. Cues are likely to be distributed both in time and
	across modalities, and key decisions may depend heavily on context.
	The usefulness of acted data is limited because it tends not to reproduce
	these features. By engaging with these challenging issues, research
	is not only achieving impressive results, but also offering a much
	deeper understanding of the problem.},
  file = {:Cowie2009b.pdf:PDF},
  keywords = {natural-important review-paper emotion recognition taxonomy nvc},
  owner = {ts00051},
  timestamp = {2010.01.07}
}

@INPROCEEDINGS{Cowie2008,
  author = {Cowie, Roddy},
  title = {Building the databases needed to understand rich, spontaneous human
	behaviour},
  booktitle = {Proceedings of the 8th IEEE International Conference on Automatic
	Face and Gesture Recognition},
  year = {2008},
  abstract = {One of the motives for studying faces and gestures is the role that
	they play in spontaneous, socially rich interaction between humans.
	If computers are to interact with humans in that mode (or to analyse
	what they are doing in it), methods of interpreting the non-verbal
	signals that they use are critical. It is becoming clear that developing
	those methods requires databases whose complexity is of a very different
	order from those that are standard elsewhere. Samples cannot be generated
	to order, because acting does not reproduce the way features are
	distributed in spontaneous action. Data collections need to be very
	large, because there are extensive situational, individual, and cultural
	differences. It is a very large task to provide annotations that
	adequately capture the meaning of facial expressions or gestures.
	These problems are not insoluble, but sustained and well-directed
	efforts are needed to solve them.},
  file = {Cowie2008.pdf:Cowie2008.pdf:PDF},
  keywords = {perception corpus emotion-encoding natural-important},
  optnote = {In press}
}

@ARTICLE{Cowie1999,
  author = {Cowie, R. and Douglas-Cowie, E. and Appolloni, B. and Taylor, J.
	and Romano, A. and and Fellenz, W},
  title = {What a neural net needs to know about emotion words},
  journal = {Computational Intelligence and Applications},
  year = {1999},
  pages = {109-114},
  note = {N. Mastorakis (Ed.), World Scientific \& Engineering Society Press},
  abstract = {We describe an empirical approach to identifying the kind of task
	that an emotion recognition system could usefully address. Three
	levels of information are elicited – a basic emotion vocabulary,
	a basic representation in ‘evaluation-activation space ’ of the meaning
	of each word, and a richer ‘schema’ representation. The results confirm
	that an approach of this kind is feasible.},
  file = {:Cowie1999.pdf:PDF},
  keywords = {nvc taxonomy nvc-applications nvc-annotation labels-emotion labels-abstract
	human-performance perception},
  owner = {ts00051},
  timestamp = {2011.08.09}
}

@ARTICLE{Cowie2005,
  author = {Cowie, R. and Douglas-Cowie E. and Cox C.},
  title = {Beyond emotion archetypes: Databases for emotion modelling using
	neural networks},
  journal = {Neural Networks},
  year = {2005},
  volume = {18},
  pages = {371-388},
  abstract = {There has been rapid development in conceptions of the kind of database
	that is needed for emotion research. Familiar archetypes are still
	influential, but the state of the art has moved beyond them. There
	is concern to capture emotion as it occurs in action and interaction
	('pervasive emotion') as well as in short episodes dominated by emotion,
	and therefore in a range of contexts, which shape the way it is expressed.
	Context links to modality-different contexts favour different modalities.
	The strategy of using acted data is not suited to those aims, and
	has been supplemented by work on both fully natural emotion and emotion
	induced by various technique that allow more controlled records.
	Applications for that kind of work go far beyond the 'trouble shooting'
	that has been the focus for application: 'really natural language
	processing' is a key goal. The descriptions included in such a database
	ideally cover quality, emotional content, emotion-related signals
	and signs, and context. Several schemes are emerging as candidates
	for describing pervasive emotion. The major contemporary databases
	are listed, emphasising those which are naturalistic or induced,
	multimodal, and influential.},
  file = {:Cowie2005.pdf:PDF},
  keywords = {natural-important corpus survey-paper posed-data natural-data situation-acted},
  owner = {ts00051},
  timestamp = {2011.08.09}
}

@INPROCEEDINGS{Cowie2009,
  author = {Cowie, Roddy and McKeown, Gary and Gibney, Ceire},
  title = {The challenges of dealing with distributed signs of emotion: theory
	and empirical evidence},
  booktitle = {Proceedings of the International Conference on Affective Computing
	\& Intelligent Interaction (ACII)},
  year = {2009},
  address = {Amsterdam},
  month = {Sept},
  abstract = {There clearly are important issues associated with the distinction
	between acted and naturalistic data, but focusing on acting may not
	be the best way to articulate them. An alternative is to focus on
	differences of structure which are often (but not always) associated
	with the distinction. Several such differences relate to the way
	signs are distributed: simultaneous or sequential appearance; in
	one modality or many; stable or unstable; suggesting one emotion
	or many; briefly or continually available. These distinctions clearly
	relate to different demands on analysis.},
  file = {Cowie2009.pdf:Cowie2009.pdf:PDF},
  keywords = {natural-important},
  owner = {ts00051},
  review = {Discusses differences in acted vs spontaneous emotion. Differences
	often occur in different contexts and different applications and
	this can be the greater effect.}
}

@INPROCEEDINGS{Cox2008,
  author = {Cox, Stephen and Richard Harvey and Yuxuan Lan and Jacob Newman and
	Barry-John Theobald},
  title = {The challenge of multispeaker lip-reading},
  booktitle = {Proceedings of the International Conference on Auditory-visual Speech
	Processing},
  year = {2008},
  pages = {179-184},
  keywords = {lipreading speechreading},
  owner = {ts00051},
  timestamp = {2012.01.09}
}

@ARTICLE{Cristinacce2008,
  author = {Cristinacce, David and Cootes, Tim},
  title = {Automatic feature localisation with constrained local models},
  journal = {Pattern Recogn.},
  year = {2008},
  volume = {41},
  pages = {3054--3067},
  number = {10},
  month = oct,
  acmid = {1385969},
  address = {New York, NY, USA},
  doi = {10.1016/j.patcog.2008.01.024},
  issn = {0031-3203},
  issue_date = {October, 2008},
  keywords = {Active appearance models, Constrained local models, Face detection,
	Feature detectors, Object detection, Object localisation, Shape modelling},
  numpages = {14},
  publisher = {Elsevier Science Inc.},
  url = {http://dx.doi.org/10.1016/j.patcog.2008.01.024}
}

@ARTICLE{DMello2007,
  author = {D'Mello, S. and Graesser, A. and Picard, R.W.},
  title = {Toward an Affect-Sensitive AutoTutor},
  journal = {Intelligent Systems, IEEE},
  year = {2007},
  volume = {22},
  pages = {53 -61},
  number = {4},
  month = {july-aug. },
  abstract = {Here, we consider the possibility of enabling AutoTutor, an intelligent
	tutoring system, to process learners' affective and cognitive states.
	AutoTutor is a fully automated computer tutor that simulates human
	tutors and converses with students in natural language.},
  doi = {10.1109/MIS.2007.79},
  issn = {1541-1672},
  keywords = {affect-sensitive AutoTutor;cognitive states;intelligent tutoring system;pedagogical
	strategies;intelligent tutoring systems;natural language interfaces;}
}

@ARTICLE{Darn2005,
  author = {Darn, Steve},
  title = {Aspects of Nonverbal Communication},
  journal = {Internet TESL Journal},
  year = {2005},
  volume = {11},
  number = {2},
  month = {February},
  keywords = {nvc taxonomy},
  owner = {tim},
  review = {Not peer reviewed?},
  timestamp = {2011.07.07}
}

@BOOK{Darwin2002,
  title = {The Expression of the Emotions in Man and Animals},
  publisher = {Oxford University Press},
  year = {2002},
  author = {Darwin, Charles},
  edition = {3rd},
  isbn = {0195158067},
  keywords = {emotion cross-cultural cross-species taxonomy emotion-definition},
  review = {He has this whole section on "Signs of affirmation or approval, and
	of negation or disapproval:
	
	nodding and shaking the head" comparing different cultures.}
}

@INPROCEEDINGS{Datcu2007,
  author = {Drago\c{s} Datcu and L\'{e}on Rothkrantz},
  title = {Facial expression recognition in still pictures and videos using
	active appearance models: a comparison approach},
  booktitle = {Proceedings of the International Conference on Computer Systems and
	Technologies},
  year = {2007},
  pages = {1--6},
  address = {New York, NY, USA},
  publisher = {ACM},
  abstract = {The paper highlights the performance of video sequence-oriented facial
	expression recognition using Active Appearance Model -- AAM, in a
	comparison with the analysis based on still pictures. The AAM is
	used to extract relevant information regarding the shapes of the
	faces to be analyzed. Specific key points from a Facial Characteristic
	Point - FCP model are used to derive the set of features. These features
	are used for the classification of the expressions of a new face
	sample into the prototypic emotions. The classification method uses
	Support Vector Machines.},
  doi = {http://doi.acm.org/10.1145/1330598.1330717},
  file = {Datcu2007.pdf:Datcu2007.pdf:PDF},
  isbn = {978-954-9641-50-9},
  keywords = {feature-generation aam-features tracker-features classification-svm
	basic-emotions labels-emotion heuristic distance variance temporal-important
	clip-statistics},
  location = {Bulgaria},
  review = {classification by SVM to exemplar expressions}
}

@ARTICLE{Davaninezhad2009,
  author = {Davaninezhad, Forogh Karimipur},
  title = {Cultural Aspects of Translation},
  journal = {Cross-Cultural Communication and Translation},
  year = {2009},
  volume = {13},
  number = {4},
  month = {October},
  abstract = {In this paper we first deal with concept of communication, especially
	non-verbal communication and its features in communicating a message
	when it happens between different cultures (cross-cultural non-verbal
	communication). Then, we show how culture affects all those features
	and see cultural differences as parameters that can hinder or even
	halt the communication process when moving from one language to another.
	Finally, we recommend translation as a reasonable way of communicating
	between different cultures and languages, and introduce two translation
	procedures.},
  keywords = {translation nvc cross-cultural},
  owner = {tim},
  review = {Not peer reviewed?},
  timestamp = {2011.07.07},
  url = {http://www.bokorlang.com/journal/50culture.htm}
}

@ARTICLE{Delaherche2012,
  author = {Delaherche, E. and Chetouani, M. and Mahdhaoui, A. and Saint-Georges,
	C. and Viaux, S. and Cohen, D.},
  title = {Interpersonal Synchrony : A Survey Of Evaluation Methods Across Disciplines},
  journal = {IEEE Transactions on Affective Computing},
  year = {2012},
  volume = {3},
  pages = {349- 365},
  number = {3},
  month = {July-September},
  category = {ACLI},
  doi = {10.1109/T-AFFC.2012.12},
  pdf = {http://www.isir.upmc.fr/files/2012ACLI2363.pdf}
}

@INCOLLECTION{DelBue2005,
  author = {A. {Del Bue} and X. Llad\'{o} and L. Agapito},
  title = {Non-rigid face modelling using shape priors},
  booktitle = {Proceedings of the IEEE International Workshop on Analysis and Modelling
	of Faces and Gestures},
  publisher = {Springer-Verlag},
  year = {2005},
  editor = {W. Zhao, S. Gong and X. Tang},
  volume = {3723},
  series = {Lecture Notes in Computer Science},
  pages = {96-107},
  abstract = {Non-rigid 3D shape recovery is an inherently ambiguous problem. Given
	a specific rigid motion, different non-rigid shapes can be found
	that fit the measurements. To solve this ambiguity prior knowledge
	on the shape and motion should be used to constrain the solution.
	This paper is based on the observation that often not all the points
	on a moving and deforming surface such as a human face are undergoing
	non-rigid motion. Some of the points are frequently on rigid parts
	of the structure – for instance the nose – while others lie on deformable
	areas. First we develop a segmentation algorithm to separate rigid
	and non-rigid motion. Once this segmentation is available, the rigid
	points can be used to estimate the overall rigid motion and to constrain
	the underlying mean shape. We propose two reconstruction algorithms
	and show that improved reconstructions can be obtained when the priors
	on the shape are used on synthetic and real data.},
  file = {:DelBue2005.pdf:PDF},
  isbn = {3-540-29229-2},
  keywords = {shape reconstruction}
}

@ARTICLE{Mendoza2008,
  author = {De Mendoza, Alejandro H.},
  title = {The Problem of Translation in Cross-Cultural Research on Emotion
	Concepts (Commentary on {Choi} \& {Han})},
  journal = {International Journal for Dialogical Science},
  year = {2008},
  volume = {3},
  pages = {241-248},
  number = {1},
  abstract = {Choi & Han conceive shimcheong as a specific Korean emotion concept
	that can be considered as an extended empathy embedded in Korean
	culture. This article analyzes from a conceptualist perspective the
	problems of translation in cross-cultural research about emotion
	concepts and the necessity of differentiating emotion concepts and
	emotional experience. Everyday concepts of emotions across languages
	and cultures are not mere tools for understanding emotions. They
	have to be considered as an object of study in itself.},
  file = {:Mendoza2008.pdf:PDF},
  keywords = {translation cross-cultural},
  owner = {tim},
  review = {"Everyday concepts of emotions across languages and cultures are not
	mere tools for understanding emotions. They have to be considered
	as an object of study in itself. In this article Choi & Han (2008)
	conceive shimcheong as a specific Korean emotion concept. From a
	realistic perspective, emotion words are mere labels that express
	subjacent biological entities and therefore, one-to-one translations
	are possible because they approximately express those universal entities."},
  timestamp = {2011.07.07}
}

@INCOLLECTION{Devillers2005,
  author = {Devillers, Laurence and Abrilian, Sarkis and Martin, Jean-Claude},
  title = {Representing real-life emotions in audiovisual data with non basic
	emotional patterns and context features},
  booktitle = {Affective Computing and Intelligent Interaction},
  publisher = {Springer},
  year = {2005},
  pages = {519--526}
}

@INPROCEEDINGS{Devillers2008,
  author = {Laurence Devillers and Jean-Claude Martin},
  title = {Coding Emotional Events in Audiovisual Corpora},
  booktitle = {Proceedings of the 6th International Conference on Language Resources
	and Evaluation (LREC)},
  year = {2008},
  address = {Marrakech, Morocco},
  month = {May},
  publisher = {European Language Resources Association (ELRA)},
  abstract = {The modelling of realistic emotional behaviour is needed for various
	applications in multimodal human-machine interaction such as the
	design of emotional conversational agents (Martin et al., 2005) or
	of emotional detection systems (Devillers and Vidrascu, 2007). Yet,
	building such models requires appropriate definition of various levels
	for representing the emotions themselves but also some contextual
	information such as the events that elicit these emotions. This paper
	presents a coding scheme that has been defined following annotations
	of a corpus of TV interviews (EmoTV). Deciding which events triggered
	or may trigger which emotion is a challenge for building efficient
	emotion eliciting protocols. In this paper, we present the protocol
	that we defined for collecting another corpus of spontaneous human-human
	interactions recorded in laboratory conditions (EmoTaboo). We discuss
	the events that we designed for eliciting emotions. Part of this
	scheme for coding emotional event is being included in the specifications
	that are currently defined by a working group of the W3C (the W3C
	Emotion Incubator Working group). This group is investigating the
	feasibility of working towards a standard representation of emotions
	and related states in technological contexts.},
  date = {28-30},
  file = {Devillers2008.pdf:Devillers2008.pdf:PDF},
  isbn = {2-9517408-4-0},
  keywords = {annotation emotion-encoding natural-data natural-important corpus-emotv
	corpus-announce},
  language = {english},
  opteditor = {Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard,
	Joseph Mariani, Jan Odjik, Stelios Piperidis, Daniel Tapias},
  optnote = {http://www.lrec-conf.org/proceedings/lrec2008/},
  owner = {ts00051},
  timestamp = {2009.10.26}
}

@INPROCEEDINGS{Dhall2011,
  author = {Dhall, A. and Asthana, A. and Goecke, R. and Gedeon, T.},
  title = {Emotion recognition using PHOG and LPQ features},
  booktitle = {Automatic Face Gesture Recognition and Workshops (FG 2011), 2011
	IEEE International Conference on},
  year = {2011},
  pages = {878-883},
  abstract = {We propose a method for automatic emotion recognition as part of the
	FERA 2011 competition. The system extracts pyramid of histogram of
	gradients (PHOG) and local phase quantisation (LPQ) features for
	encoding the shape and appearance information. For selecting the
	key frames, K-means clustering is applied to the normalised shape
	vectors derived from constraint local model (CLM) based face tracking
	on the image sequences. Shape vectors closest to the cluster centers
	are then used to extract the shape and appearance features. We demonstrate
	the results on the SSPNET GEMEP-FERA dataset. It comprises of both
	person specific and person independent partitions. For emotion classification
	we use support vector machine (SVM) and largest margin nearest neighbour
	(LMNN) and compare our results to the pre-computed FERA 2011 emotion
	challenge baseline.},
  doi = {10.1109/FG.2011.5771366},
  keywords = {emotion recognition;feature extraction;image classification;image
	sequences;pattern clustering;support vector machines;LPQ features;PHOG;SSPNET
	GEMEP-FERA dataset;appearance features extraction;automatic emotion
	recognition;constraint local model;emotion classification;face tracking;image
	sequences;k-means clustering;largest margin nearest neighbour;local
	phase quantisation features;pyramid of histogram of gradient extraction;shape
	vectors;support vector machine;Accuracy;Databases;Face;Feature extraction;Image
	sequences;Shape;Support vector machines}
}

@INCOLLECTION{Dietrich2001,
  author = {Dietrich, C. and Schwenker, F. and Palm, G.},
  title = {Classification of Time Series Utilizing Temporal and Decision Fusion},
  booktitle = {Multiple Classifier Systems},
  publisher = {Springer Berlin / Heidelberg},
  year = {2001},
  editor = {Kittler, Josef and Roli, Fabio},
  volume = {2096},
  series = {Lecture Notes in Computer Science},
  pages = {378-387},
  abstract = {In this paper we discuss classifier architectures to categorize time
	series. Three different architectures for the fusion of local classifier
	decisions are presented and applied to classify recordings of cricket
	songs. Different features from local time windows are extracted automatically
	from the waveform of the sound patterns. These features are used
	to classify the whole time series. We present results for all three
	classifier architectures on a data set of 28 different categories.},
  affiliation = {University of Ulm D-89069 Ulm Germany},
  isbn = {978-3-540-42284-6},
  keyword = {Computer Science},
  url = {http://dx.doi.org/10.1007/3-540-48219-9_38}
}

@ARTICLE{Dietterich1998,
  author = {Dietterich, Thomas G.},
  title = {Approximate statistical tests for comparing supervised classification
	learning algorithms},
  journal = {Neural Comput.},
  year = {1998},
  volume = {10},
  pages = {1895--1923},
  number = {7},
  month = oct,
  acmid = {303237},
  address = {Cambridge, MA, USA},
  doi = {10.1162/089976698300017197},
  file = {:Dietterich1998.pdf:PDF},
  issn = {0899-7667},
  issue_date = {Oct. 1998},
  keywords = {cross validation},
  numpages = {29},
  publisher = {MIT Press},
  url = {http://dx.doi.org/10.1162/089976698300017197}
}

@ARTICLE{Dirichlet1850,
  author = {Dirichlet, Gustav Lejeune},
  title = {\"Uber die Reduktion der positiven quadratischen Formen mit drei
	unbestimmten ganzen Zahle},
  journal = {Journal f\"{u}r die Reine und Angewandte Mathematik},
  year = {1850},
  volume = {40},
  pages = {209–227},
  owner = {ts00051},
  review = {Voronoi tessellation},
  timestamp = {2012.03.07}
}

@ARTICLE{Donato1999,
  author = {Gianluca Donato and Marian Stewart Bartlett and Joseph C. Hager and
	Paul Ekman and Terrence J. Sejnowski},
  title = {Classifying Facial Actions},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
  year = {1999},
  volume = {21},
  pages = {974--989},
  number = {10},
  abstract = {The facial action coding system (FACS) is an objective method for
	quantifying facial movement in terms of component actions. This paper
	explores and compares techniques for automatically recognizing facial
	actions in sequences of images. These techniques include: analysis
	of facial motion through estimation of optical flow; holistic spatial
	analysis, such as principal component analysis, independent component
	analysis, local feature analysis, and linear discriminant analysis;
	and methods based on the outputs of local filters, such as Gabor
	wavelet representations and local principal components. Performance
	of these systems is compared to naive and expert human subjects.
	Best performances were obtained using the Gabor wavelet representation
	and the independent component representation, both of which achieved
	96 percent accuracy for classifying 12 facial actions of the upper
	and lower face. The results provide converging evidence for the importance
	of using local filters, high spatial frequencies, and statistical
	independence for classifying facial actions},
  address = {Washington, DC, USA},
  doi = {http://dx.doi.org/10.1109/34.799905},
  file = {Donato1999.pdf:Donato1999.pdf:PDF},
  issn = {0162-8828},
  keywords = {posed-data feature-generation optical-flow-features classification-nn
	texture-feature human-performance facial multiannotator},
  publisher = {IEEE Computer Society},
  review = {Various methods to classify 12 FACS actions including optical flow,
	PCA and Gabor wavelet analysis on ROI. Data set was 150 subjects
	performing various FACS actions. A series of 6 images at various
	action intensities was generated. Image alignment was performed manually.}
}

@INPROCEEDINGS{Dornaika2005,
  author = {Fadi Dornaika and Franck Davoine},
  title = {Simultaneous Facial Action Tracking and Expression Recognition Using
	a Particle Filter},
  booktitle = {Proceedings of the 10th IEEE International Conference on Computer
	Vision},
  year = {2005},
  pages = {1733--1738},
  address = {Washington, DC, USA},
  publisher = {IEEE Computer Society},
  abstract = {The recognition of facial gestures and expressions in image sequences
	is an important and challenging problem. Most of the existing methods
	adopt the following paradigm. First, facial actions/features are
	retrieved from the images, and then facial expressions are recognized
	based on the retrieved temporal parameters. Unlike this main strewn,
	this paper introduces a new approach allowing the simultaneous recovery
	of facial actions and expression using a particle filter adopting,
	multiclass dynamics that are conditioned on the expression. For each
	frame in the video sequence, our approach is split in two consecutive
	stages. In the first stage, the 3D head pose is recovered using a
	deterministic registration technique based on online appearance models.
	In the second stage, the facial actions as well as the facial expression
	are simultaneously recovered using the stochastic framework with,
	mixed states. The proposed fast scheme is either as robust as existing
	ones or more robust with respect to many regards. Experimental results
	show the feasibility and robustness of the proposed approach},
  doi = {http://dx.doi.org/10.1109/ICCV.2005.225},
  file = {Dornaika2005.pdf:Dornaika2005.pdf:PDF},
  isbn = {0-7695-2334-X-02},
  keywords = {emotion recognition face-normalisation head-model head-pose labels-facs
	labels-emotion texture-feature feature-generation},
  review = {Face tracking and expression recognition. For each frame, first step:
	3D head pose is found using a technique similar to Online Appearance
	Models. Second step: the expression recognition, from seven exemplar
	expressions, is based on a particle filter model of previous frame(s)
	expressions.}
}

@ARTICLE{DouglasCowie2003,
  author = {Ellen Douglas-Cowie and Nick Campbell and Roddy Cowie and Peter Roach},
  title = {Emotional speech: Towards a new generation of databases},
  journal = {Speech Communication},
  year = {2003},
  pages = {33-60},
  abstract = {Research on speech and emotion is moving from a period of exploratory
	research into one where there is a prospect of substantial applications,
	notably in human–computer interaction. Progress in the area relies
	heavily on the development of appropriate databases. This paper addresses
	four main issues that need to be considered in developing databases
	of emotional speech: scope, naturalness, context and descriptors.
	The state of the art is reviewed. A good deal has been done to address
	the key issues, but there is still a long way to go. The paper shows
	how the challenge of developing appropriate databases is being addressed
	in three major recent projects––the Reading–Leeds project, the Belfast
	project and the CREST–ESPproject. From these and other studies the
	paper draws together the tools and methods that have been developed,
	addresses the problems that arise and indicates the future directions
	for the development of emotional speech databases.},
  file = {:DouglasCowie2003.pdf:PDF},
  keywords = {belfast naturalistic corpus natural-data survey-paper natural-important
	corpus-announce},
  owner = {ts00051},
  timestamp = {2011.12.19}
}

@ARTICLE{Drucker1997,
  author = {Drucker, Harris and Burges, Chris J.C. and Kaufman, Linda and Smola,
	Alex and Vapnik, Vladimir},
  title = {Support Vector Regression Machines},
  journal = {Advances in Neural Information Processing Systems},
  year = {1997},
  pages = {155-161},
  note = {MIT Press},
  abstract = {A new regression technique based on Vapnik’s concept of support
	
	vectors is introduced. We compare support vector regression (SVR)
	
	with a committee regression technique (bagging) based on regression
	
	trees and ridge regression done in feature space. On the basis of
	these
	
	experiments, it is expected that SVR will have advantages in high
	
	dimensionality space because SVR optimization does not depend on the
	
	dimensionality of the input space.},
  file = {Drucker1997.pdf:Drucker1997.pdf:PDF},
  keywords = {svr svm regression machine learning supervised classification-svm},
  owner = {ts00051},
  timestamp = {2010.02.22}
}

@INPROCEEDINGS{Duan2005,
  author = {Duan, Kai-Bo and Keerthi Sathiya, S.},
  title = {Which Is the Best Multiclass {SVM} Method? An Empirical Study},
  booktitle = {Proceedings of the Sixth International Workshop on Multiple Classifier
	Systems},
  year = {2005},
  abstract = {Multiclass SVMs are usually implemented by combining several two-class
	SVMs. The one-versus-all method using winner-takes-all strategy and
	the one-versus-one method implemented by max-wins voting are popularly
	used for this purpose. In this paper we give empirical evidence to
	show that these methods are inferior to another one-versusone method:
	one that uses Platt’s posterior probabilities together with the pairwise
	coupling idea of Hastie and Tibshirani. The evidence is particularly
	strong when the training dataset is sparse.},
  keywords = {svm machine learning multiclass supervised learning classification-svm},
  owner = {ts00051},
  timestamp = {2011.12.22}
}

@ARTICLE{Dunn2011,
  author = {Michael Dunn and Simon J. Greenhill and Stephen C. Levinson and Russell
	D. Gray},
  title = {Evolved structure of language shows lineage-specific trends in word-order
	universals},
  journal = {Nature},
  year = {2011},
  abstract = {Languages vary widely but not without limit. The central goal of linguistics
	is to describe the diversity of human languages and explain the constraints
	on that diversity. Generative linguists following Chomsky have claimed
	that linguistic diversity must be constrained by innate parameters
	that are set as a child learns a language1,2. In contrast, other
	linguists following Greenberg have claimed that there are statistical
	tendencies for co-occurrence of traits reflecting universal systems
	biases3–5, rather than absolute constraints or parametric variation.
	Here we use computational phylogenetic methods to address the nature
	of constraints on linguistic diversity in an evolutionary framework6.
	First, contrary to the generative account of parameter setting, we
	show that the evolution of only a few word-order features of languages
	are strongly correlated. Second, contrary to the Greenbergian generalizations,
	we show that most observed functional dependencies between traits
	are lineage-specific rather than universal tendencies. These findings
	support the view that—at least with respect to word order—cultural
	evolution is the primary factor that determines linguistic structure,
	with the current state of a linguistic system shaping and constraining
	future states.},
  doi = {10.1038/nature09923},
  file = {:Dunn2011.pdf:PDF},
  keywords = {culture cross-cultural language},
  review = {Unsure what significance this has?},
  url = {http://www.isrl.uiuc.edu/~amag/langev/paper/Dunn2011EvolvedStructureNATURE.html}
}

@ARTICLE{Eckhardt2009b,
  author = {Micah Eckhardt and Ian R. Fasel and Javier R. Movellan},
  title = {Towards Practical Facial Feature Detection},
  journal = {IJPRAI},
  year = {2009},
  pages = {379-400}
}

@INPROCEEDINGS{Eckhardt2009,
  author = {Eckhardt, Micah and Picard, Rosalind},
  title = {A More Effective Way to Label Affective Expressions},
  booktitle = {Proceedings of the International Conference on Affective Computing
	\& Intelligent Interaction (ACII)},
  year = {2009},
  address = {Amsterdam},
  month = {Sept},
  abstract = {Labeling videos for affect content such as facial expression is tedious
	and time consuming. Researchers often spend significant amounts of
	time annotating experimental data, or simply lack the time required
	to label their data. For these reasons we have developed VidL, an
	open source video labeling system that is able to harness the distributed
	people-power of the internet. Through centralized management VidL
	can be used to manage data, custom label videos, manage workers,
	visualize labels, and review coders work. As an example, we recently
	labeled 700 short videos, approximately 60 hours of work, in 2 days
	using 20 labelers working from their own computers.},
  file = {Eckhardt2009.pdf:Eckhardt2009.pdf:PDF},
  keywords = {nvc annotation annotation-tool emotion perception},
  owner = {ts00051},
  review = {Web software for distributed annotation of video clips.}
}

@INPROCEEDINGS{Cootes1998,
  author = {G. J. Edwards and C. J. Taylor and T. F. Cootes},
  title = {Interpreting Face Images Using Active Appearance Models},
  booktitle = {Proceedings of the 3rd IEEE International Conference on Automatic
	Face and Gesture Recognition},
  year = {1998},
  pages = {300},
  address = {Washington, DC, USA},
  publisher = {IEEE Computer Society},
  abstract = {We demonstrate a fast, robust method of interpreting face images using
	an Active Appearance Model (AAM). An AAM contains a statistical model
	of shape and grey level appearance which can generalise to almost
	any face. Matching to an image involves finding model parameters
	which minimise the difference between the image and a synthesised
	face. We observe that displacing each model parameter from the correct
	value induces a particular pattern in the residuals. In a training
	phase, the AAM learns a linear model of the correlation between parameter
	displacements and the induced residuals. During search it measures
	the residuals and uses this model to correct the current parameters,
	leading to a better fit. A good overall match is obtained in a few
	iterations, even from poor starting estimates. We describe the technique
	in detail and show it matching to new face images},
  file = {Cootes1998.pdf:Cootes1998.pdf:PDF},
  isbn = {0-8186-8344-9},
  keywords = {aams active_appearance model fitting feature_generation facial amm_features},
  review = {Face tracking but overlaying an active shape modeling. The model geometry
	is constrained by face training data. For video, the current shape
	has various hypothysises for new positions based on perturbation
	of shape parameters. The best match is then iteratively optimized
	to the observed face. Matching is done on intensity only.}
}

@ARTICLE{Efron1997,
  author = {Efron, Bradley and Tibshirani, Robert},
  title = {Improvements on cross-validation: the .632+ bootstrap method},
  journal = {J. Amer. Statist. Assoc.},
  year = {1997},
  volume = {92},
  pages = {548–560},
  number = {438},
  keywords = {cross validation},
  owner = {tim},
  timestamp = {2012.10.19}
}

@BOOK{Eggins1997,
  title = {Analysing Casual Conversation},
  publisher = {Equinox Publishing Ltd, London},
  year = {1997},
  author = {Eggins, Suzanne and Diana Slade},
  keywords = {informal chat context social-situation situation-informal},
  owner = {ts00051},
  review = {"We experience casual conversation as probably the only context in
	which we are talking in a relaxed, spontaneous and unselfconscious
	way." p.17
	
	
	"We will define casual conversation functionally and, initially at
	least, negatively, as talk which is NOT motivated by any clear pragmatic
	purpose." p.19},
  timestamp = {2011.12.13}
}

@INPROCEEDINGS{Ekman1972,
  author = {Ekman, Paul},
  title = {Universals and Cultural Differences in Facial Expressions of Emotion},
  booktitle = {Proceedings of the Nebraska Symposium on Motivation},
  year = {1972},
  editor = {J. Cole},
  volume = {19},
  pages = {207-282},
  publisher = {University of Nebraska Press},
  file = {:/home/ts00051/Dropbox/docs/papers/Ekman1972.pdf:PDF},
  keywords = {cross-cultural emotion NVC seminal basic-emotions},
  owner = {ts00051},
  timestamp = {2011.08.09}
}

@INCOLLECTION{Ekman99,
  author = {Paul Ekman},
  title = {Basic Emotions},
  booktitle = {Handbook of Cognition and Emotion},
  publisher = {Wiley},
  year = {1999},
  editor = {T. Dalgleish and M. Power},
  address = {Chichester, UK},
  file = {Ekman99.pdf:Ekman99.pdf:PDF},
  keywords = {basic-emotions abstract-emotion-scales emotion-encoding psychology
	cross-cultural},
  review = {Overview of evidence and its criticisms of the cross cultural universality
	of emotion and associated expression. The author concludes there
	is strong evidence that common emotions are consistently linked with
	expressions although these can be consciously suppressed or modified
	by social situation. This points toward an evolutionary basis for
	emotions.}
}

@INBOOK{Ekman1999,
  chapter = {Emotional And Conversational Nonverbal Signals},
  pages = {45-55},
  title = {Gesture, Speech and Sign},
  year = {1999},
  editor = {Messing, L. and Campbell, R.},
  author = {Ekman, P.},
  owner = {ts00051},
  timestamp = {2012.05.30}
}

@INBOOK{Ekman1979,
  chapter = {Emotional and conversational nonverbal signals},
  pages = {45-55},
  title = {Gesture, Speech, and Sign},
  publisher = {Oxford University Press},
  year = {1979},
  editor = {Messing, L.S. and Campbell, R.},
  author = {Ekman, P.},
  address = {Oxford},
  owner = {ts00051},
  timestamp = {2012.05.30}
}

@INBOOK{Ekman2009,
  chapter = {Darwin's contributions to our understanding of emotional expressions},
  pages = {3449-3451},
  title = {Philosophical Transactions of Royal Society B},
  publisher = {Royal Society},
  year = {2009},
  editor = {Robinson, Peter and el Kaliouby, Rana},
  author = {Ekman, Paul},
  volume = {364},
  abstract = {Darwin charted the field of emotional expressions with five major
	contributions. Possible explanations of why he was able to make such
	important and lasting contributions are proposed. A few of the important
	questions that he did not consider are described. Two of those questions
	have been answered at least in part; one remains a major gap in our
	understanding of emotion.},
  file = {Ekman2009.pdf:Ekman2009.pdf:PDF},
  keywords = {review-paper darwin emotion cross-cultural facs cross-species taxonomy},
  owner = {ts00051},
  timestamp = {2010.01.07}
}

@INBOOK{Ekman1984,
  chapter = {Expression And The Nature Of Emotion},
  pages = {319-344},
  title = {Approaches to Emotion},
  publisher = {Hillsdale New Jersey: Lawrence Erlbaum},
  year = {1984},
  editor = {Scherer, K. and Ekman, P.},
  author = {Ekman, P.},
  owner = {tim},
  timestamp = {2012.02.21}
}

@BOOK{Ekman1978,
  title = {Facial Action Coding System: A Technique for the Measurement of Facial
	Movement},
  publisher = {Consulting Psychologists Press},
  year = {1978},
  author = {Ekman, P. and Friesen, W. V.},
  address = {Palo Alto},
  keywords = {encoding labels NVC facial expression}
}

@BOOK{Ekman1975,
  title = {Unmasking the Face: A Guide to Recognizing Emotions from Facial Expressions},
  publisher = {Prentice-Hall},
  year = {1975},
  author = {Ekman, P. and Friesen, W. V.},
  address = {Englewood Cliffs, N.J.},
  keywords = {micro-expressions expression basic-emotions},
  owner = {tim},
  review = {Introduces micro-expressions but more as a statement of fact rather
	than based on experimental work.},
  timestamp = {2011.11.08}
}

@ARTICLE{Ekman1969,
  author = {Ekman, P. and Friesen, W. V.},
  title = {The Repertoire of Nonverbal Behavior: Categories, Origins, Usage,
	and Coding},
  journal = {Semiotica},
  year = {1969},
  volume = {1},
  pages = {48–98},
  file = {:Ekman1969.pdf:PDF},
  keywords = {translation NVC cross-cultural},
  owner = {tim},
  review = {"Translatable (also called "emblem") non-verbal behaviour consists
	of specific actions with known meanings, such as some gestures."
	
	
	Splits NVC into origin, usage and coding},
  timestamp = {2011.07.07}
}

@ARTICLE{Elfenbein2002,
  author = {Elfenbein, H. A. and Ambady, N.},
  title = {Is There an In-Group Advantage in Emotion Recognition?},
  journal = {Psychological Bulletin},
  year = {2002},
  volume = {128},
  pages = {243-249},
  number = {2},
  abstract = {H. A. Elfenbein and N. Ambady (2002) examined the evidence for an
	in-group advantage in emotion recognition, whereby recognition is
	generally more accurate for perceivers from the same cultural group
	as emotional expressors. D. Matsumoto's (2002) comment centered on
	3 asserted methodological requirements. This response addresses the
	lack of consensus conceming these "requirements" and demonstrates
	that none alter the presence of the in-group advantage. His analyses
	had a serious flaw and, once corrected, replicated the original findings.
	Furthermore, he described results from his empirical work not meeting
	a literal interpretation of his own requirements. Overall, where
	Matsumoto considers subtle cross-cultural differences in emotional
	expression a methodological artifact in judgment studies, the present
	authors find a core phenomenon worthy of attention.},
  file = {:Elfenbein2002.pdf:PDF},
  keywords = {perception in-group cross-cultural emotion},
  owner = {ts00051},
  timestamp = {2012.01.16}
}

@ARTICLE{Elfenbein2002b,
  author = {Elfenbein, H. A. and Ambady, N.},
  title = {On the universality and cultural specificity of emotion recognition:
	a meta-analysis.},
  journal = {Psychological Bulletin},
  year = {2002},
  volume = {128},
  pages = {203-35},
  number = {2},
  month = {Mar},
  abstract = {A meta-analysis examined emotion recognition within and across cultures.
	Emotions were universally recognized at better-than-chance levels.
	Accuracy was higher when emotions were both expressed and recognized
	by members of the same national, ethnic, or regional group, suggesting
	an in-group advantage. This advantage was smaller for cultural groups
	with greater exposure to one another, measured in terms of living
	in the same nation, physical proximity, and telephone communication.
	Majority group members were poorer at judging minority group members
	than the reverse. Cross-cultural accuracy was lower in studies that
	used a balanced research design, and higher in studies that used
	imitation rather than posed or spontaneous emotional expressions.
	Attributes of study design appeared not to moderate the size of the
	in-group advantage.},
  owner = {ts00051},
  timestamp = {2012.03.02}
}

@TECHREPORT{Elmenreich2002,
  author = {Elmenreich, W.},
  title = {An Introduction to Sensor Fusion},
  institution = {Institut for Technische. Informatik, Vienna University of Technology},
  year = {2002},
  address = {Austria},
  note = {Research Report 47/2001},
  owner = {tim},
  timestamp = {2012.10.25}
}

@INPROCEEDINGS{Engle1998,
  author = {Engle, R. A.},
  title = {Not channels but composite signals: Speech, gesture, diagrams and
	object demonstrations are integrated in multimodal explanations},
  booktitle = {Proceedings of the Twentieth Annual Conference of the Cognitive Science
	Society},
  year = {1998},
  editor = {M.A. Gernsbacher and S.J. Derry},
  address = {Mahwah, NJ},
  publisher = {Erlbaum},
  owner = {tim},
  timestamp = {2013.03.30}
}

@INPROCEEDINGS{Escalera2009,
  author = {Escalera, Sergio and Rosa M. Mart\'{i}nez and Jordi Vitri\`{a} and
	Petia Radeva and Teresa Anguera},
  title = {Dominance Detection in Face-to-face Conversations},
  booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision
	and Pattern Recognition Workshops},
  year = {2009},
  pages = {97-102},
  file = {Escalera2009.pdf:Escalera2009.pdf:PDF},
  keywords = {labels-dominance dyadic multiannotator feature-generation detection-features
	tracker-features nvc heuristic classification-boost},
  owner = {ts00051},
  timestamp = {2010.01.18}
}

@INPROCEEDINGS{Fanelli2010,
  author = {G. Fanelli and A.Yao and P.-L. Noel and J. Gall and L. Van Gool},
  title = {Hough Forest-Based Facial Expression Recognition from Video Sequences},
  booktitle = {Proceedings of the International Workshop on Sign, Gesture and Activity
	(SGA)},
  year = {2010},
  month = {September},
  abstract = {Automatic recognition of facial expression is a necessary step toward
	the design of more natural human-computer interaction systems. This
	work presents a user-independent approach for the recognition of
	facial expressions from image sequences. The faces are normalized
	in scale and rotation based on the eye centers’ locations into tracks
	from which we extract features representing shape and motion. Classification
	and localization of the center of the expression in the video sequences
	are performed using a Hough transform voting method based on randomized
	forests. We tested our approach on two publicly available databases
	and achieved encouraging results comparable to the state of the art.},
  file = {:Fanelli2010.pdf:PDF},
  keywords = {Facial expression recognition, generalised Hough transform basic-emotions
	texture-feature corpus-cohn-kanade corpus-mmi classification-tree
	feature-generation}
}

@ARTICLE{Fanelli2010b,
  author = {Fanelli, G. and Gall, J. and Romsdorfer, H. and Weise, T. and Van
	Gool, L.},
  title = {A 3-{D} Audio-Visual Corpus of Affective Communication},
  journal = {IEEE Transactions on Multimedia},
  year = {2010},
  volume = {12},
  pages = {591--598},
  number = {6},
  abstract = {Communication between humans deeply relies on the capability of expressing
	and recognizing feelings. For this reason, research on human-machine
	interaction needs to focus on the recognition and simulation of emotional
	states, prerequisite of which is the collection of affective corpora.
	Currently available datasets still represent a bottleneck for the
	difficulties arising during the acquisition and labeling of affective
	data. In this work, we present a new audio-visual corpus for possibly
	the two most important modalities used by humans to communicate their
	emotional states, namely speech and facial expression in the form
	
	of dense dynamic 3-D face geometries. We acquire high-quality data
	by working in a controlled environment and resort to video clips
	to induce affective states. The annotation of the speech signal includes:
	transcription of the corpus text into the phonological representation,
	accurate phone segmentation, fundamental frequency extraction, and
	signal intensity estimation of the speech signals. We employ a real-time
	3-D scanner to acquire dense dynamic facial geometries and track
	the faces throughout the sequences, achieving full spatial and temporal
	correspondences. The corpus is a valuable tool for applications like
	affective visual speech synthesis or view-independent facial expression
	recognition.},
  file = {:Fanelli2010b.pdf:PDF},
  keywords = {corpus labels-emblem posed-data corpus-announce},
  publisher = {IEEE},
  review = {Posed 3D emotion capture},
  url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5571821}
}

@INPROCEEDINGS{Fang2009,
  author = {H. Fang and N. P. Costen},
  title = {From Rank-{N} to Rank-1 Face Recognition Based on Motion Similarity.},
  booktitle = {Proceedings of the British Machine Vision Conference},
  year = {2009},
  abstract = {In this paper, we present a sequential framework using facial motion
	information as a subsidiary to improve face recognition performance.
	As is generally known, reasonable static face recognition has been
	achieved based on subspace reduction techniques. In order to further
	improve performance, some extra cues, such as temporal variation,
	are investigated by building dynamic models. We propose a permuted
	similarity motion feature and integrate it into a sequential recognition
	system. This system can select the best candidate from the Rank-N
	candidates picked up in the recognition step based on static appearance
	parameters by using motion information. The recognition rate of the
	motion similarity is compared with the motion feature obtained from
	auto-regressive models to prove its efficiency. In addition, the
	sequential system achieves better performance when the motion information
	is integrated with the static appearance information in a flexible
	manner.},
  file = {Fang2009.pdf:Fang2009.pdf:PDF},
  keywords = {identity face alignment aam-features clip-statistics corpus-banca
	corpus-manchester-university labels-identity},
  owner = {ts00051},
  timestamp = {2009.10.26}
}

@ARTICLE{Fasel2003,
  author = {Fasel, B. and Luettin, J.},
  title = {Automatic Facial Expression Analysis: A Survey},
  journal = {Pattern Recognition},
  year = {2003},
  volume = {36},
  pages = {259--275},
  number = {1},
  abstract = {Over the last decade, automatic facial expression analysis has become
	an active research area that finds potential applications in areas
	such as more engaging human–computer interfaces, talking heads, image
	retrieval and human emotion analysis. Facial expressions reflect
	not only emotions, but other mental activities, social interaction
	and physiological signals. In this survey, we introduce the most
	prominent automatic facial expression analysis methods and systems
	presented in the literature. Facial motion and deformation extraction
	approaches as well as classification methods are discussed with respect
	to issues such as face normalization, facial expression dynamics
	and facial expression intensity, but also with regard to their robustness
	towards environmental changes.},
  file = {Fasel2003.ps.gz:Fasel2003.ps.gz:PostScript},
  keywords = {survey-paper emotion-definition annotation human-performance feature-generation},
  owner = {ts00051},
  review = {Distinguishes between facial expression recognition and emotion recognition.},
  timestamp = {2009.11.18}
}

@INPROCEEDINGS{Fasel2000,
  author = {Fasel, B. and Luettin, J.},
  title = {Recognition of asymmetric facial action unit activities and intensities},
  booktitle = {Pattern Recognition, 2000. Proceedings. 15th International Conference
	on},
  year = {2000},
  volume = {1},
  pages = {1100-1103 vol.1},
  abstract = {Most automatic facial expression analysis systems try to analyze emotion
	categories. However, psychologists argue that there is no straightforward
	way to classify, emotions from facial expressions. Instead, they
	propose FACS (facial action coding system), a de-facto standard for
	categorizing facial actions independent from emotional categories.
	We describe a system that recognizes asymmetric FACS action unit
	activities and intensities without the use of markers. Facial expression
	extraction is achieved by difference images that are projected into
	a sub-space using either PCA or ICA, followed by nearest neighbor
	classification. Experiments show that this holistic approach achieves
	a recognition performance comparable to marker-based facial expression
	analysis systems or human FACS experts for a single-subject database
	recorded under controlled conditions},
  doi = {10.1109/ICPR.2000.905664},
  issn = {1051-4651},
  keywords = {face recognition;image classification;image sequences;principal component
	analysis;asymmetric facial action unit;difference images;facial action
	coding system;facial expression extraction;holistic approach;nearest
	neighbor classification;Displays;Face recognition;Facial features;Gold;Humans;Image
	coding;Image motion analysis;Optical devices;Principal component
	analysis;Testing}
}

@ARTICLE{Fehr1984,
  author = {Fehr, B. and J. Russell},
  title = {Concept of Emotion Viewed from a Prototype Perspective},
  journal = {Journal of Experimental Psychology: General},
  year = {1984},
  volume = {113},
  pages = {464},
  owner = {ts00051},
  review = {Quote "Everyone knows what an emotion is, until asked to give a definition"},
  timestamp = {2012.10.22}
}

@INBOOK{Feldman1991,
  chapter = {Social competence and nonverbal behavior},
  pages = {329-350},
  title = {Fundamentals of nonverbal behavior},
  publisher = {Cambridge University Press},
  year = {1991},
  editor = {Robert S. Feldman and Bernard Rime},
  author = {Feldman, Robert Stephen and Philippot, Pierre and Custrini, Robert
	J.},
  type = { Book; Book/Illustrated },
  note = { Includes bibliographical references and indexes },
  catalogue-url = { http://trove.nla.gov.au/work/6334888 },
  contents = { Neuropsychology of facial expression / William E. Rinn -- Brain pathology,
	lateralization, and nonverbal behavior / Pierre Feyereisen -- The
	development of facial expressions in infancy / Linda A. Camras, Carol
	Malatesta, and Carroll E. Izard -- Toward an ecology of expressiveness
	/ Amy G. Halberstadt -- Facial expression / Paul Ekman and Maureen
	O'Sullivan -- Voice and emotion / Arvid Kappas, Ursula Hess, and
	Klaus R. Scherer -- Gesture and speech / Bernard Rimé and Loris
	Schiaratura -- Expressiveness as an individual difference / Antony
	S. R. Manstead -- Social competence and nonverbal behavior / Robert
	S. Feldman, Pierre Philippot, and Robert J. Custrini -- Nonverbal
	behavior and self-presentation / Bella M. Depaulo -- Interpersonal
	coordination / Frank J. Bernieri and Robert Rosenthal -- Symbolic
	nonverbal behavior / Pio Enrico Ricci Bitti and Isabella Poggi --
	A functional approach to nonverbal exchange / Miles L. Patterson
	},
  isbn = { 0521363888 (hardback) },
  language = { English },
  subjects = { Body language; Pointing (Gesture); Nonverbal communication }
}

@INPROCEEDINGS{Feng2005,
  author = {Xiaoyi Feng and Jie Cui and Matti Pietik\"{a}inen and Abdenour Hadid},
  title = {Real Time Facial Expression Recognition Using Local Binary Patterns
	and Linear Programming},
  booktitle = {Proceedings of the Mexican International Conference on Artificial
	Intelligence},
  year = {2005},
  pages = {328-336},
  publisher = {Springer Berlin / Heidelberg},
  abstract = {In this paper, a fully automatic, real-time system is proposed to
	recognize seven basic facial expressions (angry, disgust, fear, happiness,
	neutral, sadness and surprise). First, faces are located and normalized
	based on an illumination insensitive skin model and face segmentation;
	then, the Local Binary Patterns (LBP) techniques, which are invariant
	to monotonic grey level changes, are used for facial feature extraction;
	finally, the Linear Programming (LP) technique is employed to classify
	seven facial expressions. Theoretical analysis and experimental results
	show that the proposed system performs well in some degree of illumination
	changes and head rotations.},
  file = {:Feng2005.pdf:PDF},
  keywords = {basic-emotions emotion recognition lbp-features feature-generation
	labels-emotion alignment classification-linear-programming}
}

@INBOOK{Fernandez-Dols2003,
  chapter = {Emotion, affect and mood in social judgments},
  pages = {283-298},
  title = {Handbook of Psychology. Volume Five: Personality and Social Psychology},
  publisher = {New York: Wiley},
  year = {2003},
  editor = {T. Millon and M. J. Lerner},
  author = {Fernandez-Dols, J. M. and Russell, J.A.},
  abstract = {We first outline researchers' different implicit philosophical positions
	on the issue of the relation between language and emotion. Does language
	accurately describe emotion? Influence emotion? Constitute emotion?
	Or does everyday language conceal and obscure emotion?
	
	
	One position, that we call “ontological realism” assumes that emotions
	are pre-existing entities and words such as “anger” or “sadness”
	are simply labels for these entities. A second position, that we
	call Nominalism, assumes that emotion and language about emotion
	are a same cultural product. A third position, that we call Conceptualism,
	assumes that emotions are pre-existing entities but words like “anger”
	or “sadness” are concepts rather than labels for these entities.
	A fourth position, that we call Formalism, assumes that language
	about emotion consists of some universal semantic primitives.
	
	
	An alternative to “emotion” is “affect”, a concept that is mostly
	related to the pleasure-arousal theory of emotion but has also been
	independently developed as a concept with broad applications in social
	psychology. In this chapter affect is considered as the basic, elemental
	component of emotion in an emerging paradigm that does not relegate
	emotion to a particular philosophical position. This paradigm includes
	various psychological mechanisms—including attribution, automatic
	and controlled processing, affect as information, appraisal, and
	core affect—that underlie specific emotional episodes.},
  keywords = {translation instruments nvc cross-cultural perception language},
  owner = {tim},
  timestamp = {2011.07.07}
}

@ARTICLE{Fischler1981,
  author = {Martin A. Fischler and Robert C. Bolles},
  title = {Random sample consensus: a paradigm for model fitting with applications
	to image analysis and automated cartography},
  journal = {Communications of the ACM},
  year = {1981},
  volume = {24},
  pages = {381--395},
  number = {6},
  address = {New York, NY, USA},
  doi = {http://doi.acm.org/10.1145/358669.358692},
  file = {Fischler1981.pdf:Fischler1981.pdf:PDF},
  issn = {0001-0782},
  keywords = {ransac model fitting outliers machine-learning},
  publisher = {ACM},
  review = {Standard algorithm for finding a minimum cost to find an analytic
	model to data containing some gross errors.}
}

@BOOK{Fiske2010,
  title = {Introduction to Communication Studies},
  publisher = {Routledge},
  year = {2010},
  author = {John Fiske},
  series = {The John Fiske Collection},
  edition = {3 edition},
  month = {November 14},
  owner = {tim},
  timestamp = {2013.03.30}
}

@ARTICLE{Fontaine2007,
  author = {Fontaine, J. R. and Scherer, K. R. and Roesch, E. B. and Ellsworth,
	P.},
  title = {The world of emotion is not two-dimensional},
  journal = {Psychological Science},
  year = {2007},
  volume = {18},
  pages = {1050-1057},
  abstract = {For more than half a century, emotion researchers have attempted to
	establish the dimensional space that most economically accounts for
	similarities and differences in emotional experience. Today, many
	researchers focus exclusively on two-dimensional models involving
	valence and arousal. Adopting a theoretically based approach, we
	show for three languages that four dimensions are needed to satisfactorily
	represent similarities and differences in the meaning of emotion
	words. In order of importance, these dimensions are evaluation pleasantness,
	potency-control, activation-arousal, and unpredictability. They were
	identified on the basis of the applicability of 144 features representing
	the six components of emotions: (a) appraisals of events, (b) psychophysiological
	changes, (c) motor expressions, (d) action tendencies, (e) subjective
	experiences, and (f) emotion regulation.},
  file = {:Fontaine2007.pdf:PDF},
  keywords = {SCAS, grid, cross-cultural, dimensions, humaine, abstract-emotion-scales
	perception emotion}
}

@INBOOK{Frank2005,
  chapter = {Technical Issues in Recording Nonverbal Behaviour},
  pages = {449-},
  title = {The New Handbook of Methods in Nonverbal Behaviour Research},
  publisher = {Oxford University Press},
  year = {2005},
  editor = {Harrigan, J. A.},
  author = {Frank, M.G. and Juslin, P.N. and Harrigan, J.A.},
  keywords = {situation nvc experimental situation-important natural-important},
  owner = {ts00051},
  review = {Good review of video, audio and other practical considerations in
	recording NVC.},
  timestamp = {2010.01.05}
}

@ARTICLE{Freund1997,
  author = {Yoav Freund and Robert E Schapire},
  title = {A Decision-Theoretic Generalization of On-Line Learning and an Application
	to Boosting},
  journal = {Journal of Computer and System Sciences},
  year = {1997},
  volume = {55},
  pages = {119 - 139},
  number = {1},
  doi = {10.1006/jcss.1997.1504},
  issn = {0022-0000},
  keywords = {classification-method},
  url = {http://www.sciencedirect.com/science/article/pii/S002200009791504X}
}

@INPROCEEDINGS{Freund1996,
  author = {Freund, Yoav and Schapire, Robert E.},
  title = {Experiments with a New Boosting Algorithm},
  booktitle = {Proceedings of the 13th International Conference on Machine Learning},
  year = {1996},
  pages = {148-156},
  abstract = {In an earlier paper, we introduced a new "boosting" algorithm called
	AdaBoost which, theoretically, can be used to significantly reduce
	the error of any learning algorithm that consistently generates classifiers
	whose performance is a little better than random guessing. We also
	introduced the related notion of a &quot;pseudo-loss &quot; which
	is a method for forcing a learning algorithm of multi-label conceptsto
	concentrate on the labels that are hardest to discriminate. In this
	paper, we describe experiments we carried out to assess how well
	AdaBoost with and without pseudo-loss, performs on real learning
	problems. We performed two sets of experiments. The first set compared
	boosting to Breiman's "bagging" method when used to aggregate various
	classifiers (including decision trees and single attributevalue tests).
	We compared the performance of the two methods on a collection of
	machine-learning benchmarks. In the second set of experiments, we
	studied in more detail the performance of boosting using a nearest-neighbor
	classifier on an OCR problem.},
  file = {Freund1996.pdf:Freund1996.pdf:PDF},
  keywords = {seminal adaboost machine learning supervised classification-method},
  owner = {ts00051},
  timestamp = {2009.10.26}
}

@INBOOK{Fridlund94,
  chapter = {{Facial Paralanguage and Gesture}},
  title = {Human Facial Expression: An Evolutionary View},
  publisher = {Academic Press},
  year = {1994},
  author = {Fridlund, A},
  address = {San Diego, CA},
  abstract = {This text provides an integrated view of human facial expressions
	based on contemporary knowledge about the evolution of signaling
	across the animal kingdom. Spanning fields that range from psychology
	and anthropology to neurology and linguistics, it discusses questions
	such as: What do facial expressions express? How did facial expressions
	evolve? What relationship is there between expressions and emotions
	and motives? This book is suitable for undergraduate and graduate
	use as a text or course supplement. Topics covered include: the history
	of interpreting facial expressions; Darwinian theory; modern evolutionary
	theory; the biological, cultural and developmental evolution of facial
	expressions; and the syntactics and semantics of animal signaling.},
  keywords = {facial expression cross-species},
  review = {Majority of expression from speech not emotion. Paralingustic cues
	are separated into emblems, manipulators, illustrators and regulators
	(Ekman and Friesen). Chovil groups them in a more functional manner.}
}

@ARTICLE{Friedman2001,
  author = {Friedman, J.H.},
  title = {Greedy Function Approximation: A Gradient Boosting Machine},
  journal = {Annals of Statistics},
  year = {2001},
  volume = {29},
  pages = {1189-1232},
  number = {5},
  owner = {ts00051},
  timestamp = {2012.08.14}
}

@INBOOK{Frijda1991,
  chapter = {The duration of affective phenomena or: emotions, sentiments and
	passions},
  pages = {187-225},
  title = {International review of studies on emotion},
  publisher = {John Wiley},
  year = {1991},
  editor = {Strongman, F. T.},
  author = {Frijda, N.H. and Mesquita, B. and Sonnemans, J. and Van Goozen, S},
  volume = {1},
  address = {Chichester},
  abstract = {The authors present 2 studies to explain the variability in the duration
	of emotional experience. Participants were asked to report the duration
	of their fear, anger, joy, gratitude, and sadness episodes on a daily
	basis. Information was further collected with regard to potential
	predictor variables at 3 levels: trait predictors, episode predictors,
	and moment predictors. Discrete-time survival analyses revealed that,
	for all 5 emotions under study, the higher the importance of the
	emotion-eliciting situation and the higher the intensity of the emotion
	at onset, the longer the emotional experience lasts. Moreover, a
	reappearance, either physically or merely mentally, of the eliciting
	stimulus during the emotional episode extended the duration of the
	emotional experience as well. These findings display interesting
	links with predictions within N. H. Frijda's theory of emotion, with
	the phenomenon of reinstatement (as studied within the domain of
	learning psychology), and with the literature on rumination. (PsycINFO
	Database Record (c) 2010 APA, all rights reserved)},
  keywords = {emotion, emotion-definition},
  owner = {ts00051},
  timestamp = {2010.03.01}
}

@BOOK{Frijda1986,
  title = {The Emotions},
  publisher = {Cambridge University Press},
  year = {1986},
  author = {Frijda, N. H.},
  address = {Cambridge, UK},
  abstract = {What are 'emotions'? Drawing together the threads of current research
	on the nature and funactions of emotional expression, of physiological
	reactions, and of emotional experience, this book offers a balanced
	survey of facts and theory. Nico Frijda discusses the motivational
	and neurophysiological preconditions for emotions, and the ways in
	which emotions are regulated by the individual. Considering the kinds
	of events that elicit emotions, he argues that emotions arise because
	events are appraised by people as favorable or harmful to their own
	interests. he takes an information-processing perspective: Emotions
	are viewed as outcomes of the process of assessing the world in terms
	of one's own concerns, which, in turn, modify action readiness. This
	analysis leads him to address such fundamental issues as the place
	of emotion in motivation generally and the discrepancy between the
	functions of the emotions and their often irrational and disruptive
	character. An important contribution to recent debates, The Emotions
	does not presuppose extensive prior knowledge.},
  keywords = {expression emotion-definition},
  owner = {ts00051},
  review = {Definition of emotion:
	
	
	Emotional phenomena are noninstrumental behaviors and noninstrumental
	features of behavior, physiological changes, and evaluative, subject-related
	experiences, as evoked by external or mental events, and primarily
	by the significance of such events.},
  timestamp = {2011.08.09}
}

@INBOOK{Frith2009,
  chapter = {Role of facial expressions in social interactions},
  pages = {3453-3458},
  title = {Philosophical Transactions of Royal Society B},
  publisher = {Royal Society},
  year = {2009},
  editor = {Robinson, Peter and el Kaliouby, Rana},
  author = {Frith, Chris},
  volume = {364},
  abstract = {The expressions we see in the faces of others engage a number of different
	cognitive processes. Emotional expressions elicit rapid responses,
	which often imitate the emotion in the observed face. These effects
	can even occur for faces presented in such a way that the observer
	is not aware of them. We are also very good at explicitly recognizing
	and describing the emotion being expressed. A recent study, contrasting
	human and humanoid robot facial expressions, suggests that people
	can recognize the expressions made by the robot explicitly, but may
	not show the automatic, implicit response. The emotional expressions
	presented by faces are not simply reflexive, but also have a communicative
	component. For example, empathic expressions of pain are not simply
	a reflexive response to the sight of pain in another, since they
	are exaggerated when the empathizer knows he or she is being observed.
	It seems that we want people to know that we are empathic. Of especial
	importance among facial expressions are ostensive gestures such as
	the eyebrow flash, which indicate the intention to communicate. These
	gestures indicate, first, that the sender is to be trusted and, second,
	that any following signals are of importance to the receiver.},
  file = {:Frith2009.pdf:PDF},
  keywords = {nvc emotion mimic perception psychological review-paper emotion-as-communication
	emotion-definition},
  owner = {ts00051},
  review = {Review of some interesting edge cases in emotion.},
  timestamp = {2010.01.07}
}

@ARTICLE{Fua2000,
  author = {P. Fua},
  title = {Regularized Bundle-Adjustment to Model Heads from Image Sequences
	without Calibration Data},
  journal = {International Journal of Computer Vision (IJCV)},
  year = {2000},
  volume = {38},
  pages = {153--171},
  number = {2},
  abstract = {We address the structure-from-motion problem in the context of head
	modeling from video sequences for which calibration data is not available.
	This task is made challenging by the fact that correspondences are
	difficult to establish due to lack of texture and that a quasi-euclidean
	representation is required for realism. We have developed an approach
	based on regularized bundle-adjustment. It takes advantage of our
	rough knowledge of the head's shape, in the form of a generic face
	model. It allows us to recover relative head-motion and epipolar
	geometry accurately and consistently enough to exploit a previously-developed
	stereo-based approach to head modeling. In this way, complete and
	realistic head models can be acquired with a cheap and entirely passive
	sensor, such as an ordinary video camera with minimal manual intervention.
	We chose to demonstrate and evaluate our technique mainly in the
	context of head-modeling. We do so because it is the application
	for which all the t...},
  address = {Hingham, MA, USA},
  doi = {http://dx.doi.org/10.1023/A:1008105802790},
  file = {Fua2000.pdf:Fua2000.pdf:PDF},
  issn = {0920-5691},
  keywords = {head-model},
  publisher = {Kluwer Academic Publishers}
}

@ARTICLE{GaticaPerez2009,
  author = {Daniel Gatica-Perez},
  title = {Automatic nonverbal analysis of social interaction in small groups:
	A review},
  journal = {Image and Vision Computing},
  year = {2009},
  volume = {27},
  pages = {1775 - 1787},
  number = {12},
  abstract = {An increasing awareness of the scientific and technological value
	of the automatic understanding of face-to-face social interaction
	has motivated in the past few years a surge of interest in the devising
	of computational techniques for conversational analysis. As an alternative
	to existing linguistic approaches for the automatic analysis of conversations,
	a relatively recent domain is using findings in social cognition,
	social psychology, and communication that have established the key
	role that nonverbal communication plays in the formation, maintenance,
	and evolution of a number of fundamental social constructs, which
	emerge from face-to-face interactions in time scales that range from
	short glimpses all the way to longterm encounters. Small group conversations
	are a specific case on which much of this work has been conducted.
	This paper reviews the existing literature on automatic analysis
	of small group conversations using nonverbal communication, and aims
	at bridging the current fragmentation of the work in this domain,
	currently split among half a dozen technical communities. The review
	is organized around the main themes studied in the literature and
	discusses, in a comparative fashion, about 100 works addressing problems
	related to the computational modeling of interaction management,
	internal states, personality traits, and social relationships in
	small group conversations, along with pointers to the relevant literature
	in social science. Some of the many open challenges and opportunities
	in this domain are also discussed.},
  doi = {doi:10.1016/j.imavis.2009.01.004},
  file = {GaticaPerez2009.pdf:GaticaPerez2009.pdf:PDF},
  issn = {0262-8856},
  keywords = {Social interaction analysis review-paper nvc taxonomy},
  review = {Issue name: Visual and multimodal analysis of human spontaneous behaviour}
}

@ARTICLE{Gee1996,
  author = {Andrew Gee and Roberto Cipolla},
  title = {Fast visual tracking by temporal consensus},
  journal = {Image and Vision Computing},
  year = {1996},
  volume = {14},
  pages = {105 - 114},
  number = {2},
  abstract = {At the heart of every model-based visual tracker lies a pose estimation
	routine. Recent work has emphasized the use of least-squares techniques
	which employ all the available data to estimate the pose. Such techniques
	are, however, susceptible to the sort of spurious measurements produced
	by visual feature detectors, often resulting in an unrecoverable
	tracking failure. This paper investigates an alternative approach,
	where a minimal subset of the data provides the pose estimate, and
	a robust regression scheme selects the best subset. Bayesian inference
	in the regression stage combines measurements taken in one frame
	with predictions from previous frames, eliminating the need to further
	filter the pose estimates. The resulting tracker performs very well
	on the difficult task of tracking a human face, even when the face
	is partially occluded. Since the tracker is tolerant of noisy, computationally
	cheap feature detectors, frame-rate operation is comfortably achieved
	on standard hardware.},
  doi = {10.1016/0262-8856(95)01044-0},
  issn = {0262-8856},
  keywords = {Visual tracking},
  review = {Facial tracking using alignment algorithm with RANSAC to filter the
	prediction. Points were weighted due to consistance of tracking or
	their smoothness of tracking. Inter face motion constrains were used
	to filter the tracking.},
  url = {http://www.sciencedirect.com/science/article/pii/0262885695010440}
}

@ARTICLE{Geisinger1994,
  author = {Geisinger, K. F.},
  title = {Cross-cultural normative assessment: Translation and adaptation issues
	influencing the normative interpretation of assessment instruments},
  journal = {Psychological Assessment},
  year = {1994},
  volume = {6},
  pages = {304-312},
  abstract = {This article describes some of the issues affecting measures that
	are translated and/or adapted from an original language and culture
	to a new one. It addresses steps to ensure (a) that the test continues
	to measure the same psychological characteristics, (b) that the test
	content is the same, and (c) that the research procedures needed
	to document that it effectively meets this goal are available. Specifically,
	the notions of test validation, fairness, and norms are addressed.
	An argument that such adaptations may be necessary when assessing
	members of subpopulations in U.S. culture is proposed.},
  file = {:Geisinger1994.pdf:PDF},
  keywords = {translation instruments},
  owner = {tim},
  timestamp = {2011.07.07}
}

@ARTICLE{deGelder98,
  author = {Beatrice de Gelder and Jean Vroomen},
  title = {Impairment of speech-reading in prosopagnosia},
  journal = {Speech Communication},
  year = {1998},
  volume = {26},
  pages = {89--96},
  number = {1-2},
  abstract = {The face is a source of information processed by a complex system
	of partly independent subsystems. The extent of the independence
	of processing personal identity, facial expression and facial speech
	remains at present unclear. We investigated the speech-reading ability
	of a prosopagnosic patient, LH, who is severely impaired on recognition
	of personal identity and recognition of facial expressions. Previous
	reports of such cases raised the possibility that speech-reading
	might still be intact, even if almost all other aspects of face processing
	are lost. A series of speech-reading tasks were administered to LH
	including still photographs, video clips, short-term memory tasks
	for auditory and speech-read materials, and tasks aimed at assessing
	the impact of the visual input on auditory speech recognition. LH
	was severely impaired on these tasks. We conclude that in LH there
	is a strong association between severe face processing deficits and
	loss of speech-reading skills.},
  address = {Amsterdam, The Netherlands, The Netherlands},
  doi = {http://dx.doi.org/10.1016/S0167-6393(98)00052-1},
  issn = {0167-6393},
  keywords = {mcgurk facial lipreading speechreading brain perception},
  publisher = {Elsevier Science Publishers B. V.},
  review = {Review of cases of prosopagnosia (face blindness) and their ability
	in speech reading photographs and video. There is no clear pattern
	in disability. Prosopagnosia sufferer LH was tested against computer
	generated faces for the McGurk effect.}
}

@ARTICLE{Gick2009,
  author = {Gick, Bryan and Derrick, Donald},
  title = {Aero-tactile integration in speech perception},
  journal = {Nature},
  year = {2009},
  volume = {462},
  pages = {502-504},
  abstract = {Visual information from a speaker’s face can enhance[1] or interfere
	with[2] accurate auditory perception. This integration of information
	across auditory and visual streams has been observed in functional
	imaging studies[3,4], and has typically been attributed to the frequency
	and robustness with which perceivers jointly encounter event-specific
	information from these two modalities[5]. Adding the tactile modality
	has long been considered a crucial next step in understanding multisensory
	integration. However, previous studies have found an influence of
	tactile input on speech perception only under limited circumstances,
	either where perceivers were aware of the task[6,7] or where they
	had received training to establish a cross-modal mapping[8–10]. Here
	we show that perceivers integrate naturalistic tactile information
	during auditory speech perception without previous training. Drawing
	on the observation that some speech sounds produce tiny bursts of
	aspiration (such as English ‘p’)[11], we applied slight, inaudible
	air puffs on participants’ skin at one of two locations: the right
	hand or the neck. Syllables heard simultaneously with cutaneous air
	puffs were more likely to be heard as aspirated (for example, causing
	participants to mishear ‘b’ as ‘p’). These results demonstrate that
	perceivers integrate event-relevant tactile information in auditory
	perception in much the same way as they do visual information.},
  file = {Gick2009.pdf:Gick2009.pdf:PDF},
  keywords = {mcgurk lipreading speechreading},
  owner = {ts00051},
  timestamp = {2009.12.02}
}

@INPROCEEDINGS{Girard2011,
  author = {Girard, Jeffrey M. and Cohn, Jeffrey F.},
  title = {Criteria and metrics for thresholded {AU} detection},
  booktitle = {Proceedings of the First IEEE International Workshop on Benchmarking
	Facial Image Analysis Technologies (BeFIT)},
  year = {2011},
  address = {Barcelona, Spain},
  month = {November},
  abstract = {Implementing a computerized facial expression analysis system for
	automatic coding requires that a threshold for the system’s classifier
	outputs be selected.
	
	However, there are many potential ways to select a threshold. How
	do different criteria and metrics compare? Manually FACS coded video
	of 45 clinical interviews (Spectrum dataset) were processed using
	person-specific active appearance models (AAM). Support vector machine
	(SVM) classifiers were trained using an independent dataset (RU-FACS).
	Spectrum sessions were randomly assigned to training (n=32) and testing
	sets (n=13). Six different threshold selection criteria were compared
	for automatic AU coding.
	
	Three major findings emerged: 1) Thresholds that attempt to balance
	the confusion matrix (using kappa, F1, or MCC) performed significantly
	better on all metrics than thresholds that select arbitrary error
	or accuracy rates (such as TPR, FPR, or EER). 2) AU detection scores
	for kappa, F1, and MCC were highly intercorrelated; accuracy was
	uncorrelated with the others. And 3) Kappa, MCC, and F1 were all
	positively correlated with base rate. They increased with increases
	in AU base rates. Accuracy, by contrast, showed the opposite pattern.
	It was strongly negatively correlated with base rate. These findings
	suggest that better automatic coding can be obtained by using threshold-selection
	criteria that balance the confusion matrix and benefit from increased
	AU base rates in the training data.},
  file = {:Girard2011.pdf:PDF},
  keywords = {corpus-spectrum aam-features classification-svm corpus-RU-FACS transfer
	labels-facs feature-generation},
  owner = {ts00051},
  timestamp = {2011.11.22}
}

@ARTICLE{Gjersing2010,
  author = {Gjersing, Linn and Caplehorn, John RM and Clausen, Thomas},
  title = {Cross-cultural adaptation of research instruments: language, setting,
	time and statistical considerations},
  journal = {BMC Medical Research Methodology},
  year = {2010},
  volume = {10},
  number = {13},
  abstract = {Background: Research questionnaires are not always translated appropriately
	before they are used in new temporal, cultural or linguistic settings.
	The results based on such instruments may therefore not accurately
	reflect what they are supposed to measure. This paper aims to illustrate
	the process and required steps involved in the cross-cultural adaptation
	of a research instrument using the adaptation process of an attitudinal
	instrument as an example.
	
	
	Methods: A questionnaire was needed for the implementation of a study
	in Norway 2007. There was no appropriate instruments available in
	Norwegian, thus an Australian-English instrument was cross-culturally
	adapted. 
	
	
	Results: The adaptation process included investigation of conceptual
	and item equivalence. Two forward and two back-translations were
	synthesized and compared by an expert committee. Thereafter the instrument
	was pretested and adjusted accordingly. The final questionnaire was
	administered to opioid maintenance treatment staff (n=140) and harm
	reduction staff (n=180). The overall response rate was 84%. The original
	instrument failed confirmatory analysis. Instead a new two-factor
	scale was identified and found valid in the new setting. 
	
	
	Conclusions: The failure of the original scale highlights the importance
	of adapting instruments to current research settings. It also emphasizes
	the importance of ensuring that concepts within an instrument are
	equal between the original and target language, time and context.
	If the described stages in the cross-cultural adaptation process
	had been omitted, the findings would have been misleading, even if
	presented with apparent precision. Thus, it is important to consider
	possible barriers when making a direct comparison between different
	nations, cultures and times.},
  doi = {doi:10.1186/1471-2288-10-13},
  file = {:Gjersing2010.pdf:PDF},
  keywords = {translation cross-cultural instruments},
  owner = {tim},
  timestamp = {2011.07.07}
}

@INPROCEEDINGS{Goecke2006,
  author = {Roland Goecke},
  title = {Audio-video automatic speech recognition: an example of improved
	performance through multimodal sensor input},
  booktitle = {Proceedings of the NICTA-HCSNet Multimodal User Interaction Workshop},
  year = {2006},
  pages = {25--32},
  address = {Darlinghurst, Australia},
  publisher = {Australian Computer Society, Inc.},
  abstract = {One of the advantages of multimodal HCI technology is the performance
	improvement that can be gained over conventional single-modality
	technology by employing complementary sensors in different modalities.
	Such information is particular useful in practical, real-world applications
	where the application's performance must be robust against all kinds
	of noise. An example is the domain of automatic speech recognition
	(ASR). Traditionally, ASR systems only use in- formation from the
	audio modality. In the presence of acoustic noise, the performance
	drops quickly. However, it can and has been shown that incorporating
	additional visual speech information from the video modality improves
	the performance significantly, so that AV ASR systems can be employed
	in applications areas where audio-only ASR systems would fail, thus
	opening new application areas for ASR technology. In this paper,
	a non-intrusive (no artificial markers), real-time 3D lip tracking
	system is presented as well as its application to AV ASR. The multivariate
	statistical analysis 'co-inertia analysis' is also shown, which offers
	improved numerical stability over other multivariate analyses even
	for small sample sizes.},
  file = {Goecke2006.pdf:Goecke2006.pdf:PDF},
  isbn = {1-920-68239-2},
  keywords = {speech recognition multimodal head-pose tracking corpus-avozes tracker-features
	classification-hmms labels-phonemes},
  location = {Sydney, Australia}
}

@INPROCEEDINGS{Goecke2000,
  author = {Goecke, R and Millar, B and Zelinsky, A and Robert-Ribes, J},
  title = {Automatic Extraction of Lip Feature Points},
  booktitle = {Proceedings of the Australian Conference on Robotics and Automation},
  year = {2000},
  pages = {31-36},
  abstract = {We present a novel algorithm for the robust and reliable automatic
	extraction of lip feature points for speechreading. The algorithm
	uses a combination of colour information in the image data and knowledge
	about the structure of the mouth area to find certain feature points
	on the inner lip contour. A new confidence measure quantifying how
	well the feature extraction process worked is introduced. A parameter
	set describing the shape of the mouth is derived from the positions
	of the feature points. Using a stereo camera system, measurements
	are in 3D. Such a 3D parameter set is of great value for automatic
	speech-reading systems.},
  file = {Goecke2000.ps:Goecke2000.ps:PostScript},
  keywords = {head-model lip tracking facial face},
  review = {Extraction of four points on inner lip by colour and structure of
	lips. Various stragegies are employed depending lip closed, clip
	open, lip partially open. Confidence measure is found by comparing
	with a second calibrated camera with a different point of view.}
}

@ARTICLE{SusanGoldin2008,
  author = {Goldin-Meadow, Susan and So, Wing Chee and \"{O}zy\"{u}rek, Aslı
	and Mylander, Carolyn},
  title = {The natural order of events: How speakers of different languages
	represent events nonverbally},
  journal = {Proceedings of the National Academy of Sciences},
  year = {2008},
  volume = {105},
  pages = {9163-9168},
  number = {27},
  abstract = {To test whether the language we speak influences our behavior even
	when we are not speaking, we asked speakers of four languages differing
	in their predominant word orders (English, Turkish, Spanish, and
	Chinese) to perform two nonverbal tasks: a communicative task (describing
	an event by using gesture without speech) and a noncommunicative
	task (reconstructing an event with pictures). We found that the word
	orders speakers used in their everyday speech did not influence their
	nonverbal behavior. Surprisingly, speakers of all four languages
	used the same order and on both nonverbal tasks. This order, actor–patient–act,
	is analogous to the subject–object–verb pattern found in many languages
	of the world and, importantly, in newly developing gestural languages.
	The findings provide evidence for a natural order that we impose
	on events when describing and reconstructing them nonverbally and
	exploit when constructing language anew.},
  doi = {10.1073/pnas.0710060105},
  eprint = {http://www.pnas.org/content/105/27/9163.abstract.pdf},
  file = {SusanGoldin2008.pdf:SusanGoldin2008.pdf:PDF},
  keywords = {nvc expression cross-cultural cultural differences},
  url = {http://www.pnas.org/content/105/27/9163.abstract}
}

@ARTICLE{Goldman2003,
  author = {Goldman, Sally A. and Scott, Stephen D.},
  title = {Multiple-Instance Learning of Real-Valued Geometric Patterns},
  journal = {Annals of Mathematics and Artificial Intelligence},
  year = {2003},
  volume = {39},
  pages = {259--290},
  month = {November},
  abstract = {Recently there has been significant research in multiple-instance
	learning, yet most of this work has only considered this model when
	there are Boolean labels. However, in many of the application areas
	for which the multiple-instance model fits, real-valued labels are
	more appropriate than Boolean labels. We define and study a real-valued
	multiple-instance model in which each multiple-instance example (bag)
	is given a real-valued label in [0, 1] that indicates the degree
	to which the bag satisfies the target concept. To provide additional
	structure to the learning problem, we associate a real-valued label
	with each point in the bag. These values are then combined using
	a real-valued aggregation operator to obtain the label for the bag.
	We then present on-line agnostic algorithms for learning real-valued
	multiple-instance geometric concepts defined by axis-aligned boxes
	in constant-dimensional space and describe several possible applications
	of these algorithms. We obtain our learning algorithms by reducing
	the problem to one in which the exponentiated gradient or gradient
	descent algorithm can be used. We also give a novel application of
	the virtual weights technique. In typical applications of the virtual
	weights technique, all of the concepts in a group have the same weight
	and prediction, allowing a single “representative” concept from each
	group to be tracked. However, in our application there are an exponential
	number of different weights and possible predictions. Hence, boxes
	in each group have different weights and predictions, making the
	computation of the contribution of a group significantly more involved.
	However, we are able to both keep the number of groups polynomial
	in the number of trials and efficiently compute the overall prediction.},
  acmid = {858361},
  address = {Hingham, MA, USA},
  doi = {10.1023/A:1024671512350},
  issn = {1012-2443},
  issue = {3},
  keywords = {content-based image retrieval, exponentiated gradient, geometric patterns,
	landmark matching, multiple-instance learning, multiplicative weight
	updates, scene classification, virtual weights classification-method},
  numpages = {32},
  publisher = {Kluwer Academic Publishers},
  url = {http://dl.acm.org/citation.cfm?id=858359.858361}
}

@INPROCEEDINGS{Gonzalez2007,
  author = {Jose Gonzalez and Fernando De la Torre Frade and Rajesh Murthi and
	Nicolas Guil Mata and E. Zapata},
  title = {Bilinear Active Appearance Models},
  booktitle = {Proceedings of the Workshop on Non-rigid Registration and Tracking
	through Learning},
  year = {2007},
  month = {October},
  abstract = {Appearance Models have been applied to model the space of human faces
	over the last two decades. In particular, Active Appearance Models
	(AAMs) have been successfully used for face tracking, synthesis and
	recognition, and they are one of the state-of-the-art approaches
	due to its efficiency and representational power. Although widely
	employed, AAMs suffer from a few drawbacks, such as the inability
	to isolate pose, identity and expression changes. This paper proposes
	Bilinear Active Appearance Models (BAAMs), an extension of AAMs,
	that effectively decouple changes due to pose and expression/identity.
	We derive a gradient-descent algorithm to efficiently fit BAAMs to
	new images. Experimental results show how BAAMs improve generalization
	and convergence with respect to the linear model. In addition, we
	illustrate decoupling benefits of BAAMs in face recognition across
	pose. We show how the pose normalization provided by BAAMs increase
	the recognition performance of commercial systems.},
  file = {Gonzalez2007.pdf:Gonzalez2007.pdf:PDF},
  keywords = {aam-features method model head-model pose-estimation}
}

@ARTICLE{Goren2006,
  author = {Goren, Deborah and Wilson, Hugh R.},
  title = {Quantifying facial expression recognition across viewing conditions},
  journal = {Vision Research},
  year = {2006},
  volume = {46},
  pages = {1253--1262},
  number = {8-9},
  month = {Apr},
  abstract = {Inversion Facial expressions are key to social interactions and to
	assessment of potential danger in various situations. Therefore,
	our brains must be able to recognize facial expressions when they
	are transformed in biologically plausible ways. We used synthetic
	happy, sad, angry and fearful faces to determine the amount of geometric
	change required to recognize these emotions during brief presentations.
	Five-alternative forced choice conditions involving central viewing,
	peripheral viewing and inversion were used to study recognition among
	the four emotions. Two-alternative forced choice was used to study
	affect discrimination when spatial frequency information in the stimulus
	was modified. The results show an emotion and task-dependent pattern
	of detection. Facial expressions presented with low peak frequencies
	are much harder to discriminate from neutral than faces defined by
	either mid or high peak frequencies. Peripheral presentation of faces
	also makes recognition much more difficult, except for happy faces.
	Differences between fearful detection and recognition tasks are probably
	due to common confusions with sadness when recognizing fear from
	among other emotions. These findings further support the idea that
	these emotions are processed separately from each other.},
  keywords = {Facial expression viewing angle perception human-performance},
  review = {Synthesis for diagrammatically faces with various expressions. Assessment
	of human performance under addition of noise, peripheral vision and
	inversion of the image. Some expressions were recognisable in a wide
	range of situations while others were not. This may be because brain
	processing of each expression is handled using different functional
	areas.},
  url = {http://www.sciencedirect.com/science/article/B6T0W-4HVDYJ8-4/1/9166ac88a95a140758000ab30909f838}
}

@INPROCEEDINGS{Goswami2010,
  author = {Goswami, B. and Chi Ho Chan and Kittler, J. and Christmas, B.},
  title = {Local Ordinal Contrast Pattern histograms for spatiotemporal, lip-based
	speaker authentication},
  booktitle = {Biometrics: Theory Applications and Systems (BTAS), 2010 Fourth IEEE
	International Conference on},
  year = {2010},
  pages = {1 -6},
  month = {sept.},
  abstract = {The lip-region can be interpreted as either a genetic or behavioral
	biometric trait depending on whether static or dynamic information
	is used. Despite this breadth of possible application as a biometric,
	lip-based biometric systems are scarcely developed in scientific
	literature compared to other more popular traits such as face or
	voice. This is because of the generalized view of the research community
	about the lack of discriminative power in the lip region. In this
	paper, we propose a new method of texture representation called Local
	Ordinal Contrast Pattern (LOCP) for use in the representation of
	both appearance and dynamics features observed within a given lip-region
	during speech production. The use of this new feature representation,
	in conjunction with some standard speaker verification engines based
	on Linear Discriminant Analysis and Histogram-distance based methods,
	is shown to drastically improve the performance of the lip-biometric
	trait compared to the existing state-of-the-art methods. The best,
	reported state-of-the-art performance was an HTER of 13.35% for the
	XM2VTS database. We obtained HTER of less than 1%. The improvement
	obtained is remarkable and suggests that there is enough discriminative
	information in the mouth-region to enable its use as a primary biome
	#x0301;trie modality as opposed to a #x201C;soft #x201D; biome #x0301;trie
	trait as has been done in previous research.},
  doi = {10.1109/BTAS.2010.5634469},
  keywords = {behavioral biometric trait;histogram-distance based methods;linear
	discriminant analysis;lip-based speaker authentication;local ordinal
	contrast pattern histograms;speaker verification engines;texture
	representation;biometrics (access control);image enhancement;image
	recognition;image texture;pattern classification;statistical analysis;}
}

@TECHREPORT{Grandvalet2006,
  author = {Grandvalet, Yves and Bengio, Yoshua},
  title = {Hypothesis Testing for Cross-Validation},
  institution = {D{\'{e}}partement d'informatique et recherche op{\'{e}}rationnelle,
	Universit{\'{e}} de Montr{\'{e}}al},
  year = {2006},
  number = {1285},
  abstract = {K-fold cross-validation produces variable estimates, whose variance
	cannot be estimated unbiasedly. However, in practice, one would like
	to provide a figure related to the variability of this estimate.
	The first part of this paper lists a series of restrictive assumptions
	(on the distribution of cross-validation residuals) that allow to
	derive unbiased estimates. We exhibit three such estimates, corresponding
	to differing assumptions. Their similar expressions however entail
	almost identical empirical behaviors. Then, we look for a conservative
	test for detecting significant differences in performances between
	two algorithms. Our proposal is based on the derivation of the form
	of a t-statistic parametrized by the correlation of residuals between
	each validation set. Its calibration is compared to the usual t-test.
	While the latter is overconfident in concluding that differences
	are indeed significant, our test is bound to be more skeptical, with
	smaller type-I error.},
  cat = {T},
  keywords = {cross validation},
  topics = {ModelSelection,Comparative},
  url = {http://www.iro.umontreal.ca/~lisa/pointeurs/xv_rho_stat_tr1285.pdf}
}

@INPROCEEDINGS{Gray1997,
  author = {Michael S. Gray and Javier R. Movellan and Terrence J. Sejnowski},
  title = {Dynamic features for visual speechreading: A systematic comparison},
  booktitle = {Advances in Neural Information Processing Systems},
  year = {1997},
  volume = {9},
  pages = {751},
  abstract = {Humans use visual as well as auditory speech signals to recognize
	spoken words. A variety of systems have been investigated for performing
	this task. The main purpose of this research was to systematically
	compare the performance of a range of dynamic visual features on
	a speechreading task. We have found that normalization of images
	to eliminate variation due to translation, scale, and planar rotation
	yielded substantial improvements in generalization performance regardless
	of the visual representation used. In addition, the dynamic information
	in the difference between successive frames yielded better performance
	than optical-flow based approaches, and compression by local low-pass
	filtering worked surprisingly better than global principal components
	analysis (PCA). These results are examined and possible explanations
	are explored. 1},
  file = {Gray1997.pdf:Gray1997.pdf:PDF},
  institution = {CiteSeer [http://cs1.ist.psu.edu/cgi-bin/oai.cgi] (United States)},
  keywords = {facial feature-generation hmm pca-features image-patch speechreading
	lipreading optical-flow-features classification-hmms},
  review = {A comparison between optical flow and temporal change in low pass
	filtered gray intensity in a visual numeral recognition task. Data
	was classified by PCA and HMM. Best results were obtained using change
	in grey levels for pose normalised faces. This was thought to be
	due to noise in optical flow tracking degrading its relative performance.},
  url = {http://citeseer.ist.psu.edu/505584.html}
}

@ARTICLE{Green2009,
  author = {Green, Samuel and Yang, Yanyun},
  title = {Commentary on Coefficient Alpha: A Cautionary Tale},
  journal = {Psychometrika},
  year = {2009},
  volume = {74},
  pages = {121-135},
  note = {10.1007/s11336-008-9098-4},
  abstract = {The general use of coefficient alpha to assess reliability should
	be discouraged on a number of grounds. The assumptions underlying
	coefficient alpha are unlikely to hold in practice, and violation
	of these assumptions can result in nontrivial negative or positive
	bias. Structural equation modeling was discussed as an informative
	process both to assess the assumptions underlying coefficient alpha
	and to estimate reliability},
  affiliation = {Arizona State University P.O. Box 870611 Tempe AZ 85287-0611 USA},
  issn = {0033-3123},
  issue = {1},
  keyword = {Behavioral Science},
  publisher = {Springer New York},
  url = {http://dx.doi.org/10.1007/s11336-008-9098-4}
}

@ARTICLE{Griggs2010,
  author = {Jessica Griggs},
  title = {Five emotions you never knew you had},
  journal = {New Scientist},
  year = {2010},
  volume = {2743},
  file = {Griggs2010.pdf:Griggs2010.pdf:PDF},
  keywords = {emotion taxonomy basic-emotions universal},
  owner = {ts00051},
  review = {Lists work on other cross cultural emotions: Elevation, Interest,
	Gratitude, Pride and Confusion. Some are not outwardly expressed
	and but some are innately known.},
  timestamp = {2010.01.14}
}

@CONFERENCE{Grimm2007,
  author = {Grimm, Michael and Kroschel, Kristian and Narayanan, Shrikanth},
  title = {Support Vector Regression for Automatic Recognition of Spontaneous
	Emotions in Speech},
  booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech
	and Signal Processing},
  year = {2007},
  volume = {4},
  pages = {1085-1088},
  file = {Grimm2007.pdf:Grimm2007.pdf:PDF},
  keywords = {annotation abstract-emotion-scales labels-emotion regression emotion
	recognition feature-generation classification-svm vam-corpus situation-interview
	audio-features clip-statistics feature-selection classification-nn
	classification-fuzzy-logic},
  owner = {ts00051},
  timestamp = {2010.02.19}
}

@INPROCEEDINGS{Gunes2011,
  author = {H. Gunes, B. Schuller, M. Pantic, and R. Cowie},
  title = {Emotion Representation, Analysis and Synthesis in Continuous Space:
	A Survey},
  booktitle = {Proc. of the 1st International Workshop on Emotion Synthesis, Presentation,
	and Analysis in Continuous space, IEEE FG},
  year = {2011},
  pages = {827-834},
  address = {Santa Barbara, California, USA},
  month = {21 March},
  publisher = {IEEE Press},
  owner = {tim},
  timestamp = {2013.03.30}
}

@BOOK{Hall1959,
  title = {The Silent Language},
  publisher = {Bantam Doubleday Dell Publishing Group},
  year = {1959},
  author = {Hall, Edward T.},
  keywords = {cross-cultural NVC informal formal},
  owner = {ts00051},
  review = {Influential early book on NVC
	
	quote "people are bound by cultural rules and are not masters of their
	fates"},
  timestamp = {2010.01.18}
}

@INBOOK{Hall2012,
  chapter = {Nonverbal Cues and Communication},
  pages = {626-28},
  title = {Encyclopedia of Social Psychology.},
  publisher = {SAGE},
  year = {2012},
  author = {Hall, Judith A.},
  address = {Thousand Oaks, CA},
  month = {2 Jul},
  owner = {tim},
  timestamp = {2013.03.30}
}

@ARTICLE{Hallowell2004,
  author = {Hallowell, B and Lansing, C. R.},
  title = {Tracking eye movements to study cognition and communication},
  journal = {The ASHA Leader},
  year = {2004},
  file = {Hallowell2004.pdf:Hallowell2004.pdf:PDF},
  keywords = {gaze gaze-tracking},
  optmonth = {Nov. 16},
  optpages = {4-5, 22-25}
}

@INPROCEEDINGS{Haq2009,
  author = {Haq, S. and Jackson, P. J. B.},
  title = {Speaker-Dependent Audio-Visual Emotion Recognition.},
  booktitle = {Proc. Auditory-Visual Speech Processing.},
  year = {2009},
  file = {Haq2009.pdf:Haq2009.pdf:PDF},
  owner = {tim},
  timestamp = {2012.10.30}
}

@ARTICLE{Haralick1989,
  author = {Haralick, R.M. and Joo, H. and Lee, C. and Zhuang, X. and Vaidya,
	V.G. and Kim, M.B.},
  title = {Pose estimation from corresponding point data},
  journal = {IEEE Transactions on Systems, Man and Cybernetics},
  year = {1989},
  volume = {19},
  pages = {1426-1446},
  number = {6},
  month = {Nov/Dec},
  abstract = {Solutions for four different pose estimation problems are presented.
	Closed-form least-squares solutions are given to the overconstrained
	2D-2D and 3D-3D pose estimation problems. A globally convergent iterative
	technique is given for the 2D-perspective-projection-3D pose estimation
	problem. A simplified linear solution and a robust solution to the
	2D-perspective-projection-2D-perspective-projection pose-estimation
	problem are also given. Simulation experiments consisting of millions
	of trials with varying numbers of pairs of corresponding points and
	varying signal-to-noise ratios (SNRs) with either Gaussian or uniform
	noise provide data suggesting that accurate inference of rotation
	and translation with noisy data may require corresponding point data
	sets with hundreds of corresponding point pairs when the SNR is less
	than 40 dB. The experimental results also show that the robust technique
	can suppress the blunder data which come from outliers or mismatched
	points},
  doi = {10.1109/21.44063},
  issn = {0018-9472},
  keywords = {computerised pattern recognition, computerised picture processing,
	iterative methods, least squares approximations2D-perspective-projection-2D-perspective-projection
	pose-estimation problem, 2D-perspective-projection-3D pose estimation
	problem, 3D-3D pose estimation problems, Gaussian noise, closed-form
	solutions, computer vision, computerised picture processing, globally
	convergent iterative technique, mismatched points, outliers, overconstrained
	2D-2D pose estimation problem, pattern recognition, rotation, signal-to-noise
	ratios, translation, uniform noise, model fitting head-pose},
  review = {Estimation of 3D pose from 2D or 3D positions. The 3D-3D method is
	a closed for minimisation of the distances with weighted terms to
	improve robustness. The 2D-3D method uses an estimate of distance
	from camera and then iterating the 3D-3D method.}
}

@INBOOK{Harrigan2005,
  chapter = {Proxemics, kinesics, and gaze},
  pages = {137-198},
  title = {The New Handbook of Methods in Nonverbal Behaviour Research},
  publisher = {Oxford University Press},
  year = {2005},
  editor = {J. A. Harrigan and R. Rosenthal and K. R. Scherer},
  author = {Harrigan, J. A.},
  address = {Oxford},
  keywords = {nvc gaze taxonomy annotation},
  review = {Overview of NVC coding approaches}
}

@PHDTHESIS{Hassan2012,
  author = {Hassan, Ali},
  title = {On automatic emotion classification using acoustic features},
  school = {University of Southampton},
  year = {2012},
  month = {June},
  abstract = {In this thesis, we describe extensive experiments on the classification
	of emotions from speech using acoustic features. This area of research
	has important applications in human computer interaction. We have
	thoroughly reviewed the current literature and present our results
	on some of the contemporary emotional speech databases. The principal
	focus is on creating a large set of acoustic features, descriptive
	of different emotional states and finding methods for selecting a
	subset of best performing features by using feature selection methods.
	In this thesis we have looked at several traditional feature selection
	methods and propose a novel scheme which employs a preferential Borda
	voting strategy for ranking features. The comparative results show
	that our proposed scheme can strike a balance between accurate but
	computationally intensive wrapper methods and less accurate but computationally
	less intensive filter methods for feature selection. By using the
	selected features, several schemes for extending the binary classifiers
	to multiclass classification are tested. Some of these classifiers
	form serial combinations of binary classifiers while others use a
	hierarchical structure to perform this task. We describe a new hierarchical
	classification scheme, which we call Data-Driven Dimensional Emotion
	Classification (3DEC), whose decision hierarchy is based on non-metric
	multidimensional scaling (NMDS) of the data. This method of creating
	a hierarchical structure for the classification of emotion classes
	gives significant improvements over other methods tested. The NMDS
	representation of emotional speech data can be interpreted in terms
	of the well-known valence-arousal model of emotion. We find that
	this model does not give a particularly good fit to the data: although
	the arousal dimension can be identified easily, valence is not well
	represented in the transformed data. From the recognition results
	on these two dimensions, we conclude that valence and arousal dimensions
	are not orthogonal to each other. In the last part of this thesis,
	we deal with the very difficult but important topic of improving
	the generalisation capabilities of speech emotion recognition (SER)
	systems over different speakers and recording environments. This
	topic has been generally overlooked in the current research in this
	area. First we try the traditional methods used in automatic speech
	recognition (ASR) systems for improving the generalisation of SER
	in intra? and inter?database emotion classification. These traditional
	methods do improve the average accuracy of the emotion classifier.
	In this thesis, we identify these differences in the training and
	test data, due to speakers and acoustic environments, as a covariate
	shift. This shift is minimised by using importance weighting algorithms
	from the emerging field of transfer learning to guide the learning
	algorithm towards that training data which gives better representation
	of testing data. Our results show that importance weighting algorithms
	can be used to minimise the differences between the training and
	testing data. We also test the effectiveness of importance weighting
	algorithms on inter?database and cross-lingual emotion recognition.
	From these results, we draw conclusions about the universal nature
	of emotions across different languages.},
  url = {http://eprints.soton.ac.uk/340672/}
}

@INPROCEEDINGS{Hassan2009,
  author = {Hassan, A. and Damper, R. I.},
  title = {Emotion recognition from speech using extended feature selection
	and a simple classifier.},
  booktitle = {In Proceedings of 10th Annual of the International Speech Communication
	Association, Interspeech’09},
  year = {2009},
  pages = {2403–2406},
  address = {Brighton, UK},
  file = {Hassan2009.pdf:Hassan2009.pdf:PDF},
  owner = {tim},
  timestamp = {2012.10.30}
}

@BOOK{Hayes2005,
  title = {Statistical Methods For Communication Science},
  publisher = {Lawrence Erlbaum Associates},
  year = {2005},
  author = {Hayes, Andrew F.},
  owner = {tim},
  timestamp = {2012.10.19}
}

@INPROCEEDINGS{He2005,
  author = {Lianghua He and Jianzhong Zhou and Die Hu and Cairong Zou and Li
	Zhao},
  title = {Boosted Independent Features for Face Expression Recognition},
  booktitle = {Proceedings of the Second International Conference on Advances in
	Neural Networks},
  year = {2005},
  pages = {137-146},
  abstract = {Independent Component Analysis (ICA) is used widely to extract statistical
	independent features for analysis and discrimination in recent years.
	But its random properties make it very difficult to test the efficiency
	and validation of the extracted independent features. In this paper,
	we propose a new method called BoostedICA to solve such problems
	by running ICA several times and boosting the selected independent
	components. Because of the local extremum question in calculating
	independent component, several times of running could get the more
	valid components with larger probability. The AdaBoost algorithm
	can guarantee the discriminating efficient of the selected features
	from the statistical theory. The proposed method achieves both computational
	efficiency and accuracy through optimizing extracting and choosing
	features. Finally we describe face expression recognition experiments
	on person-dependent and person-independent. The experimental results
	of 97.5% and 86% recognition rate respectively show that our method
	has better performance compared with other methods.},
  file = {:He2005.pdf:PDF},
  keywords = {facial expression texture-feature posed-data situation-acted corpus-jaffe
	basic-emotions classification-boost},
  review = {Use of boostICA to extract classifiable features separately. The transformed
	unknown variables are then processed using ADAboost into seven facial
	expressions.},
  url = {http://www.springerlink.com/content/nyeu1yfxttndavlm}
}

@INPROCEEDINGS{Heinzmann1998,
  author = {Jochen Heinzmann and Alexander Zelinsky},
  title = {3-{D} Facial Pose and Gaze Point Estimation using a Robust Real-Time
	Tracking Paradigm},
  booktitle = {Proceedings of the {IEEE} International Conference on Automatic Face
	and Gesture Recognition},
  year = {1998},
  pages = {142-147},
  abstract = {Facial pose and gaze point are fundamental to any visually directed
	human-machine interface. In this paper we propose a system capable
	of tracking a face and estimating the 3-D pose and the gaze point
	all in a real-time video stream of the head. This is done by using
	a 3-D model together with multiple triplet triangulation of feature
	positions assuming an affine projection. Using feature-based tracking
	the calculation of a 3-D eye gaze direction vector is possible even
	with head rotation and using a monocular camera. The system is also
	able to automatically initialise the feature tracking and to recover
	from total tracking failures which can occur when a person becomes
	occluded or temporarily leaves the image.},
  keywords = {head-pose facial feature-generation gaze tracking model fitting},
  review = {This system uses a 3 layers (hardware template tracking, 2D model
	and 3d model) working in a network of Kalman filters. Pose estimation
	is achieved by multiple triplets of tracked templates, each with
	a confidence level. Eye tracking is calculated considering the pose
	of the head and the information from each eye is merged depending
	on confidence level.},
  url = {citeseer.ist.psu.edu/139125.html}
}

@INPROCEEDINGS{Hillard03,
  author = {Dustin Hillard and Mari Ostendorf},
  title = {Detection Of Agreement Vs. Disagreement In Meetings: Training With
	Unlabeled Data},
  booktitle = {Proceedings of the Human Language Technology Conference / North American
	chapter of the Association for Computational Linguistics},
  year = {2003},
  abstract = {To support summarization of automatically transcribed meetings, we
	introduce a classifier to recognize agreement or disagreement utterances,
	utilizing both word-based and prosodic cues. We show that hand-labeling
	efforts can be minimized by using unsupervised training on a large
	unlabeled data set combined with supervised training on a small amount
	of data. For ASR transcripts with over 45% WER, the system recovers
	nearly 80% of agree/disagree utterances with a confusion rate of
	only 3%.},
  file = {Hillard03.pdf:Hillard03.pdf:PDF},
  keywords = {unsupervised agree disagree situation-meeting audio-features classification-tree
	nvc-categories labels-meaning taxonomy}
}

@INPROCEEDINGS{Hiransoog2006,
  author = {Hiransoog, Chalita and Murray-Pitts, Lucien},
  title = {Introduction to Interactive Document Search with Multi-Class Diverse
	Density},
  booktitle = {Proceedings of the 2006 conference on Advances in Intelligent IT:
	Active Media Technology 2006},
  year = {2006},
  pages = {408--412},
  address = {Amsterdam, The Netherlands, The Netherlands},
  publisher = {IOS Press},
  abstract = {An interactive document search is introduced. It is a document search
	that allows a user to report to the search engine their opinion on
	how relevant each document is. The search engine then uses this information
	to narrow down the list of documents given to a user as a result
	of the search. It is proposed that there are three types of keywords:
	general keywords; hidden keywords; and negative keywords, that the
	search engine needs to uncover. The problem is then redefined as
	Multi-Class Multiple-Instance learning problem where the Multi-Class
	Diverse Density method developed as part of this research was used
	to find the solution.},
  acmid = {1566636},
  isbn = {1-58603-615-7},
  keywords = {Interactive Document Search, Multi-Class Diverse Density, Multi-Class
	Multiple-Instance Learning, Multiple-Instance Learning, mil, multiclass,
	classification-method},
  numpages = {5},
  url = {http://dl.acm.org/citation.cfm?id=1566561.1566636}
}

@ARTICLE{Hobaiter2011,
  author = {Hobaiter, C. and Byrne, R.W.},
  title = {The gestural repertoire of the wild chimpanzee},
  journal = {Animal Cognition},
  year = {2011},
  abstract = {Great ape gestural communication is known to be intentional, elaborate
	and flexible; yet there is controversy over the best interpretation
	of the system and how gestures are acquired, perhaps because most
	studies have been made in restricted, captive settings. Here, we
	report the first systematic analysis of gesture in a population of
	wild chimpanzees. Over 266 days of observation, we recorded 4,397
	cases of intentional gesture use in the Sonso community, Budongo,
	Uganda. We describe 66 distinct gesture types: this estimate appears
	close to asymptote, and the Sonso repertoire includes most gestures
	described informally at other sites. Differences in repertoire were
	noted between individuals and age classes, but in both cases, the
	measured repertoire size was predicted by the time subjects were
	observed gesturing. No idiosyncratic usages were found, i.e. no gesture
	type was used only by one individual. No support was found for the
	idea that gestures are acquired by 'ontogenetic ritualization' from
	originally effective actions; moreover, in detailed analyses of two
	gestures, action elements composing the gestures did not closely
	match those of the presumed original actions. Rather, chimpanzee
	gestures are species-typical; indeed, many are 'family-typical',
	because gesture types recorded in gorillas, orangutans and chimpanzee
	overlap extensively, with 24 gestures recorded in all three genera.
	Nevertheless, chimpanzee gestures are used flexibly across a range
	of contexts and show clear adjustment to audience (e.g. silent gestures
	for attentive targets, contact gestures for inattentive ones). Such
	highly intentional use of a species-typical repertoire raises intriguing
	questions for the evolution of advanced communication.},
  keywords = {nonhuman, nonverbal, culture, animal, nvc, taxonomy, cross-species,
	personal-differences},
  owner = {tim},
  timestamp = {2011.07.08}
}

@INPROCEEDINGS{Hong2006,
  author = {Taehwa Hong and Yang-Bok Lee and Yong-Guk Kim and Hagbae Kim},
  title = {Facial Expression Recognition Using Active Appearance Model},
  booktitle = {Advances in Neural Information Processing Systems},
  year = {2006},
  pages = {69-76},
  abstract = {This paper describes a facial expression recognition system based
	upon Active Appearance Model (AAM), which has been typically used
	for the face recognition task. Given that AAM has been also used
	in tracking the moving object, we thought it could be effective in
	recognizing the facial expressions of humans. Our results show that
	the performance of the facial expression recognition using AAM is
	reliably high when it combined with an enhanced Fisher classification
	model.},
  file = {:Hong2006.pdf:PDF},
  keywords = {basic-emotions corpus-cohn-kanade emotion recognition labels-emotion
	aam-features tracker-features classification-efm},
  review = {Detecting facial expressions using AAMs using an enhanced Fisher classification
	model (first using PCA then standard Fisher linear discriminator).}
}

@ARTICLE{Honts2007,
  author = {Honts, Charles R. and Alloway, Wendy R.},
  title = {Information does not affect the validity of a comparison question
	test},
  journal = {Legal and Criminological Psychology},
  year = {2007},
  volume = {12},
  pages = {311-320(10)},
  number = {2},
  month = {September},
  abstract = {Study results yielded no significant effects of providing information
	on the validity of the comparison question test (CQT). However, the
	reported use of countermeasures was associated with a lower probability
	of truthfulness. Results of the debriefing questionnaire were found
	to support predictions made by the theory of the CQT. In summary,
	concerns that readily available information will enable guilty individuals
	to produce false-negative errors seem unfounded. The most commonly
	used test for psychophysiological deception detection (PDD) in most
	countries is the CQT. CQTs operate on the premise that guilty and
	innocent participants will react differently to relevant and comparison
	questions. The Test for Espionage and Sabotage (TES) is a variation
	of the CQT and a relatively new polygraph testing format developed
	for use in national security. Previous research indicates that CQTs
	that use directed-lie comparison questions have equivalent or higher
	accuracy than CQTs that use the traditional probable-lie comparison
	questions. This study examined the effects of providing detailed
	information on the CQT and CQT-oriented countermeasures on the validity
	of the TES. Participants in the study consisted of 21 males and 19
	females aged 18 to 44 years.},
  doi = {doi:10.1348/135532506X123770},
  keywords = {nvc-applications polygraph},
  review = {Commonly available information on polygraph countermeasures increases
	the probability of truthfulness rather than reducing false-negative
	errors.},
  url = {http://onlinelibrary.wiley.com/doi/10.1348/135532506X123770/abstract}
}

@ARTICLE{Hopkins2011,
  author = {Hopkins, Ingrid and Gower, Michael and Perez, Trista and Smith, Dana
	and Amthor, Franklin and Casey Wimsatt, F. and Biasini, Fred},
  title = {Avatar Assistant: Improving Social Skills in Students with an ASD
	Through a Computer-Based Intervention},
  journal = {Journal of Autism and Developmental Disorders},
  year = {2011},
  volume = {41},
  pages = {1543-1555},
  note = {10.1007/s10803-011-1179-z},
  abstract = {This study assessed the efficacy of FaceSay , a computer-based social
	skills training program for children with Autism Spectrum Disorders
	(ASD). This randomized controlled study ( N = 49) indicates that
	providing children with low-functioning autism (LFA) and high functioning
	autism (HFA) opportunities to practice attending to eye gaze, discriminating
	facial expressions and recognizing faces and emotions in FaceSay’s
	structured environment with interactive, realistic avatar assistants
	improved their social skills abilities. The children with LFA demonstrated
	improvements in two areas of the intervention: emotion recognition
	and social interactions. The children with HFA demonstrated improvements
	in all three areas: facial recognition, emotion recognition, and
	social interactions. These findings, particularly the measured improvements
	to social interactions in a natural environment, are encouraging.},
  affiliation = {Department of Psychology, University of Alabama at Birmingham, 1300
	University Blvd., CH 328, Birmingham, AL 35924-1170, USA},
  issn = {0162-3257},
  issue = {11},
  keyword = {Behavioral Science},
  publisher = {Springer Netherlands},
  url = {http://dx.doi.org/10.1007/s10803-011-1179-z}
}

@CONFERENCE{Hoque2009,
  author = {Hoque, Mohammed E. and el Kaliouby, Rana and Picard, Rosalind W.},
  title = {When Human Coders (and Machines) Disagree on the Meaning of Facial
	Affect in Spontaneous Videos},
  booktitle = {Proceedings of the 9th International Conference on Intelligent Virtual
	Agents},
  year = {2009},
  address = {Boston, Massachusetts, USA},
  month = {April},
  abstract = {This paper describes the challenges of getting ground truth affective
	labels for spontaneous video, and presents implications for systems
	such as virtual agents that have automated facial analysis capabilities.
	We first present a dataset from an intelligent tutoring application
	and describe the most prevalent approach to labeling such data. We
	then present an alternative labeling approach, which closely models
	how the majority of automated facial analysis systems are designed.
	We show that while participants, peers and trained judges report
	high inter-rater agreement on expressions of delight, confusion,
	flow, frustration, boredom, surprise, and neutral when shown the
	entire 30 minutes of video for each participant, inter-rater agreement
	drops below chance when human coders are asked to watch and label
	short 8 second clips for the same set of labels. We also perform
	discriminative analysis for facial action units for each affective
	state represented in the clips. The results emphasize that human
	coders heavily rely on factors such as familiarity of the person
	and context of the interaction to correctly infer a person’s affective
	state; without this information, the reliability of humans as well
	as machines attributing affective labels to spontaneous facial-head
	movements drops significantly.},
  file = {Hoque2009.pdf:Hoque2009.pdf:PDF},
  keywords = {multiannotator perception nvc-annotation human-performance context-important},
  owner = {ts00051},
  timestamp = {2009.11.23}
}

@ARTICLE{Horvitz2003,
  author = {Horvitz, Eric and Kadie, Carl and Paek, Tim and Hovel, David},
  title = {{Models of attention in computing and communication: from principles
	to applications}},
  journal = {Commun. ACM},
  year = {2003},
  volume = {46},
  pages = {52--59},
  number = {3},
  month = mar,
  abstract = {{Creating computing and communication systems that sense and reason
	about human attention by fusing together information from multiple
	streams.}},
  address = {New York, NY, USA},
  citeulike-article-id = {1416274},
  citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=636772.636798},
  citeulike-linkout-1 = {http://dx.doi.org/10.1145/636772.636798},
  doi = {10.1145/636772.636798},
  issn = {0001-0782},
  keywords = {attention},
  posted-at = {2009-03-12 19:50:34},
  priority = {2},
  publisher = {ACM},
  url = {http://dx.doi.org/10.1145/636772.636798}
}

@ARTICLE{Hsee1992,
  author = {Hsee, Christopher K. and Hatfield, Elaine and Chemtob, Claude},
  title = {{Assessments of the Emotional States of Others: Conscious Judgments
	Versus Emotional Contagion}},
  journal = {Journal of Social and Clinical Psychology},
  year = {1992},
  volume = {11},
  pages = {119--128},
  number = {2},
  citeulike-article-id = {9433466},
  key = {Hsee.ea.1992},
  keywords = {human, interaction, nonverbal, verbal},
  posted-at = {2011-06-20 10:55:19},
  priority = {2}
}

@INBOOK{Humphrey1993,
  chapter = {Casual Chat and Ethnic Identity: Women's Second-Language Use among
	Buryats in the USSR},
  title = {Bilingual Women: Anthropological Approaches to Second Language Use},
  publisher = {Berg: Oxford},
  year = {1993},
  editor = {Ardener, Shirley and Pauline Burton and Ketaki Kushari Dyson},
  author = {Humphrey, C.},
  month = {Mar},
  keywords = {translation},
  owner = {ts00051},
  review = {Quote:
	
	
	"As Levi-Strauss remarked about myth or Geertz about 'common sense',
	we can also recognise casual chat, however different the culture
	is from our own. We know it by its informality, its lack of focus,
	its haphazard reiteration, as 'topics of conversation' crumble away
	in the compulsion of people saying what they can't help saying."},
  timestamp = {2011.12.13}
}

@ARTICLE{Hupont2013,
  author = {Hupont, Isabelle and Baldassarri, Sandra and Cerezo, Eva},
  title = {Facial emotional classification: from a discrete perspective to a
	continuous emotional space},
  journal = {Pattern Analysis and Applications},
  year = {2013},
  volume = {16},
  pages = {41-54},
  number = {1},
  doi = {10.1007/s10044-012-0286-6},
  issn = {1433-7541},
  keywords = {Affective computing; Algorithms; Facial expression analysis; Intelligent
	user interfaces},
  language = {English},
  publisher = {Springer-Verlag},
  url = {http://dx.doi.org/10.1007/s10044-012-0286-6}
}

@ARTICLE{Hyvarinen1999,
  author = {Hyv\"arinen, A.},
  title = {Fast and Robust Fixed-Point Algorithms for Independent Component
	Analysis},
  journal = {IEEE Transactions on Neural Networks},
  year = {1999},
  volume = {10},
  pages = {626--634},
  number = {3},
  month = {unsupervised learning ica machine learning method},
  abstract = {Independent component analysis (ICA) is a statistical method for transforming
	an observed multidimensional random vector into components that are
	statistically as independent from each other as possible. In this
	paper, we use a combination of two different approaches for linear
	ICA: Comon's information-theoretic approach and the projection pursuit
	approach. Using maximum entropy approximations of differential entropy,
	we introduce a family of new contrast (objective) functions for ICA.
	These...},
  citeulike-article-id = {1048545},
  citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.50.4731},
  file = {Hyvarinen1999.pdf:Hyvarinen1999.pdf:PDF},
  keywords = {ica unsupervised learning machine method dimensional-reduction fastica},
  posted-at = {2007-01-18 01:24:45},
  priority = {2},
  review = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.50.4731}
}

@ARTICLE{Isard1998,
  author = {Isard, Michael and Blake, Andrew},
  title = {{CONDENSATION} -- conditional density propagation for visual tracking},
  journal = {International Journal of Computer Vision (IJCV)},
  year = {1998},
  volume = {29},
  pages = {5--28},
  number = {1},
  abstract = {The problem of tracking curves in dense visual clutter is challenging.
	Kalman filtering is inadequate because it is based on Gaussian densities
	which, being unimodal, cannot represent simultaneous alternative
	hypotheses. The CONDENSATION algorithm uses ``factored sampling'',
	previously applied to the interpretation of static images, in which
	the probability distribution of possible interpretations is represented
	by a randomly generated set. CONDENSATION uses learned dynamical
	models, together with visual observations, to propagate the random
	set over time. The result is highly robust tracking of agile motion.
	Notwithstanding the use of stochastic methods, the algorithm runs
	in near real-time.},
  owner = {ts00051},
  timestamp = {2012.03.19}
}

@ARTICLE{Jack2012,
  author = {Jack, R. and Garrod, O. and Yu, H. and Caldara R. and Schyns, P.},
  title = {Facial expressions of emotion are not culturally universal},
  journal = {Proceedings of the National Academy of Sciences},
  year = {2012},
  volume = {109},
  pages = {7241-7244},
  number = {9},
  doi = {10.1073/pnas.1200155109},
  file = {:Jack2012.pdf:PDF},
  owner = {ts00051},
  timestamp = {2012.07.26}
}

@ARTICLE{Jack2009,
  author = {Jack, Rachael E. and Blais, Caroline and Scheepers, Christoph and
	Schyns, Philippe G. and Caldara, Roberto},
  title = {Cultural Confusions Show that Facial Expressions Are Not Universal},
  journal = {Current Biology},
  year = {2009},
  volume = {19},
  pages = {1543 - 1548},
  number = {18},
  abstract = {Central to all human interaction is the mutual understanding of emotions,
	achieved primarily by a set of biologically rooted social signals
	evolved for this purpose—facial expressions of emotion. Although
	facial expressions are widely considered to be the universal language
	of emotion [1,2,3], some negative facial expressions consistently
	elicit lower recognition levels among Eastern compared to Western
	groups (see [4] for a meta-analysis and [5,6] for review). Here,
	focusing on the decoding of facial expression signals, we merge behavioral
	and computational analyses with novel spatiotemporal analyses of
	eye movements, showing that Eastern observers use a culture-specific
	decoding strategy that is inadequate to reliably distinguish universal
	facial expressions of “fear” and “disgust.” Rather than distributing
	their fixations evenly across the face as Westerners do, Eastern
	observers persistently fixate the eye region. Using a model information
	sampler, we demonstrate that by persistently fixating the eyes, Eastern
	observers sample ambiguous information, thus causing significant
	confusion. Our results question the universality of human facial
	expressions of emotion, highlighting their true complexity, with
	critical consequences for cross-cultural communication and globalization.},
  doi = {10.1016/j.cub.2009.07.051},
  file = {Jack2009.pdf:Jack2009.pdf:PDF},
  keywords = {gaze human-performance perception emotion cross-cultural basic-emotions},
  owner = {ts00051},
  timestamp = {2009.10.26}
}

@ARTICLE{Jamieson2004,
  author = {Jamieson, Susan},
  title = {Likert scales: how to (ab)use them},
  journal = {Medical Education},
  year = {2004},
  volume = {38},
  pages = {1217--1218},
  number = {12},
  doi = {10.1111/j.1365-2929.2004.02012.x},
  issn = {1365-2923},
  publisher = {Blackwell Science Ltd},
  url = {http://dx.doi.org/10.1111/j.1365-2929.2004.02012.x}
}

@INBOOK{Jandt2004,
  chapter = {Nonverbal Communication},
  pages = {120-145},
  title = {An Introduction to Intercultural Communication},
  publisher = {Sage Publications},
  year = {2004},
  author = {Jandt, Fred E.},
  edition = {4th Edition},
  abstract = {The Fourth Edition of Fred Jandt’s text An Introduction to Intercultural
	Communication: Identities in a Global Community challenges students
	to develop cultural competency by developing an understanding of
	how we perceive and react to cultural rules – not only those of others,
	but also our own.
	
	
	Going beyond an "American" assessment of the field, this textbook
	assumes that no culture is privileged over another, be that culture
	from across the globe or a subculture or subgroup around the corner.
	Issues of identity, nationality, assimilation, and inter-group relations
	promote appreciation of diversity among people. An Introduction to
	Intercultural Communication is intended for introductory courses
	in Intercultural Communication and related topics. A new accompanying
	reader, Intercultural Communication: A Global Reader, is also available
	and can be used alone or in conjunction with An Introduction to Intercultural
	Communication.},
  keywords = {nvc-definition nvc taxonomy},
  owner = {ts00051},
  review = {Cross cultural view of NVC. Section on NVC message codes. Yes, no
	NVC reversed in Albania and Bulgaria, p131. Message codes: Proxemics,
	Kinesics, Chronemics, Paralanguage, Silence, Haptics, Clothing and
	Physical Appearance, Territoriality, Olfactics and Oculesics.
	
	
	"Nonverbal communication can be narrowly used to refer to intential
	use, as in using a nonspoken symbol to communication a specific message."},
  timestamp = {2010.01.18}
}

@INPROCEEDINGS{Jang2008,
  author = {Jang, Jun-Su and Kanade, Takeo},
  title = {Robust 3{D} Head Tracking by Online Feature Registration},
  booktitle = {Proceedings of the 8th IEEE International Conference on Automatic
	Face and Gesture Recognition},
  year = {2008},
  abstract = {This paper presents a robust method for tracking the position and
	orientation of a head in videos. The proposed method can overcome
	occlusions and divergence problems. We introduce an online registration
	technique to detect and register feature point of the head while
	tracking. A set of point features is registered and updated for each
	reference pose serving a multi-view head detector. The online feature
	registration rectifies error accumulation and provides fast recovery
	after occlusion has ended, while preventing divergence problem which
	frequently occurs in conventional frame-to-frame tracking methods.
	The robustness of the proposed tracker is experimentally shown with
	video sequences that include occlusions and large pose variations.},
  file = {Jang2008.pdf:Jang2008.pdf:PDF},
  keywords = {head tracker},
  owner = {ts00051},
  timestamp = {2009.10.26}
}

@INPROCEEDINGS{Jeni2011,
  author = {Jeni, Laszlo A. and Takacs, Daniel and Lorincz, Andras},
  title = {High Quality Facial Expression Recognition in Video Streams using
	Shape Related Information only},
  booktitle = {Proceedings of the First IEEE International Workshop on Benchmarking
	Facial Image Analysis Technologies (BeFIT)},
  year = {2011},
  address = {Barcelona, Spain},
  month = {November},
  abstract = {Person independent and pose invariant facial emotion classification
	is important for situation analysis and for automated video annotation.
	Shape and its changes are advantageous for these purposes. We estimated
	the potentials of shape measurements from the raw 2D shape data of
	the CK+ database. We used a simple Procrustes transformation and
	applied the multi-class SVM leave-one-out method. We found close
	to 100% classification performance demonstrating the relevance of
	details in shape space. Precise, pose invariant 3D shape information
	can be computed by means of constrained local
	
	models (CLM). We used this method: we fitted 3D CLM to CK+ data and
	derived the frontal views of the 2D shapes. Performance reached and
	sometimes surpassed state-of-the-art results. In another experiment,
	we studied pose invariance: we rendered 3D emotional database with
	different poses using BU 4DFE database, fitted 3D CLM, transformed
	the 3D shape to frontal pose and evaluated the outputs of our classifier.
	Results show that the high quality classification is robust against
	pose variations. The superior performance suggests that shape, which
	is typically neglected or used only as side information in facial
	expression categorization, could make a good benchmark for future
	studies.},
  file = {:Jeni2011.pdf:PDF},
  keywords = {basic-emotions corpus-cohn-kanade classification-svm corpus-BU-4DFE
	multi-angle clm-features labels-emotion person-normalisation},
  owner = {ts00051},
  timestamp = {2011.11.22}
}

@ARTICLE{Jepson2003,
  author = {Jepson, A. D. and Fleet, D. J. and El-Maraghi, T. F.},
  title = {Robust online appearance models for visual tracking},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
  year = {2003},
  volume = {25},
  pages = {1296--1311},
  number = {10},
  abstract = {We propose a framework for learning robust, adaptive, appearance models
	to be used for motion-based tracking of natural objects. The model
	adapts to slowly changing appearance, and it maintains a natural
	measure of the stability of the observed image structure during tracking.
	By identifying stable properties of appearance, we can weight them
	more heavily for motion estimation, while less stable properties
	can be proportionately downweighted. The appearance model involves
	a mixture of stable image structure, learned over long time courses,
	along with two-frame motion information and an outlier process. An
	online EM-algorithm is used to adapt the appearance model parameters
	over time. An implementation of this approach is developed for an
	appearance model based on the filter responses from a steerable pyramid.
	This model is used in a motion-based tracking algorithm to provide
	robustness in the face of image outliers, such as those caused by
	occlusions, while adapting to natural changes in appearance such
	as those due to facial expressions or variations in 3D pose.},
  citeulike-article-id = {751220},
  doizzz = {http://dx.doi.org/10.1109/TPAMI.2003.1233903},
  file = {Jepson2003.pdf:Jepson2003.pdf:PDF},
  keywords = {motion, tracking facial feature},
  owner = {ts00051},
  posted-at = {2007-11-02 17:25:45},
  priority = {5},
  timestamp = {2009.10.26},
  urlzzz = {http://dx.doi.org/10.1109/TPAMI.2003.1233903}
}

@ARTICLE{Ji2004,
  author = {Qiang Ji and Zhiwei Zhu and Lan, P.},
  title = {Real-time nonintrusive monitoring and prediction of driver fatigue},
  journal = {Vehicular Technology, IEEE Transactions on},
  year = {2004},
  volume = {53},
  pages = { 1052 - 1068},
  number = {4},
  month = {july},
  abstract = { This paper describes a real-time online prototype driver-fatigue
	monitor. It uses remotely located charge-coupled-device cameras equipped
	with active infrared illuminators to acquire video images of the
	driver. Various visual cues that typically characterize the level
	of alertness of a person are extracted in real time and systematically
	combined to infer the fatigue level of the driver. The visual cues
	employed characterize eyelid movement, gaze movement, head movement,
	and facial expression. A probabilistic model is developed to model
	human fatigue and to predict fatigue based on the visual cues obtained.
	The simultaneous use of multiple visual cues and their systematic
	combination yields a much more robust and accurate fatigue characterization
	than using a single visual cue. This system was validated under real-life
	fatigue conditions with human subjects of different ethnic backgrounds,
	genders, and ages; with/without glasses; and under different illumination
	conditions. It was found to be reasonably robust, reliable, and accurate
	in fatigue characterization.},
  doi = {10.1109/TVT.2004.830974},
  issn = {0018-9545},
  keywords = { charge-coupled-device cameras; computer vision system; driver fatigue
	prediction; eyelid movement; facial expression; gaze movement; head
	movement; imaging algorithms; infrared illuminators; probabilistic
	model; real-time nonintrusive monitoring; video images; visual cues;
	CCD image sensors; accident prevention; biomedical optical imaging;
	monitoring; probability; real-time systems; video cameras; video
	signal processing;}
}

@INPROCEEDINGS{Jiang2011,
  author = {B. Jiang and M. F. Valstar and M. Pantic},
  title = {Action Unit detection using sparse appearance descriptors in space-time
	video volumes},
  booktitle = {Proceedings of IEEE International Conference on Automatic Face and
	Gesture Recognition (FG'11)},
  year = {2011},
  pages = {314--321},
  address = {Santa Barbara, CA, USA},
  month = {March}
}

@ARTICLE{Jiang2012,
  author = {Jiang, Yu-Gang and Bhattacharya, Subhabrata and Chang, Shih-Fu and
	Shah, Mubarak},
  title = {High-level event recognition in unconstrained videos},
  journal = {International Journal of Multimedia Information Retrieval},
  year = {2012},
  pages = {1-29},
  doi = {10.1007/s13735-012-0024-2},
  issn = {2192-6611},
  keywords = {Video events; Recognition; Unconstrained videos; Multimedia event
	detection; Multimodal features; Fusion},
  language = {English},
  publisher = {Springer-Verlag},
  url = {http://dx.doi.org/10.1007/s13735-012-0024-2}
}

@INPROCEEDINGS{Johnson1996,
  author = {M. E. Johnson},
  title = {Synthesis of English Intonation using Explicit Models of Reading
	and Spontaneous Speech},
  booktitle = {Proceedings of the International Conference on Spoken Language Processing
	(Interspeech)},
  year = {1996},
  volume = {3},
  pages = {1844-1847},
  address = {Philadelphia, PA},
  abstract = {A model of English intonation is presented in which a variety of intonation
	contours can be generated from a quantitative prominence labelling
	of stressed syllables. In one style of speech production, spontaneous
	speech, a short-lookahead model can generate a variety of contours
	from the same quantitative prominence labelling. For another style,
	reading aloud, a long-lookahead model determines the types of intonation
	patterns associated with texts. These typically come out as sequences
	of downward-stepping contours, given appropriate initial conditions
	(though there is no explicit downstep constant in the model). In
	both styles, the intonation contours are generated on the basis of
	a quantitative model of contour pitch prominence, in which the pitch
	prominence of the contour segments which make up the accent contours
	(and thence the intonation contours) is computed as a non-linear
	function of the duration of the contour segment, the ratio of the
	F0 value at one end of the segment to that at the other, and a rhythm
	constant.},
  keywords = {speech synthesis},
  url = {citeseer.ist.psu.edu/johnson96synthesis.html}
}

@ARTICLE{Jones99,
  author = {Jones, C.M. and Dlay, S.S.},
  title = {The face as an interface: the new paradigm for {HCI}},
  journal = {IEEE International Conference on Systems, Man, and Cybernetics},
  year = {1999},
  volume = {1},
  pages = {774-779 vol.1},
  abstract = {Usability should be paramount in the development of any multimodal
	interface. Unfortunately, the software industry market their product
	upgrades based on the number of additional features rather than attempting
	to improve the user interaction. This tends to produce packages so
	complex that many of the facilities remain unknown to the user and
	invariably unused. Instead, by using a synthetic human face or cartoon-style
	characters, we attempt to make the interface more transparent and
	begin to readdress the complexity versus usability balance. This
	research continues the development of the MARTI project with enhancement
	of facial modelling. The study has considered key work in the field
	of speechreading and lip reading, and has extended the domain to
	develop a novel conversational, American English, viseme set. Furthermore,
	the work considers the application of psychological ideas to facial
	modelling and animation, in order to create highly believable and
	life-like facial synthesis. New levels of visual accuracy have been
	achieved for both human and cartoon character animation, to attain
	the highest performance to date for automated, speech to face, teeth,
	tongue, lips, and jaw articulation},
  doi = {10.1109/ICSMC.1999.814189},
  keywords = {computer animation, image recognition, user interfacesAmerican English
	viseme set, HCI, MARTI project, articulation, facial modelling, human
	computer interaction, life-like facial synthesis, lip reading, multimodal
	interface, psychological ideas, speechreading, usability, visual
	accuracy, nvc-applications, nvc}
}

@ARTICLE{Jordan97,
  author = {Jordan, T. and Sergeant, P. and Martin, C. and Thomas, S. and Thow,
	E.},
  title = {Effects of horizontal viewing angle on visual and audiovisual speech
	perception},
  journal = {IEEE International Conference on Systems, Man, and Cybernetics},
  year = {1997},
  volume = {2},
  pages = {1626-1631},
  month = {Oct},
  note = {'Computational Cybernetics and Simulation'},
  abstract = {The authors investigated the effects of changes in horizontal viewing
	angle on visual and audiovisual speech recognition in 4 experiments,
	using a talker's face viewed full face, three quarters, and in profile.
	When only experimental items were shown (Experiments 1 and 2), identification
	of unimodal visual speech and visual speech influences on congruent
	and incongruent auditory speech were unaffected by viewing angle
	changes. However, when experimental items were intermingled with
	distractor items (Experiments 3 and 4), identification of unimodal
	visual speech decreased with profile views, whereas visual speech
	influences on congruent and incongruent auditory speech remained
	unaffected by viewing angle changes. These findings indicate that
	audiovisual speech recognition withstands substantial changes in
	horizontal viewing angle, but explicit identification of visual speech
	is less robust. Implications of this distinction for understanding
	the processes underlying visual and audiovisual speech recognition
	are discussed.},
  doi = {10.1109/ICSMC.1997.638235},
  file = {Jordan97.pdf:Jordan97.pdf:PDF},
  keywords = {hearing, human factors, speech recognition, visual perception, audiovisual
	speech perception, hearing, horizontal viewing angle, seeing, visual
	perception, multi-angle},
  review = {Cites earlier work that finds increasing viewing angle reduces visual
	speech recognition but tests of human performance seem to imply accuracy
	is largely independent of viewing angle.}
}

@INPROCEEDINGS{Jurie2002,
  author = {Jurie, Fr\'{e}d\'{e}ric and Dhome, Michel},
  title = {Real time robust template matching},
  booktitle = {Proceedings of the British Machine Vision Conference},
  year = {2002},
  pages = {123-131},
  abstract = {One of the most popular methods to extract useful informations from
	an image sequence is the template matching approach. In this well
	known method the tracking of a certain feature or target over time
	is based on the comparison of the content of each image with a sample
	template. In this article, we propose an efficient robust template
	matching algorithm that is able to track targets in real time. Special
	attention is paid to occlusions handling and illumination variations.},
  file = {Jurie2002.pdf:Jurie2002.pdf:PDF},
  keywords = {template feature-tracking tracker occlusions}
}

@PHDTHESIS{elKaliouby2005Thesis,
  author = {el Kaliouby, Rana Ayman},
  title = {Mind-reading machines:automated inference of complex mental states},
  school = {University of Cambridge},
  year = {2005},
  month = {July},
  note = {UCAM-CL-TR-636},
  owner = {tim},
  timestamp = {2012.11.03}
}

@INBOOK{Kaliouby2005,
  chapter = {{Real-Time Inference of Complex Mental States from Facial Expressions
	and Head Gestures}},
  pages = {181-200},
  title = {Real-time vision for {HCI}},
  publisher = {Springer},
  year = {2005},
  author = {el Kaliouby, Rana and Robinson, Peter},
  file = {Kaliouby2005.pdf:Kaliouby2005.pdf:PDF},
  issn = {ISBN 0 387 27697 1},
  keywords = {situation-acted tracking-features facial classification mind-reading
	corpus labels-meaning au-features classification-dbn},
  owner = {ts00051},
  timestamp = {2009.10.26}
}

@INPROCEEDINGS{ElKaliouby2004,
  author = {el Kaliouby, Rana and Robinson, Peter},
  title = {Real-Time Inference of Complex Mental States from Facial Expressions
	and Head Gestures},
  booktitle = {Proceedings of the Conference on Computer Vision and Pattern Recognition
	Workshop},
  year = {2004},
  volume = {10},
  pages = {154},
  address = {Washington, DC, USA},
  publisher = {IEEE Computer Society},
  abstract = {This paper presents a system for inferring complex mental states from
	video of facial expressions and head gestures in real-time. The system
	is based on a multi-level dynamic Bayesian network classifier which
	models complex mental states as a number of interacting facial and
	head displays, identified from component-based facial features. Experimental
	results for 6 mental states groups- agreement, concentrating, disagreement,
	interested, thinking and unsure are reported. Real-time performance,
	unobtrusiveness and lack of preprocessing make our system particularly
	suitable for user-independent human computer interaction.},
  file = {ElKaliouby2004.pdf:ElKaliouby2004.pdf:PDF},
  isbn = {0-7695-2158-4},
  keywords = {emotion recognition situation-acted au-features tracker-features labels-acted
	posed-data mind-reading},
  review = {Detection of emotion based on exemplars of agreeing, concentrating,
	disagreeing, interested, thinking and unsure. System uses Dynamic
	Bayesian Networks operating on movement of tracked features. Data
	set was the mind reading DVD which contains posed human expressions
	performed by actors. Expression was detected using multi level temporal
	abstraction.}
}

@INPROCEEDINGS{ElKaliouby03,
  author = {el Kaliouby, Rana and Robinson, Peter and Keates, Simeon},
  title = {Temporal Context and the Recognition of Emotion from Facial Expression},
  booktitle = {Proceedings of HCI International Conference},
  year = {2003},
  month = {June},
  abstract = {Facial displays are an important channel for the expression of emotions
	and are often thought of as projections or “read out ” of a person’s
	mental state. While it is generally believed that emotion recognition
	from facial expression improves with context, there is little literature
	available quantifying this improvement. This paper describes an experiment
	in which these effects are measured in a way that is directly applicable
	to the design of affective user interfaces. These results are being
	used to inform the design of emotion spectacles, an affective user
	interface based on the analysis of facial expressions.},
  citeulike-article-id = {790966},
  file = {:ElKaliouby03.pdf:PDF},
  keywords = {emotion face situation-acted human-performance perception annotation
	nvc-duration},
  posted-at = {2006-08-09 15:20:20},
  priority = {0},
  review = {Study of human performance in recognition of human expressions. Each
	video clip was subdivided and the last segment was shown, observer
	asked which emotion is show, then the last two, last three etc to
	investigate how the temporal micro expressions affect recognition.
	Two groups of emition were used, basic (happy, sad, surprise, etc)
	and complex (interest boredom, confusion). Results were humans could
	recognise basic emotions quickly and additional video did not improve
	accuracy while complex emotions required more time to be recognised.
	After a certain length of video, complex emotion recognition plateaued.}
}

@INPROCEEDINGS{Kanaujia2006,
  author = {Atul Kanaujia and Yuchi Huang and Dimitris Metaxas},
  title = {Emblem Detections by Tracking Facial Features},
  booktitle = {Proceedings of the Conference on Computer Vision and Pattern Recognition
	Workshop},
  year = {2006},
  pages = {108},
  address = {Washington, DC, USA},
  publisher = {IEEE Computer Society},
  abstract = {Tracking facial features across large head rotations is a challenging
	research problem. Both 2D and 3D model based approaches have been
	proposed for feature analysis from multiple views. Accurate feature
	tracking enables useful video processing applications like emblem
	detection(an event or movement that symbolizes an idea), facial expressions
	recognition, morphing and synthesis. A crucial requirement is generalizability
	of the tracking framework across appearance variations, presence
	of facial hair and illumination changes. We propose a framework to
	detect emblems that combines active shape model with a predictive
	face aspect model to track features across large head movements and
	runs close to real time. Active Shape Model(ASM) is a deformable
	model for shape registration that detect facial features by combining
	prior shape information with the observed image data. Our view based
	framework represents various head poses by multiple 2D shape models
	and accounts for large head rotations by dynamically switching between
	them. Our switching variable (the current model to use) is discriminatively
	predicted from the SIFT descriptors computed over the bounding box
	of low resolution face image. We demonstrate the use of tracking
	framework to recognize high level events like head nodding, shaking
	and eye blinking.},
  doizzz = {http://dx.doi.org/10.1109/CVPRW.2006.69},
  file = {Kanaujia2006.pdf:Kanaujia2006.pdf:PDF},
  isbn = {0-7695-2646-2},
  keywords = {basic-emotions labels-emotion asm-features pose-estimation pose-specific-mode
	sift labels-emblem corpus-cohn-kanade},
  owner = {ts00051},
  timestamp = {2009.10.26}
}

@INPROCEEDINGS{Kanluan2008,
  author = {Kanluan, Ittipan and Grimm, Michael and Kroschel, Kristian},
  title = {Audio-visual Emotion Recognition using an Emotion Space Concept},
  booktitle = {Proceedings of the 16th European Signal Processing Conference},
  year = {2008},
  address = {Lausanne, Switzerland},
  month = {August},
  abstract = {In this paper, we present novel methods for estimating spontaneously
	expressed emotions using audio-visual information. Emotions are described
	with three continuous-valued emotion primitives, namely valence,
	activation, and dominance in a 3D emotion space. We used prosodic
	and spectral features to represent the audio characteristics of the
	emotional speech. For the extraction of visual features, the 2-dimensional
	Discrete Cosine Transform (2D-DCT) was applied to blocks of a predefined
	size in facial images. Support Vector Machines (SVM) are used in
	their application for regression (Support Vector Regression, SVR)
	to estimate these 3 emotion primitives. The result showed that the
	emotion primitives activation and dominance can be best estimated
	with acoustic features, whereas the estimation of valence yields
	the best result when visual features are used. Both monomodal emotion
	estimations were subsequently fused at a decision level by a weighted
	linear combination. The average estimation error of the fused result
	was 17.6% and 12.7% below the individual error of the acoustic and
	visual emotion recognition, respectively. The correlation between
	the emotion estimates and the manual reference was increased by 12.3%
	and 9.0%, respectively.},
  file = {Kanluan2008.pdf:Kanluan2008.pdf:PDF},
  keywords = {emotion recognition abstract-emotion-scales labels-emotion feature-generation
	regression classification-svm corpus-vam situation-interview nvc-annotation
	avatar audio-features texture-feature feature-selection feature-fusion
	multimodal},
  owner = {ts00051},
  timestamp = {2010.02.19}
}

@ARTICLE{Kass1987,
  author = {Kass, Michael and Witkin, Andrew and Terzopoulos, Demetri},
  title = {Snakes: Active contour models},
  journal = {International Journal of Computer Vision (IJCV)},
  year = {1988},
  volume = {V1},
  pages = {321--331},
  number = {4},
  month = {January},
  abstract = {A snake is an energy-minimizing spline guided by external constraint
	forces and influenced by image forces that pull it toward features
	such as lines and edges. Snakes are active contour models: they lock
	onto nearby edges, localizing them accurately. Scale-space continuation
	can be used to enlarge the capture region surrounding a feature.
	Snakes provide a unified account of a number of visual problems,
	including detection of edges, lines, and subjective contours; motion
	tracking; and stereo matching. We have used snakes successfully for
	interactive interpretation, in which user-imposed constraint forces
	guide the snake near features of interest.},
  citeulike-article-id = {893626},
  doi = {10.1007/BF00133570},
  file = {Kass1987.pdf:Kass1987.pdf:PDF},
  keywords = {active\_contours, snake, model, fitting, tracking},
  posted-at = {2008-01-29 03:14:20},
  priority = {2},
  url = {http://dx.doi.org/10.1007/BF00133570}
}

@BOOK{Kassabova2008,
  title = {Bulgaria},
  publisher = {New Holland Publishers},
  year = {2008},
  author = {Kassabova, Kapka},
  keywords = {cross-cultural nvc},
  owner = {ts00051},
  review = {Comments on nodding and shaking head are reversed compared to UK},
  timestamp = {2012.01.13}
}

@ARTICLE{Kazemzadeh2013,
  author = {Kazemzadeh, Abe and Lee, Sungbok and Narayanan, Shrikanth S.},
  title = {Fuzzy Logic Models for the Meaning of Emotion Words},
  journal = {IEEE Computational Intelligence Magazine},
  year = {2013},
  volume = {8},
  pages = {34-49},
  number = {2},
  month = may,
  abstract = {This paper presents two models that use interval type-2 fuzzy sets
	(IT2 FSs) for representing the meaning of words that refer to emotions.
	In the first model, the meaning of an emotion word is represented
	by IT2 FSs on valence, activation, and dominance scales. In the second
	model, the meaning of an emotion word is represented by answers to
	an open-ended set of questions from the game of Emotion Twenty Questions
	(EMO20Q). The notion of meaning in the two proposed models is made
	explicit using the Fregean framework of extensional and intensional
	components of meaning. Inter- and intra-subject uncertainty is captured
	by using IT2 FSs learned from interval approach surveys. Similarity
	and subsethood operators are used for comparing the meaning of pairs
	of words. For the first model, we apply similarity and subsethood
	operators for the task of translating one emotional vocabulary, represented
	as a computing with words (CWW) codebook, to another. This act of
	translation is shown to be an example of CWW that is extended to
	use the three scales of valence, activation, and dominance to represent
	a single variable. We experimentally evaluate the use of the first
	model for translations and mappings between vocabularies. Accuracy
	is high when using a small emotion vocabulary as an output, but performance
	decreases when the output vocabulary is larger. The second model
	was devised to deal with larger emotion vocabularies, but presents
	interesting technical challenges in that the set of scales underlying
	two different emotion words may not be the same. We evaluate the
	second model by comparing it with results from a single-slider survey.
	We discuss the theoretical insights that the two models allow and
	the advantages and disadvantages of each.},
  keywords = {emotion, emotion space concept, emotion theories, fuzzy logic}
}

@ARTICLE{Kendon1967,
  author = {Kendon, A.},
  title = {Some functions of gaze direction in social interaction},
  journal = {Acta Psychologica},
  year = {1967},
  abstract = {Films of two-person conversations were transcribed and analyzed from
	the point of view of how gaze direction is related to utterance and
	silence. It was found that patterns of looking were systematically
	related to features of talk and could be accounted for in terms of
	the monitoring functions of gaze. At the same time, evidence was
	presented that suggested that gaze direction may also play a role
	in the regulation of turn-taking in conversation.},
  keywords = {gaze mutual conversation turn-regulation},
  optpages = {22-63},
  optvolume = {26}
}

@INCOLLECTION{Kim2010,
  author = {Kim, Minyoung and Pavlovic, Vladimir},
  title = {Structured Output Ordinal Regression for Dynamic Facial Emotion Intensity
	Prediction},
  booktitle = {Computer Vision – ECCV 2010},
  publisher = {Springer Berlin / Heidelberg},
  year = {2010},
  editor = {Daniilidis, Kostas and Maragos, Petros and Paragios, Nikos},
  volume = {6313},
  series = {Lecture Notes in Computer Science},
  pages = {649-662},
  abstract = {We consider the task of labeling facial emotion intensities in videos,
	where the emotion intensities to be predicted have ordinal scales
	(e.g., low, medium, and high) that change in time. A significant
	challenge is that the rates of increase and decrease differ substantially
	across subjects. Moreover, the actual absolute differences of intensity
	values carry little information, with their relative order being
	more important. To solve the intensity prediction problem we propose
	a new dynamic ranking model that models the signal intensity at each
	time as a label on an ordinal scale and links the temporally proximal
	labels using dynamic smoothness constraints. This new model extends
	the successful static ordinal regression to a structured (dynamic)
	setting by using an analogy with Conditional Random Field (CRF) models
	in structured classification. We show that, although non-convex,
	the new model can be accurately learned using efficient gradient
	search. The predictions resulting from this dynamic ranking model
	show significant improvements over the regular CRFs, which fail to
	consider ordinal relationships between predicted labels. We also
	observe substantial improvements over static ranking models that
	do not exploit temporal dependencies of ordinal predictions. We demonstrate
	the benefits of our algorithm on the Cohn-Kanade dataset for the
	dynamic facial emotion intensity prediction problem and illustrate
	its performance in a controlled synthetic setting.},
  affiliation = {Department of Computer Science, Rutgers University, 110 Frelinghuysen
	Road, Piscataway, NJ 08854-8019, USA},
  isbn = {978-3-642-15557-4},
  keyword = {Computer Science},
  url = {http://dx.doi.org/10.1007/978-3-642-15558-1_47}
}

@INPROCEEDINGS{Kipp2009,
  author = {Kipp, Michael and Martin, Jean-Claude},
  title = {Gesture and Emotion: Can basic gestural form features discriminate
	emotions?},
  booktitle = {Proceedings of the International Conference on Affective Computing
	\& Intelligent Interaction (ACII)},
  year = {2009},
  address = {Amsterdam},
  month = {Sept},
  abstract = {The question how exactly gesture and emotion are interrelated
	
	is still sparsely covered in research, yet highly relevant
	
	for building affective artificial agents. In our study, we
	
	investigate how basic gestural form features (handedness,
	
	hand shape, palm orientation and motion direction) are related
	
	to components of emotion. We argue that material
	
	produced by actors in filmed theater stagings are particularly
	
	well suited for such analyses. Our results indicate that
	
	there may be a universal association of gesture handedness
	
	with the emotional dimensions of pleasure and arousal. We
	
	discuss this and more specific findings, and conclude with
	
	possible implications and applications of our study.},
  file = {:Kipp2009.pdf:PDF},
  keywords = {situation-acted emotion annotation gesture},
  owner = {ts00051},
  review = {Creation of corpus of hand gestures from death of a salesman. Relation
	to emotions.},
  timestamp = {2009.11.20}
}

@INBOOK{Kittler1978,
  chapter = {Feature Set Search Algorithms},
  pages = {41-60},
  title = {Pattern Recognition and Signal Processing},
  year = {1978},
  editor = {Chen, C. H.},
  author = {Kittler, J},
  address = {Alphen aan den Rijn, The Netherlands: Sijthoff and Noordhoff},
  owner = {tim},
  timestamp = {2012.03.02}
}

@ARTICLE{Kittler2003,
  author = {Kittler, J. and Alkoot, F.M.},
  title = {Sum versus vote fusion in multiple classifier systems},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
  year = {2003},
  volume = {25},
  pages = { 110 - 115},
  number = {1},
  abstract = {Amidst the conflicting experimental evidence of superiority of one
	over the other, we investigate the Sum and majority Vote combining
	rules in a two class case, under the assumption of experts being
	of equal strength and estimation errors conditionally independent
	and identically distributed. We show, analytically, that, for Gaussian
	estimation error distributions, Sum always outperforms Vote. For
	heavy tail distributions, we demonstrate by simulation that Vote
	may outperform Sum. Results on synthetic data confirm the theoretical
	predictions. Experiments on real data support the general findings,
	but also show the effect of the usual assumptions of conditional
	independence, identical error distributions, and common target outputs
	of the experts not being fully satisfied.},
  doi = {10.1109/TPAMI.2003.1159950},
  file = {:Kittler2003.pdf:PDF},
  issn = {0162-8828},
  keywords = {combining rules; estimation error; fusion rules; majority vote; multiple
	classifier combination; multiple classifier systems; sum; pattern
	classification; pattern recognition; sensor fusion; decision-fusion}
}

@INPROCEEDINGS{Klein06,
  author = {Klein, G. S. W. and Murray, D. W.},
  title = {Full-3{D} Edge Tracking with a Particle Filter},
  booktitle = {Proceedings of the British Machine Vision Conference},
  year = {2006},
  pages = {III:1119},
  bibsource = {http://www.visionbib.com/bibliography/motion-f707.html#TT53228},
  keywords = {tracking},
  review = {Performs hidden edge removal and estimation of pose likelyhoods using
	hardware acceleration (OpenGl pixel shader). Particle model is condensation
	algorithm with annealing.}
}

@INBOOK{Knapp1978,
  chapter = {Nonverbal Communication: Basic Perspectives},
  pages = {91-106},
  title = {Shared Experiences in Human Communication},
  publisher = {Transaction Publishers},
  year = {1978},
  editor = {Tubbs, Stewart L. and Carter, Robert M.},
  author = {Knapp, Mark L.},
  keywords = {CleverHans nvc-definition NVC-important},
  owner = {ts00051},
  review = {Describes Clever Hans phenomena},
  timestamp = {2010.01.04}
}

@BOOK{Knapp2009,
  title = {Nonverbal Communication in Human Interaction},
  publisher = {Cengage Learning},
  year = {2009},
  author = {Knapp, Mark L. and Hall, Judith A.},
  abstract = {This introductory text is designed for courses in nonverbal communication.
	Using the cross-disciplinary approaches of speech and social psychology,
	Knapp and Hall center on how nonverbal communication research affects
	a wide variety of academic interests. It is the most comprehensive,
	most readable compendium of research and theory on nonverbal communication
	available. It is the standard reference in this area.},
  keywords = {nvc nvc-definition taxonomy},
  owner = {ts00051},
  timestamp = {2012.01.13}
}

@ARTICLE{Ko2008,
  author = {Ming Hsiao Ko and Geoff West and Svetha Venkatesh and Mohan Kumar},
  title = {Using dynamic time warping for online temporal fusion in multisensor
	systems},
  journal = {Information Fusion},
  year = {2008},
  volume = {9},
  pages = {370 - 388},
  number = {3},
  note = {<ce:title>Special Issue on Distributed Sensor Networks</ce:title>},
  abstract = {Sensor fusion is concerned with gaining information from multiple
	sensors by fusing across raw data, features or decisions. Traditionally
	these fusion processes only concern fusion at specific points in
	time. However recently, there is a growing interest in inferring
	the behavioural aspects of environments or objects that are monitored
	by multisensor systems, rather than just their states at specific
	points in time. In order to infer environmental behaviours, it may
	be necessary to fuse data acquired from (i) geographically distributed
	sensors at specific points of time and (ii) specific sensors over
	a period of time. Fusing multisensor data over a period of time (also
	known as Temporal fusion) is a challenging task, since the data to
	be fused consists of complex sequences that are multi-dimensional,
	multimodal, interacting, and time-varying in nature. Additionally,
	performing temporal fusion efficiently in real-time is another challenge
	due to the large amounts of data to be fused. To address this issue,
	we propose a robust and efficient framework that uses dynamic time
	warping (DTW) as the core recognizer to perform online temporal fusion
	on either the raw data or the features. We evaluate the performance
	of the online temporal fusion system on two real world datasets:
	(1) accelerometer data acquired from performing two hand gestures,
	and (2) a benchmark dataset acquired from carrying a mobile device
	and performing the predefined user scenarios. Performance results
	of the DTW-based system are compared with those of a Hidden Markov
	Model (HMM) based system. The experimental results from both datasets
	demonstrate that the proposed system outperforms HMM based systems,
	and has the capability to perform online temporal fusion efficiently
	and accurately in real-time.},
  doi = {10.1016/j.inffus.2006.08.002},
  issn = {1566-2535},
  keywords = {Dynamic time warping},
  url = {http://www.sciencedirect.com/science/article/pii/S1566253506000674}
}

@INPROCEEDINGS{Koda2007,
  author = {Tomoko Koda},
  title = {Cross-Cultural Study of Avatars' Facial Expressions and Design Considerations
	Within {Asian} Countries},
  booktitle = {Proceedings of the 1st International Conference on Intercultural
	Collaboration},
  year = {2007},
  pages = {207-220},
  abstract = {Avatars are increasingly used to express our emotions in our online
	communications. Such avatars are used based on the assumption that
	avatar expressions are interpreted universally among any cultures.
	However, our former study showed there are cultural differences in
	interpreting avatar facial expressions. This paper summarizes the
	results of cross cultural evaluations of avatar expressions among
	five Asian countries. The goals of this study are: 1) to investigate
	cultural differences in avatar expression evaluation and apply findings
	from Psychological study in human facial expression recognition,
	2) to identify design features that cause cultural differences in
	avatar facial expression interpretation. The results confirmed that
	1) there are cultural differences in interpreting avatars' facial
	expressions among Asian countries, and the psychological theory that
	suggests physical proximity affects facial expression recognition
	accuracy is also applicable to avatar facial expressions, 2) use
	of gestures and gesture marks may sometimes cause counter-effects
	in recognizing avatar facial expressions.},
  file = {:Koda2007.pdf:PDF},
  keywords = {cross-cultural perception avatar},
  owner = {tim},
  review = {Found differences in cultural interpretation of avatars},
  timestamp = {2011.07.07}
}

@ARTICLE{Koelstra2010,
  author = {S. Koelstra and M. Pantic and I. Patras},
  title = {A dynamic texture based approach to recognition of facial actions
	and their temporal models},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {2010},
  volume = {32},
  pages = {1940-1954},
  number = {11},
  month = {november}
}

@INPROCEEDINGS{Kohavi1995,
  author = {Kohavi, Ron},
  title = {A study of cross-validation and bootstrap for accuracy estimation
	and model selection},
  booktitle = {Proceedings of the 14th international joint conference on Artificial
	intelligence - Volume 2},
  year = {1995},
  series = {IJCAI'95},
  pages = {1137--1143},
  address = {San Francisco, CA, USA},
  publisher = {Morgan Kaufmann Publishers Inc.},
  acmid = {1643047},
  isbn = {1-55860-363-8},
  keywords = {cross validation},
  location = {Montreal, Quebec, Canada},
  numpages = {7},
  url = {http://dl.acm.org/citation.cfm?id=1643031.1643047}
}

@INBOOK{Krauss1996,
  chapter = {Nonverbal behavior and nonverbal communication: What do conversational
	hand gestures tell us?},
  pages = {389-450},
  title = {Advances in experimental social psychology},
  publisher = {Academic Press},
  year = {1996},
  editor = {M. Zanna},
  author = {Krauss, R. M., Chen, Y., \& Chawla, P.},
  address = {San Diego, CA},
  owner = {ts00051},
  timestamp = {2012.11.12}
}

@TECHREPORT{Kruskal1978,
  author = {Kruskal, J. B., and Wish, M.},
  title = {Multidimensional Scaling},
  institution = {Sage Publications},
  year = {1978},
  number = {07-011},
  address = {Beverly Hills and London},
  note = {Sage University Paper series on Quantitative Application in the Social
	Sciences},
  owner = {tim},
  timestamp = {2013.03.30}
}

@ARTICLE{La2000,
  author = {La Cascia, Marco and Sclaroff, Stan},
  title = {Fast, reliable head tracking under varying illumination: An approach
	based on robust registration of texture-mapped 3{D} models},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
  year = {2000},
  volume = {22},
  pages = {322-336},
  abstract = {A technique for 3D head tracking under varying illumination is proposed.
	The head is modeled as a texture mapped cylinder. Tracking is formulated
	as an image registration problem in the cylinder's texture map image.
	The resulting dynamic texture map provides a stabilized view of the
	face that can be used as input to many existing 2D techniques for
	face recognition, facial expressions analysis, lip reading, and eye
	tracking. To solve the registration problem with lighting variation
	and head motion, the residual registration error is modeled as a
	linear combination of texture warping templates and orthogonal illumination
	templates. Fast stable online tracking is achieved via regularized
	weighted least-squares error minimization. The regularization tends
	to limit potential ambiguities that arise in the warping and illumination
	templates. It enables stable tracking over extended sequences. Tracking
	does not require a precise initial model fit; the system is initialized
	automatically using a simple 2D face detector. It is assumed that
	the target is facing the camera in the first frame. The formulation
	uses texture mapping hardware. The nonoptimized implementation runs
	at about 15 frames per second on a SGI O2 graphic workstation. Extensive
	experiments evaluating the effectiveness of the formulation are reported.
	The sensitivity of the technique to illumination, regularization
	parameters, errors in the initial positioning, and internal camera
	parameters are analyzed. Examples and applications of tracking are
	reported},
  keywords = {face head-model tracking model fitting head-pose},
  owner = {ts00051},
  timestamp = {2009.10.26}
}

@ARTICLE{LakshmiDeepika2009,
  author = {Lakshmi Deepika, C. and Kandaswamy, A.},
  title = {An Algorithm for Improved Accuracy in Unimodal Biometric Systems
	through Fusion of Multiple Feature Sets},
  journal = {ICGST International Journal on Graphics, Vision and Image Processing,
	GVIP},
  year = {2009},
  volume = {09},
  pages = {33--40},
  issue = {III}
}

@INPROCEEDINGS{Lan2009,
  author = {Yuxuan Lan and Richard Harvey and Barry-John Theobald and Eng-Jon
	Ong and Richard Bowden},
  title = {Comparing Visual Features for Lipreading},
  booktitle = {Proc. of International Conference on Auditory-visual Speech Processing},
  year = {2009},
  address = {Norwich, UK},
  month = {Sept},
  owner = {tim},
  timestamp = {2013.03.30}
}

@ARTICLE{Lanzetta1970,
  author = {Lanzetta, John T. and Kleck, Robert E.},
  title = {Encoding and decoding of nonverbal affect in humans},
  journal = {Journal of Personality and Social Psychology},
  year = {1970},
  volume = {16},
  pages = {12-19},
  number = {1},
  month = {Sept},
  abstract = {12 male undergraduates viewed a sequence of equally spaced and randomly
	ordered red and green lights, in which the red light signaled the
	advent of shock. Continuous skin-resistance measures were taken.
	Ss nonverbal responses to the red and green stimuli were video taped
	without their knowledge and were later viewed by themselves and 5
	of the other Ss, individually, under conditions which required them
	to discriminate between shock and nonshock trials. Ss accuracy scores
	were above chance levels, and significant differences in error rates
	for stimulus persons but not for judges were found. Ss who were proficient
	at the discrimination task were themselves poor stimuli for others
	and vice versa. The degree of physiological reactivity of the Ss
	was positively associated with the number of errors made to them
	as stimuli, but negatively related to their error scores as judges.
	Alternative explanations for this last result are discussed. (27
	ref.)},
  keywords = {NVC perception in-group personal-differences expression},
  owner = {tim},
  timestamp = {2011.11.08}
}

@INBOOK{Leach1972,
  chapter = {The Influence of Cultural Context on Non-Verbal Communication in
	Man},
  pages = {315-347},
  title = {Non-Verbal Communication},
  publisher = {Cambridge University Press},
  year = {1972},
  editor = {Hinde, R.},
  author = {Leach, Edmund},
  owner = {tim},
  timestamp = {2013.03.30}
}

@ARTICLE{Lee2009,
  author = {Hyung-Soo Lee and Daijin Kim},
  title = {Tensor-Based AAM with Continuous Variation Estimation: Application
	to Variation-Robust Face Recognition},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2009},
  volume = {31},
  pages = {1102 -1116},
  number = {6},
  month = {june },
  abstract = {The active appearance model (AAM) is a well-known model that can represent
	a non-rigid object effectively. However, the fitting result is often
	unsatisfactory when an input image deviates from the training images
	due to its fixed shape and appearance model. To obtain more robust
	AAM fitting, we propose a tensor-based AAM that can handle a variety
	of subjects, poses, expressions, and illuminations in the tensor
	algebra framework, which consists of an image tensor and a model
	tensor. The image tensor estimates image variations such as pose,
	expression, and illumination of the input image using two different
	variation estimation techniques: discrete and continuous variation
	estimation. The model tensor generates variation-specific AAM basis
	vectors from the estimated image variations, which leads to more
	accurate fitting results. To validate the usefulness of the tensor-based
	AAM, we performed variation-robust face recognition using the tensor-based
	AAM fitting results. To do, we propose indirect AAM feature transformation.
	Experimental results show that tensor-based AAM with continuous variation
	estimation outperforms that with discrete variation estimation and
	conventional AAM in terms of the average fitting error and the face
	recognition rate.},
  doi = {10.1109/TPAMI.2008.286},
  issn = {0162-8828},
  keywords = {active appearance model;continuous variation estimation;feature transformation;tensor
	algebra;variation-robust face recognition;face recognition;Algorithms;Artificial
	Intelligence;Biometry;Computer Simulation;Face;Image Enhancement;Image
	Interpretation, Computer-Assisted;Models, Biological;Models, Statistical;Pattern
	Recognition, Automated;Reproducibility of Results;Sensitivity and
	Specificity;Subtraction Technique;}
}

@INPROCEEDINGS{Lee1999,
  author = {Lee, M. W. and Ranganath, S.},
  title = {3{D} deformable face model for pose determination and face synthesis},
  booktitle = {Proceedings of the International Conference on Image Analysis and
	Processing},
  year = {1999},
  pages = {260-265},
  abstract = {This paper describes a scheme for pose determination and face synthesis
	using a deformable 3D face model. This model is a composite of three
	sub-models: edge model, color model and a wire frame model (WFM)
	which jointly describe the shape of the face and various facial features.
	The edge and color models are used for image analysis and the WFM
	is used mainly for face synthesis. We can project this 3D model onto
	different view planes based on three rotational parameters to generate
	2D face templates for matching face images. After matching, the pose
	can be readily estimated. The composite face model is allowed to
	undergo complex deformation to handle face shape variations among
	different people. The WFM is incorporated into the model so that
	after matching, the WFM can be readily adapted to the image and image
	synthesis can be carried out. This technique can be used to analyze
	and synthesize face images over a wide range of poses},
  bibsource = {http://www.visionbib.com/bibliography/people890.html#TT71946},
  file = {:Lee1999.pdf:PDF},
  keywords = {head-pose; facial; model fitting synthesis edge head-model},
  review = {Pose is determined by matching a 3D generic face to a 2D image using
	edge and colour region matching. The texture was then extracted from
	the image to synthises new face poses.}
}

@ARTICLE{Lee1998,
  author = {Lee, V. and Beattie, G.},
  title = {The Rhetorical Organization of Verbal and Nonverbal Behavior in Emotion
	Talk},
  journal = {Semiotica},
  year = {1998},
  volume = {120},
  pages = {39-92},
  number = {1/2},
  keywords = {nvc-annotation nvc duration interview conversation smiles gaze duchenne},
  review = {Very interesting attempt to apply discourse analysis to non-verbal
	communication. Short sequences are annotated for gaze and smiles.}
}

@INPROCEEDINGS{Lehtonen1981,
  author = {Lehtonen, Jaakko},
  title = {Non-Verbal Aspects of Impromptu Speech},
  booktitle = {Problems in the Linguistic Study of Impromptu Speech},
  year = {1981},
  editor = {Enkvist, Nils Erik},
  address = {Abo, Finland},
  month = {November 20-22},
  publisher = {ERIC Clearinghouse},
  catalogue-url = { http://trove.nla.gov.au/work/154115357 },
  language = { English },
  subjects = { Communication (Thought Transfer); Discourse Analysis; Kinesthetic
	Perception; Language Research; Linguistic Theory; Nonverbal Communication;
	Oral Language; Paralinguistics; Research Methodology; Speech Acts;
	Spontaneous Behavior; Spontaneous Speech },
  type = { Article; Article/Report }
}

@INPROCEEDINGS{Leistner2010,
  author = {Christian Leistner and Amir Saffari and Horst Bischof},
  title = {{MIForests}: Multiple-Instance Learning with Randomized Trees},
  booktitle = {Proceedings of the European Conference on Computer Vision},
  year = {2010},
  abstract = {Multiple-instance learning (MIL) allows for training classifiers from
	ambiguously labeled data. In computer vision, this learning paradigm
	has been recently used in many applications such as object classification,
	detection and tracking. This paper presents a novel multiple-instance
	learning algorithm for randomized trees called MIForests. Randomized
	trees are fast, inherently parallel and multi-class and are thus
	increasingly popular in computer vision. MIForest combine the advantages
	of these classifiers with the flexibility of multiple instance learning.
	In order to leverage the randomized trees for MIL, we define the
	hidden class labels inside target bags as random variables. These
	random variables are optimized by training random forests and using
	a fast iterative homotopy method for solving the non-convex optimization
	problem. Additionally, most previously proposed MIL approaches operate
	in batch or off-line mode and thus assume access to the entire training
	set. This limits their applicability in scenarios where the data
	arrives sequentially and in dynamic environments. We show that MIForests
	are not limited to off-line problems and present an on-line extension
	of our approach. In the experiments, we evaluate MIForests on standard
	visual MIL bench-mark datasets where we achieve state-of-the-art
	results while being faster than previous approaches and being able
	to inherently solve multi-class problems. The on-line version of
	MIForests is evaluated on visual object tracking where we outperform
	the state-of-the-art method based on boosting.},
  file = {Leistner2010.pdf:Leistner2010.pdf:PDF},
  keywords = {mil supervised learning instance bag classification-method},
  owner = {tim},
  timestamp = {2011.07.11}
}

@ARTICLE{Li2004,
  author = {Stan Z. Li and Senior Member and ZhenQiu Zhang},
  title = {FloatBoost Learning and Statistical Face Detection},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
  year = {2004},
  volume = {26},
  pages = {2004},
  abstract = {A novel learning procedure, called FloatBoost, is proposed for learning
	a boosted classifier for achieving the minimum error rate. FloatBoost
	learning uses a backtrack mechanism after each iteration of AdaBoost
	learning to minimize the error rate directly, rather than minimizing
	an exponential function of the margin as in the traditional AdaBoost
	algorithms. A second contribution of the paper is a novel statistical
	model for learning best weak classifiers using a stagewise approximation
	of the posterior probability. These novel techniques lead to a classifier
	which requires fewer weak classifiers than AdaBoost yet achieves
	lower error rates in both training and testing, as demonstrated by
	extensive experiments. Applied to face detection, the FloatBoost
	learning method, together with a proposed detector pyramid architecture,
	leads to the first real-time multiview face detection system reported.},
  file = {Li2004.pdf:Li2004.pdf:PDF},
  keywords = {classification machine learning supervised method}
}

@ARTICLE{Liao2006,
  author = {Wenhui Liao and Weihong Zhang and Zhiwei Zhu and Qiang Ji and Wayne
	D. Gray},
  title = {Toward a decision-theoretic framework for affect recognition and
	user assistance},
  journal = {International Journal of Human-Computer Studies},
  year = {2006},
  volume = {64},
  pages = {847 - 873},
  number = {9},
  abstract = {There is an increasing interest in developing intelligent human–computer
	interaction systems that can fulfill two functions—recognizing user
	affective states and providing the user with timely and appropriate
	assistance. In this paper, we present a general unified decision-theoretic
	framework based on influence diagrams for simultaneously modeling
	user affect recognition and assistance. Affective state recognition
	is achieved through active probabilistic inference from the available
	multi modality sensory data. User assistance is automatically accomplished
	through a decision-making process that balances the benefits of keeping
	the user in productive affective states and the costs of performing
	user assistance. We discuss three theoretical issues within the framework,
	namely, user affect recognition, active sensory action selection,
	and user assistance. Validation of the proposed framework via a simulation
	study demonstrates its capability in efficient user affect recognition
	as well as timely and appropriate user assistance. Besides the theoretical
	contributions, we build a non-invasive real-time prototype system
	to recognize different user affective states (stress and fatigue)
	from four-modality user measurements, namely physical appearance
	features, physiological measures, user performance, and behavioral
	data. The affect recognition component of the prototype system is
	subsequently validated through a real-world study involving human
	subjects.},
  doi = {10.1016/j.ijhcs.2006.04.001},
  issn = {1071-5819},
  keywords = {Affective computing},
  url = {http://www.sciencedirect.com/science/article/pii/S1071581906000486}
}

@INPROCEEDINGS{Lien1998,
  author = {Lien, J. and T. Kanade and J. F. Cohn and C. Li and A. Zlochower},
  title = {Subtly Different Facial Expression Recognition and Expression Intensity
	Estimation},
  booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision
	and Pattern Recognition (CVPR)},
  year = {1998},
  pages = {853},
  address = {Washington, DC, USA},
  publisher = {IEEE Computer Society},
  abstract = {We have developed a computer vision system,
	
	including both facial feature extraction and recognition,
	
	that automatically discriminates among subtly different
	
	facial expressions. Expression classification is based on
	
	Facial Action Coding System (FACS) action units (AUs),
	
	and discrimination is performed using Hidden Markov
	
	Models (HMMs). Three methods are developed to extract
	
	facial expression information for automatic recognition.
	
	The first method is facial feature point tracking using a
	
	coarse-to-fine pyramid method. This method is sensitive
	
	to subtle feature motion and is capable of handling large
	
	displacements with sub-pixel accuracy. The second
	
	method is dense flow tracking together with principal
	
	component analysis (PCA), where the entire facial motion
	
	information per frame is compressed to a lowdimensional
	
	weight vector. The third method is high
	
	gradient component (i.e., furrow) analysis in the spatiotemporal
	
	domain, which exploits the transient variation
	
	associated with the facial expression. Upon extraction of
	
	the facial information, non-rigid facial expression is
	
	separated from the rigid head motion component, and the
	
	face images are automatically aligned and normalized
	
	using an affine transformation. This system also provides
	
	expression intensity estimation, which has significant
	
	effect on the actual meaning of the expression.},
  file = {Lien1998.ps:Lien1998.ps:PostScript},
  isbn = {0-8186-8497-6},
  keywords = {computer vision, face recognition, feature extraction, hidden Markov
	models, image coding, user interfacesFACS action units, Facial Action
	Coding System, action unit combinations, automated facial expression
	recognition, computer vision research, computer vision system, dense
	flow tracking, facial expression, facial expression information extraction,
	facial feature point tracking, furrow detection, hidden Markov models,
	high gradient component detection, human-computer interaction, individual
	action units, lower face action, principal component analysis, psychological
	phenomena, upper face, upper face expressions feature-generation
	tracker-features texture-features optical-flow-features hmms classification-hmms
	supervised posed-data},
  review = {FACS action detection using pyramidal LK tracking, PCA of optical
	flow into 10 eigen flows and an edge detector to find furrows. HMMs
	are trained on the concatenated vector.}
}

@INPROCEEDINGS{Lien1998a,
  author = {Lien, J. J. and Kanade, T. and Cohn, J. F. and Ching-Chung Li},
  title = {Automated facial expression recognition based on {FACS} action units},
  booktitle = {Proceedings of the 3rd IEEE International Conference. Automatic Face
	and Gesture Recognition},
  year = {1998},
  pages = {390-395},
  month = {Apr},
  abstract = {Automated recognition of facial expression is an important addition
	to computer vision research because of its relevance to the study
	of psychological phenomena and the development of human-computer
	interaction (HCI). We developed a computer vision system that automatically
	recognizes individual action units or action unit combinations in
	the upper face using hidden Markov models (HMMs). Our approach to
	facial expression recognition is based an the Facial Action Coding
	System (FACS), which separates expressions into upper and lower face
	action. We use three approaches to extract facial expression information:
	(1) facial feature point tracking; (2) dense flow tracking with principal
	component analysis (PCA); and (3) high gradient component detection
	(i.e. furrow detection). The recognition results of the upper face
	expressions using feature point tracking, dense flow tracking, and
	high gradient component detection are 85%, 93% and 85%, respectively.},
  doi = {10.1109/AFGR.1998.670980},
  file = {:Lien1998a.pdf:PDF},
  keywords = {computer vision, face recognition, feature extraction, hidden Markov
	models, image coding, user interfacesFACS action units, Facial Action
	Coding System, action unit combinations, automated facial expression
	recognition, computer vision research, computer vision system, dense
	flow tracking, facial expression, facial expression information extraction,
	facial feature point tracking, furrow detection, hidden Markov models,
	high gradient component detection, human-computer interaction, individual
	action units, lower face action, principal component analysis, psychological
	phenomena, upper face, upper face expressions feature-generation
	tracker-features texture-features optical-flow-features hmms classification-hmms
	supervised posed-data},
  owner = {ts00051},
  timestamp = {2009.10.26}
}

@ARTICLE{Likert1932,
  author = {Likert, Rensis},
  title = {A Technique for the Measurement of Attitudes},
  journal = {Archives of Psychology},
  year = {1932},
  volume = {140},
  pages = {1–55},
  keywords = {questionnaire instruments},
  owner = {ts00051},
  timestamp = {2011.12.22}
}

@ARTICLE{Lin2005,
  author = {Lin, Da Yu and Shao-Zhong Zhang and Eric Block and Lawrence C. Katz},
  title = {Encoding social signals in the mouse main olfactory bulb},
  journal = {Nature},
  year = {2005},
  volume = {434},
  pages = {470-477},
  doi = {doi:10.1038/nature03414},
  owner = {ts00051},
  timestamp = {2012.10.05}
}

@INPROCEEDINGS{Liscombe2003,
  author = {Liscombe, Jackson and Jennifer Venditti and Julia Hirschberg},
  title = {Classifying subject ratings of emotional speech using acoustic features},
  booktitle = {Proceedings of Eurospeech},
  year = {2003},
  abstract = {This paper presents results from a study examining emotional speech
	using acoustic features and their use in automatic machine learning
	classification. In addition, we propose a classification scheme for
	the labeling of emotions on continuous scales. Our findings support
	those of previous research as well as indicate possible future directions
	utilizing spectral tilt and pitch contour to distinguish emotions
	in the valence dimension.},
  file = {:Liscombe2003.pdf:PDF},
  keywords = {activation valence labels-continuous audio labels-abstract feature-generation
	audio-features abstract-emotion-scales},
  owner = {ts00051},
  timestamp = {2011.12.22}
}

@INPROCEEDINGS{Liu2000,
  author = {Zicheng Liu and Zhengyou Zhang},
  title = {Robust Head Motion Computation by Taking Advantage of Physical Properties},
  booktitle = {Proceedings of the Workshop on Human Motion},
  year = {2000},
  pages = {73-77},
  file = {:Liu2000.pdf:PDF},
  keywords = {head-model head-pose fitting lma},
  review = {Estimation of pose by minimizing the difference between tracker positions
	and projected corresponding points at an initial pose. The pose is
	incrementally improved by the Levenberg-Marquardt method. The method
	uses symmetry of the face to reduce noise. citeseer.ist.psu.edu/455580.html}
}

@ARTICLE{Liwicki2012,
  author = {S. Liwicki and G. Tzimiropoulos and S. Zafeiriou and M. Pantic},
  title = {Euler Principal Component Analysis},
  journal = {International Journal of Computer Vision},
  year = {2012},
  note = {(in press)}
}

@ARTICLE{Liwicki2012b,
  author = {S. Liwicki and G. Tzimiropoulos and S. Zafeiriou and M. Pantic},
  title = {Efficient Online Subspace Learning with an Indefinite Kernel for
	Visual Tracking and Recognition},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  year = {2012},
  volume = {23},
  pages = {1624--1636},
  month = {October}
}

@ARTICLE{Lorber2003,
  author = {Lorber, MichaelF. and O'Leary, SusanG. and Kendziora, KimberlyT.},
  title = {Mothers' Overreactive Discipline and Their Encoding and Appraisals
	of Toddler Behavior},
  journal = {Journal of Abnormal Child Psychology},
  year = {2003},
  volume = {31},
  pages = {485-494},
  number = {5},
  doi = {10.1023/A:1025496914522},
  issn = {0091-0627},
  keywords = {parenting; cognition; overreactivity; appraisal; child},
  language = {English},
  publisher = {Kluwer Academic Publishers-Plenum Publishers},
  url = {http://dx.doi.org/10.1023/A%3A1025496914522}
}

@PHDTHESIS{Lu2006,
  author = {Lu, Xiaoguang},
  title = {Three-dimensional face recognition across pose and expression},
  school = {Michigan State University},
  year = {2006},
  address = {East Lansing, MI, USA},
  note = {AAI3236364},
  advisor = {Jain, Anil K.},
  isbn = {978-0-542-90812-5},
  publisher = {Michigan State University}
}

@INPROCEEDINGS{Lucas1981,
  author = {Lucas, B. D. and Kanade, T.},
  title = {An iterative image registration technique with an application to
	stereo vision},
  booktitle = {Proceedings of Imaging understanding workshop},
  year = {1981},
  abstract = {Image registration finds a variety of applications in computer vision.
	Unfortunately, traditional image registration techniques tend to
	be costly. We present a new image registration technique that makes
	use of the spatial intensity gradient of the images to find a good
	match using a type of Newton-Raphson iteration. Our technique is
	taster because it examines far fewer potential matches between the
	images than existing techniques. Furthermore, this registration technique
	can be generalized to handle rotation, scaling and shearing. We show
	how our technique can be adapted tor use in a stereo vision system.},
  file = {Lucas1981.pdf:Lucas1981.pdf:PDF},
  keywords = {seminal tracking LK tracker},
  optpages = {21-130}
}

@INPROCEEDINGS{Lucey2009,
  author = {Lucey, Patrick and Cohn, Jeffrey and Lucey, Simon and Matthews, Iain},
  title = {Automatically Detecting Pain Using Facial Actions},
  booktitle = {Proceedings of the International Conference on Affective Computing
	\& Intelligent Interaction (ACII)},
  year = {2009},
  address = {Amsterdam},
  month = {Sept},
  abstract = {Pain is generally measured by patient self-report, normally via verbal
	communication. However, if the patient is a child or has limited
	ability to communicate (i.e. the mute, mentally impaired, or patients
	having assisted breathing) self-report may not be a viable measurement.
	In addition, these self-report measures only relate to the maximum
	pain level experienced during a sequence so a frame-by-frame measure
	is currently not obtainable. Using image data from patients with
	rotator-cuff injuries, in this paper we describe an AAM-based automatic
	system which can detect pain on a frame-by-frame level. We do this
	two ways: directly (straight from the facial features); and indirectly
	(through the fusion of individual AU detectors). From our results,
	we show that the latter method achieves the optimal results as most
	discriminant features from each AU detector (i.e. shape or appearance)
	are used.},
  file = {:Lucey2009.pdf:PDF},
  keywords = {label-pain feature-generation situation-medical corpus-UNBC-McMaster
	labels-facs multiannotator amm-features texture-features classification-svm
	heuristic decision-fusion},
  owner = {ts00051},
  review = {Use of spontanious pain videos, recognition on direct features or
	via AUs (AUs are better).},
  timestamp = {2009.11.20}
}

@INPROCEEDINGS{Lucey2006,
  author = {Lucey, Patrick and Gerasimos Potamianos},
  title = {Lipreading Using Profile Versus Frontal Views},
  booktitle = {Proceedings of the 8th IEEE Workshop on Multimedia Signal Processing},
  year = {2006},
  pages = {24-28},
  month = {Oct.},
  doi = {10.1109/MMSP.2006.285261},
  keywords = {audio-visual systems, face recognition, feature extraction, speech
	recognition, video databases, vocabularyAVASR, audio capturing, audio-visual
	automatic speech recognition, frontal image, information extraction,
	multisensory database, speakers face, visual information, vocabulary,
	speechreading, view-angle, multi-angle}
}

@INPROCEEDINGS{Lucey2000,
  author = {Simon Lucey and Sridha Sridharan and Vinod Chandran},
  title = {Initialized Eigenlip Estimator for Fast Lip Tracking Using Linear
	Regression},
  booktitle = {Proceedings of the International Conference on Pattern Recognition},
  year = {2000},
  volume = {3},
  pages = {182-185},
  address = {Barcelona, Spain},
  abstract = {Multimodal speech processing in which visual facial features are jointly
	processed with audio features is a rapidly advancing field. Lip movements
	and configurations provide useful information to improve speech and
	speaker recognition. However, the use of this visual information
	requires accurate and fast lip tracking algorithms. A new technique
	is outlined that is able to estimate the outer lip contour directly
	from a given lip intensity image via linear regression. An active
	shape model that is able to track speaker's lips without requiring
	time-consuming iterative energy minimization techniques can improve
	this estimate. Results of performance are presented against known
	tracking algorithms using the M2VTS database.},
  keywords = {asm-features feature-generation lip tracking facial feature-generation
	classification contour-feature},
  review = {Use of image intensity in the lip ROI to determine linear regression.
	The lip was represented by an active shape model. Performance was
	testing in single digit recognition task.}
}

@ARTICLE{Luettin1996,
  author = {Luettin, J. and Thacker, N.A. and Beet, S.W.},
  title = {Speechreading using shape and intensity information},
  journal = {Fourth International Conference on Spoken Language (ICSLP 96)},
  year = {1996},
  volume = {1},
  pages = {58-61 vol.1},
  month = {Oct},
  abstract = {We describe a speechreading system that uses both, shape information
	from the lip contours and intensity information from the mouth area.
	Shape information is obtained by tracking and parameterising the
	inner and outer lip boundary in an image sequence. Intensity information
	is extracted from a grey level model, based on principal component
	analysis. In comparison to other approaches, the intensity area deforms
	with the shape model to ensure that similar object features are represented
	after non-rigid deformation of the lips. We describe speaker independent
	recognition experiments based on these features and Hidden Markov
	Models. Preliminary results suggest that similar performance can
	be achieved by using either shape or intensity information and slightly
	higher performance by their combined use.},
  doi = {10.1109/ICSLP.1996.607024},
  keywords = {edge detection, feature extraction, hidden Markov models, image sequences,
	performance evaluation, speech recognition, statistical analysisboundary
	tracking, grey level model, hidden Markov models, image sequence,
	intensity information, lip contours, mouth, nonrigid deformation,
	object features, performance, principal component analysis, shape
	information, speaker independent recognition experiments, speechreading
	system, lip reading, hmm, feature-generation, deformable-roi classification-hmms
	alignment},
  review = {PCA of lip shapes was used find local grey levels perpendicular to
	lip shape. Several were combined into a global intensity vector.
	he MSE between the intensity model and image was minimised to track
	the lips. Either the intensity or the shape model or both were used
	in recognition of speech by Hidden Markov Model.}
}

@ARTICLE{Luria2009,
  author = {Luria, Gil and Rosenblum, Sara},
  title = {Comparing the Handwriting Behaviours of True and False Writing with
	Computerized Handwriting Measures},
  journal = {Applied Cognitive Psychology},
  year = {2009},
  abstract = {The goal of this study is to compare the handwriting behaviours of
	true and false writing. Based on the cognitive load and dis-automaticity
	known to be experienced while communicating a deceptive message,
	we hypothesized a difference (in temporal and spatial, pressure measures
	and peak velocities) between the handwriting of true vs. false messages.
	Thirty-four participants wrote true and false sentences on a digitizer,
	which is part of a new system called the Computerized Penmanship
	Evaluation Tool (ComPET). The ComPET evaluates brain-hand performance,
	as manifested through handwriting behaviour, and was found to be
	a valid measure for detecting the dis-automaticity that is indicative
	of certain diseases in the clinical field. Differences were found
	in mean pressure, spatial measures (mean stroke length and mean stroke
	height), but no differences were found in temporal measures and in
	the number of peak velocities. The use of ComPET in lie detection
	is discussed.},
  doi = {10.1002/acp.1621},
  keywords = {polygraph label-truth},
  owner = {ts00051},
  timestamp = {2010.01.18}
}

@INBOOK{MacKay1972,
  chapter = {Formal analysis of communicative processes},
  pages = {3-},
  title = {Non-verbal Communication},
  publisher = {Cambridge U. Press},
  year = {1972},
  editor = {Hinde, Robert A.},
  author = {MacKay, Donald M},
  owner = {tim},
  timestamp = {2013.03.30}
}

@PHDTHESIS{Madan2003,
  author = {Madan, Anmol P.},
  title = {Thin Slices of Interest},
  school = {MIT},
  year = {2003},
  abstract = {In this thesis we describe an automatic human interest detector that
	uses speech, physiology, body movement, location and proximity information.
	The speech features, consisting of activity, stress, empathy and
	engagement measures are used in three large experimental evaluations;
	measuring interest in short conversations, attraction in speed dating,
	and understanding the interactions within a focus group, all within
	a few minutes. In the conversational interest experiment, the speech
	features predict about 45% of the variance in self-reported interest
	ratings for 20 male and female participants. Stress and ac-
	
	tivity measures play the most important role, and a simple activity-based
	classifier predicts low or high interest with 74% accuracy (for men).
	
	In the speed-dating study, we use the speech features measured from
	five minutes of conversation to predict attraction between people.
	The features predict 40% of the variance in outcomes for attraction,
	friendship and business relationships. Speech features are used in
	an SVM classifier that is 75%-80% accurate in predicting outcomes
	based on speaking style.
	
	In the context of measuring consumer interest in focus groups, the
	speech features help to identify a pattern of behavior where subjects
	changed their opinions after discussion. Finally, we propose a prototype
	wearable ‘interest meter’ and various application scenarios. We portray
	a world where cell phones can automatically measure interest and
	engagement, and share this information between families and workgroups.},
  keywords = {situation-dating head-pose-features audio-features annotation labels-self-assessment
	corpus},
  owner = {ts00051},
  review = {Speed-dating},
  timestamp = {2010.02.18}
}

@INPROCEEDINGS{Madan2005,
  author = {Madan, Anmol P. and Caneel, Ron and Pentland, Alex},
  title = {Voices of Attraction},
  booktitle = {Proceedings of Augmented Cognition, HCI},
  year = {2005},
  address = {Las Vegas, NV},
  abstract = {Non-linguistic social signals (e.g., `tone of voice’) are often as
	important as linguistic content in predicting behavioural outcomes
	[1,2]. This paper describes four automated measures of such social
	signalling within the experimental context of speech in speed dating.},
  file = {:Madan2005.pdf:PDF},
  keywords = {situation-dating label-self-assessment audio-features recognition
	classification corpus-announce},
  owner = {ts00051},
  timestamp = {2010.02.18}
}

@ARTICLE{Mano1993,
  author = {Mano, Haim and Oliver, Richard L},
  title = {Assessing the Dimensionality and Structure of the Consumption Experience:
	Evaluation, Feeling, and Satisfaction},
  journal = {Journal of Consumer Research},
  year = {1993},
  volume = {20},
  pages = {451-66},
  number = {3},
  url = {http://EconPapers.repec.org/RePEc:ucp:jconrs:v:20:y:1993:i:3:p:451-66}
}

@ARTICLE{Mar2008,
  author = {Mar, R. A. and Oatley, K.},
  title = {The function of fiction is the abstraction and simulation of social
	experience.},
  journal = {Perspectives on Psychological Science},
  year = {2008},
  volume = {13},
  pages = {173-192},
  owner = {ts00051},
  timestamp = {2012.11.12}
}

@ARTICLE{Marin1991,
  author = {Maring, G and Marin, BV},
  title = {Research with Hispanic Populations},
  journal = {Applied Social Research Methods Series},
  year = {1991},
  volume = {23},
  note = {Newbury Park(CA): Sage Publications},
  keywords = {instruments, translation},
  owner = {tim},
  review = {'Cultural competence refers to the requirement that the translated
	instrument adequately reflect the cultural assumptions, norms, values,
	and expectations of the target population (Marin and Marin, 1991)'},
  timestamp = {2011.07.07}
}

@ARTICLE{Maron1998,
  author = {Maron, Oded and Lozano-P\'{e}rez, Tom\'{a}s},
  title = {A Framework for Multiple-Instance Learning},
  journal = {Advances in Neural Information Processing Systems},
  year = {1998},
  pages = {570--576},
  note = {MIT Press},
  abstract = {Multiple-instance learning is a variation on supervised learning,
	where the task is to learn a concept given positive and negative
	bags of instances. Each bag may contain many instances, but a bag
	is labeled positive even if only one of the instances in it falls
	within the concept. A bag is labeled negative only if all the instances
	in it are negative. We describe a new general framework, called Diverse
	Density, for solving multiple-instance learning problems. We apply
	this framework to learn a simple description of a person from a series
	of images (bags) containing that person, to a stock selection problem,
	and to the drug activity prediction problem.},
  file = {Maron1998.pdf:Maron1998.pdf:PDF},
  keywords = {classification-mil supervised learning method classification-method},
  owner = {ts00051},
  timestamp = {2010.03.04}
}

@ARTICLE{Marsh2003,
  author = {Marsh, Abigail A. and Elfenbein, Hillary Anger and Ambady, Nalini},
  title = {Nonverbal ``Accents'': Cultural Differences in Facial Expressions
	of Emotion},
  journal = {Psychological Science},
  year = {2003},
  volume = {14},
  pages = {373-376},
  number = {4},
  abstract = {We report evidence for nonverbal "accents," subtle differences in
	the appearance of facial expressions of emotion across cultures.
	Participants viewed photographs of Japanese nationals and Japanese
	Americans in which posers' muscle movements were standardized to
	eliminate differences in expressions, cultural or otherwise. Participants
	guessed the nationality of posers displaying emotional expressions
	at above-chance levels, and with greater accuracy than they judged
	the nationality of the same posers displaying neutral expressions.
	These findings indicate that facial expressions of emotion can contain
	nonverbal accents that identify the expresser's nationality or culture.
	Cultural differences are intensified during the act of expressing
	emotion, rather than residing only in facial features or other static
	elements of appearance. This evidence suggests that extreme positions
	regarding the universality of emotional expressions are incomplete.},
  keywords = {emotion perception cross-cultural basic-emotions psychological},
  owner = {ts00051},
  review = {Evidence that "clusters" of interpreters of NVC exist. Nationality
	of emotion encoder could be recognised.},
  timestamp = {2010.01.19}
}

@INBOOK{Martin1990,
  chapter = {The Mental Status Examination},
  title = {Clinical Methods: The History, Physical, and Laboratory Examinations},
  publisher = {Butterworths},
  year = {1990},
  editor = {Walker, H Kenneth and Hall, W Dallas and Hurst, J Willis},
  author = {Martin, DC},
  number = {207},
  address = {Boston},
  edition = {3rd edition},
  owner = {ts00051},
  timestamp = {2012.10.09}
}

@ARTICLE{Massaro90,
  author = {Massaro, DW and Cohen, MM},
  title = {Perception of synthesized audible and visible speech},
  journal = {Psychological Science},
  year = {1990},
  volume = {1},
  pages = {55-63},
  number = {{1}},
  month = {{JAN}},
  issn = {{0956-7976}},
  keywords = {mcgurk, perception-modelling, lipreading},
  review = {Paper on testing human perception of synthetic 3D visual face and
	speech audio. The tests varied sound and visual independently from
	/ba/ to /da/ to find how humans integrate multiple and contradictory
	information sources. Two models of human perception were fitted to
	the classification of /ba/ or /da/ by humans (FLMP, CMP).},
  unique-id = {{ISI:A1990EG38300012}}
}

@ARTICLE{Massaro96,
  author = {Massaro, D.W. and Ellison, J.W},
  title = {Perceptual recognition of facial affect: Cross-cultural comparisons},
  journal = {Memory \& Cognition},
  year = {1996},
  volume = {24},
  pages = {812-822},
  number = {6},
  abstract = {Previous research has shown that the perception of affect in faces
	is well described by the fuzzy logical model of perception (FLMP).
	In this study, we asked whether the processes involved in recognition
	depended on the race/culture of the face and/or of the perceiver.
	A computer-generated face was used to manipulate two features of
	facial affect: brow deflection and mouth deflection. An expanded-factorial
	design was used, with four levels of brow deflection crossed with
	four levels of mouth deflection, as well as their corresponding half-face
	conditions. Participants identified these faces as either happy or
	angry. Japanese and U.S. students were tested on faces from these
	two countries that were texture-mapped onto the animated face. The
	FLMP gave the best description of performance for both groups and
	for both types of faces. These findings challenge previous claims
	of holistic perception, categorical perception, and additive feature
	integration.},
  keywords = {perception-modelling facial emotion synthetic-data posed-data},
  review = {Study testing of human facial expression recognition of people within
	and outside an observers culture. Facial expressions were computer
	generated to change brow and mouth positions. Human response to brow
	and mouth positions may be modelled using Fuzzy Logical Model of
	Perception (FLMP).}
}

@ARTICLE{Masuda2008,
  author = {Masuda, T. and Ellsworth, P. C. and Mesquita, B. and Leu, J. and
	Tanida, S. and van de Veerdonk, E.},
  title = {Placing the face in context: Cultural differences in the perception
	of facial emotion},
  journal = {Journal of Personality and Social Psychology},
  year = {2008},
  volume = {94},
  pages = {365-381},
  abstract = {Two studies tested the hypothesis that in judging people's emotions
	from their facial expressions, Japanese, more than Westerners, incorporate
	information from the social context. In Study 1, participants viewed
	cartoons depicting a happy, sad, angry, or neutral person surrounded
	by other people expressing the same emotion as the central person
	or a different one. The surrounding people's emotions influenced
	Japanese but not Westerners' perceptions of the central person. These
	differences reflect differences in attention, as indicated by eye-tracking
	data (Study 2): Japanese looked at the surrounding people more than
	did Westerners. Previous findings on East-West differences in contextual
	sensitivity generalize to social contexts, suggesting that Westerners
	see emotions as individual feelings, whereas Japanese see them as
	inseparable from the feelings of the group.},
  keywords = {perception gaze eye-tracking emotion human-performance cross-cultural}
}

@INPROCEEDINGS{Matas2006,
  author = {Matas, J. and Zimmermann, K. and Svoboda, T. and Hilton, A.},
  title = {Learning Efficient Linear Predictors for Motion Estimation},
  booktitle = {Proceedings of the 5th Indian Conference on Computer Vision, Graphics
	and Image Processing},
  year = {2006},
  pages = {445-456},
  abstract = {A novel object representation for tracking is proposed. The tracked
	object is represented as a constellation of spatially localised linear
	predictors which are learned on a single training image. In the learning
	stage, sets of pixels whose intensities allow for optimal least square
	predictions of the transformations are selected as a support of the
	linear predictor. The approach comprises three contributions: learning
	object specific linear predictors, explicitly dealing with the predictor
	precision -- computational complexity trade-off and selecting a view-specific
	set of predictors suitable for global object motion estimate. Robustness
	to occlusion is achieved by RANSAC procedure. The learned tracker
	is very efficient, achieving frame rate generally higher than 30
	frames per second despite the Matlab implementation.},
  bibsource = {http://www.visionbib.com/bibliography/motion-i761.html#TT58827},
  keywords = {tracker},
  review = {Paper describes training of linear predictors, balancing between tracking
	and computation time and robustness to occulsion. (Need to come back
	to this when I understand linear predictors better)}
}

@INBOOK{Matsumoto2006,
  chapter = {Culture and Nonverbal Behavior},
  pages = {219-236},
  title = {The {SAGE} Handbook of Nonverbal Communication},
  publisher = {Sage Publications, Inc},
  year = {2006},
  editor = {Manusov, Valerie and Patterson, Miles L.},
  author = {Matsumoto, David},
  file = {:Matsumoto2006.pdf:PDF},
  keywords = {nvc nvc-definition taxonomy cross-cultural perception in-group translation},
  owner = {ts00051},
  review = {General review of cultural effects. Has info on Cultural Influences
	on Judgments of Emotion. Ekman found cross cultural recognition of
	primary and secondary emotion. Absolute scores may vary between cultures.
	Mixed results for in-group recognition advantage. Elfenbein and Ambady
	suggested "the ingroup hypothesis exists precisely because of non-equivalence
	in the expressions being judged".},
  timestamp = {2010.01.19}
}

@ARTICLE{Matsumoto1990,
  author = {Matsumoto, D.},
  title = {Cultural similarities and differences in display rules},
  journal = {Motivation and Emotion},
  year = {1990},
  volume = {14},
  pages = {195-214},
  number = {3},
  abstract = {Two decades of cross-cultural research on the emotions have produced
	a wealth of information concerning cultural similarities and differences
	in the communication of emotion. Still, gaps in our knowledge remain.
	This article presents a theoretical framework that predicts cultural
	differences in display rules according to cultural differences in
	individualism-collectivism (I-C) and power distance (PD; Hofstede,
	1980, 1983), and the social distinctions ingroups-outgroups and status.
	The model was tested using an American-Japanese comparison, where
	subjects in both cultures rated the appropriateness of the six universal
	facial expressions of emotion in eight different social situations.
	The findings were generally supportive of the theoretical model,
	and argue for the joint consideration of display rules and actual
	emotional behaviors in cross-cultural research.},
  keywords = {cross-cultural, display-rules, emotion, expression, social-situation,
	basic-emotions},
  owner = {tim},
  timestamp = {2011.11.08}
}

@INBOOK{Matsumoto1991,
  chapter = {Analyzing Nonverbal Behavior},
  pages = {153-165},
  title = {Practical Guild to Using Video in the Behavioral Sciences},
  publisher = {John Wiley \& Sons},
  year = {1991},
  editor = {Dowrick, Peter W.},
  author = {Matsumoto, David and Ekman, Paul and Fridlund, Alan},
  owner = {tim},
  timestamp = {2012.10.08}
}

@ARTICLE{Matsumoto2000,
  author = {Matsumoto, David and Leroux, Jeff and Wilson-Cohn, Carinda and Raroque,
	Jake and Kooken, Kristie and Ekman, Paul and Yrizarry, Nathan and
	Loewinger, Sherry and Uchida, Hideko and Yee, Albert and Lisa Amo
	and Angeline Goh},
  title = {A new test to measure emotion recognition ability: {Matsumoto} and
	{Ekman}ʼs {Japanese} and {Caucasian} Brief Affect Recognition Test
	({JACBART})},
  journal = {Journal of Nonverbal Behavior},
  year = {2000},
  volume = {24},
  pages = {179–209},
  number = {3},
  abstract = {In this article, we report the development of a new test designed
	to measure individual differences in emotion recognition ability
	(ERA), five studies examining the reliability and validity of the
	scores produced using this test, and the first evidence for a correlation
	between ERA measured by a standardized test and personality. Utilizing
	Matsumoto and Ekmans (1988) Japanese and Caucasian Facial Expressions
	of Emotion (JACFEE) and Neutral Faces (JACNeuF), we call this measure
	the Japanese and Caucasian Brief Affect Recognition Test (JACBART).
	The JACBART improves on previous measures of ERA by (1) using expressions
	that have substantial validity and reliability data associated with
	them, (2) including posers of two visibly different races (3) balanced
	across seven universal emotions (4) with equal distribution of poser
	race and sex across emotions (5) in a format that eliminates afterimages
	associated with fast exposures. Scores derived using the JACBART
	are reliable, and three studies demonstrated a correlation between
	ERA and the personality constructs of Openness and Conscientiousness,
	while one study reports a correlation with Extraversion and Neuroticism.},
  publisher = {Springer},
  url = {http://www.springerlink.com/index/L0499720741P0581.pdf}
}

@ARTICLE{Matsumoto08,
  author = {Matsumoto, David and Yoo, Seung H. and Fontaine, Johnny},
  title = {Mapping Expressive Differences Around the World: The Relationship
	Between Emotional Display Rules and Individualism Versus Collectivism},
  journal = {Journal of Cross-Cultural Psychology},
  year = {2008},
  volume = {39},
  pages = {55--74},
  number = {1},
  month = {January},
  abstract = {Despite the importance of the concept of cultural display rules in
	explaining cultural differences in emotional expression and despite
	the fact that it has been more than 30 years since this concept was
	coined, there is yet to be a study that surveys display rules across
	a wide range of cultures. This article reports such a study. More
	than 5,000 respondents in 32 countries completed the Display Rule
	Assessment Inventory. The authors examined five hypotheses concerning
	the relationship between display rules and individualism-collectivism
	(IC). The findings indicated the existence of several universal effects,
	including greater expression toward in-groups versus out-groups,
	and an overall regulation effect. Individualistic and collectivistic
	cultures differed on overall expressivity endorsement and in norms
	concerning specific emotions in in-group and out-group situations.},
  citeulike-article-id = {2301558},
  doi = {10.1177/0022022107311854},
  file = {:Matsumoto08.pdf:PDF},
  keywords = {display, emotion, rules, cross-cultural in-group display-rules emotion
	perception},
  posted-at = {2008-01-29 10:14:10},
  priority = {5},
  review = {Cross cultural survey of the expression of emotion in various social
	situations. Individual expressivity variation was larger than variations
	in the cultural mean. Levels of emotion expressivity were higher
	for close relationships in all cultures surveyed. It is speculated
	that cultural differences change the type of emotion rather than
	the expressivity in a particular circumstance. },
  url = {http://dx.doi.org/10.1177/0022022107311854}
}

@INPROCEEDINGS{Matthews1998,
  author = {Matthews, Iain and Tim Cootes and Stephen Cox and Richard Harvey
	and J. Andrew Bangham},
  title = {Lipreading Using Shape, Shading And Scale},
  booktitle = {Proceedings of International Conference on Auditory-Visual Speech
	Processing},
  year = {1998},
  pages = {73-78},
  address = {Sydney, Australia},
  month = {December},
  abstract = {This paper compares three methods of lipreading for visual and audio-visual
	speech recognition. Lip shape information is obtained using an Active
	Shape Model (ASM) lip tracker but is not as effective as modelling
	the combined shape and enclosed greylevel surface using an Active
	Appearance Model (AAM). A nontracked alternative is a nonlinear transform
	of the image using a multiscale spatial analysis (MSA). This performs
	almost identically to AAM's in both visual and audio-visual recognition
	tasks on a multi-talker database of isolated letters.},
  file = {:Matthews1998.pdf:PDF},
  keywords = {feature-generation lipreading speechreading amm-features asm-features
	texture-features classification-hmms facial},
  review = {Lip tracking using AAMs with shape and gray level models. Detection
	rates of visemes was relatively low. Results for AAMs where about
	the same as Multi Spectral Analysis (MSA).},
  url = {http://citeseer.ist.psu.edu/124620.html}
}

@ARTICLE{McCarthy2006,
  author = {McCarthy, Anjanie and Lee, Kang and Itakura, Shoji and Muir, Darwin
	W},
  title = {Cultural Display Rules Drive Eye Gaze During Thinking},
  journal = {Journal of Cross-Cultural Psychology},
  year = {2006},
  volume = {37},
  pages = {717-722},
  number = {6},
  abstract = {The authors measured the eye gaze displays of Canadian, Trinidadian,
	and Japanese participants as they answered questions for which they
	either knew, or had to derive, the answers. When they knew the answers,
	Trinidadians maintained the most eye contact, whereas Japanese maintained
	the least. When thinking about the answers to questions, Canadians
	and Trinidadians looked up, whereas Japanese looked down. Thus, for
	humans, gaze displays while thinking are at least in part culturally
	determined.},
  file = {:McCarthy2006.pdf:PDF},
  issn = {0022-0221},
  keywords = {gaze cross-cultural nvc},
  pubmedid = {19122788},
  url = {http://www.biomedsearch.com/nih/Cultural-display-rules-drive-eye/19122788.html}
}

@ARTICLE{McCowan2005,
  author = {McCowan, L. and Gatica-Perez, D. and Bengio, S. and Lathoud, G. and
	Barnard, M. and Zhang, D.},
  title = {Automatic analysis of multimodal group actions in meetings},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2005},
  volume = {27},
  pages = {305 -317},
  number = {3},
  month = {march },
  abstract = {This paper investigates the recognition of group actions in meetings.
	A framework is employed in which group actions result from the interactions
	of the individual participants. The group actions are modeled using
	different HMM-based approaches, where the observations are provided
	by a set of audiovisual features monitoring the actions of individuals.
	Experiments demonstrate the importance of taking interactions into
	account in modeling the group actions. It is also shown that the
	visual modality contains useful information, even for predominantly
	audio-based events, motivating a multimodal approach to meeting analysis.},
  doi = {10.1109/TPAMI.2005.49},
  issn = {0162-8828},
  keywords = {HMM;audio based events;audiovisual feature monitoring;automatic meeting
	analysis;group action modeling;individual participant interactions;multimodal
	group action recognition;hidden Markov models;speech processing;speech
	recognition;Algorithms;Artificial Intelligence;Behavioral Sciences;Cluster
	Analysis;Computer Simulation;Group Processes;Humans;Information Storage
	and Retrieval;Models, Biological;Models, Statistical;Pattern Recognition,
	Automated;Reproducibility of Results;Sensitivity and Specificity;Social
	Behavior;}
}

@ARTICLE{McGurk1976,
  author = {McGurk, Harry and MacDonald, John},
  title = {Hearing lips and seeing voices},
  journal = {Nature},
  year = {1976},
  volume = {264},
  pages = {746--748},
  number = {5588},
  month = {Dec},
  abstract = {MOST verbal communication occurs in contexts where the listener can
	see the speaker as well as hear him. However, speech perception is
	normally regarded as a purely auditory process. The study reported
	here demonstrates a previously unrecognised influence of vision upon
	speech perception. It stems from an observation that, on being shown
	a film of a young woman's talking head, in which repeated utterances
	of the syllable [ba] had been dubbed on to lip movements for [ga],
	normal adults reported hearing [da]. With the reverse dubbing process,
	a majority reported hearing [bagba] or [gaba]. When these subjects
	listened to the soundtrack from the film, without visual input, or
	when they watched untreated film, they reported the syllables accurately
	as repetitions of [ba] or [ga]. Subsequent replications confirm the
	reliability of these findings; they have important implications for
	the understanding of speech perception.},
  day = {23},
  doi = {10.1038/264746a0},
  keywords = {lipreading coupled perception},
  url = {http://dx.doi.org/10.1038/264746a0}
}

@ARTICLE{McHoul08,
  author = {McHoul, Alec and Rapley, Mark and Antaki, Charles},
  title = {You gotta light? On the luxury of context for understanding talk
	in interaction},
  journal = {Journal of Pragmatics},
  year = {2008},
  volume = {40},
  pages = {827-839},
  number = {1},
  month = {Jan},
  file = {:McHoul08.pdf:PDF},
  keywords = {context-important},
  review = {Meaning of an utterance can be dependant on shared knowledge of conversation
	participants, biographical information, etc. The literal meaning
	may bear no relation to the following utterances.}
}

@INBOOK{McKee1993,
  chapter = {What can be fused?},
  pages = {71 - 84},
  title = {Multisensor Fusion for Computer Vision},
  publisher = {Nato Advanced Studies Institute Series F},
  year = {1993},
  editor = {Aggarwal, J. K.},
  author = {G. T. McKee},
  volume = {99},
  owner = {tim},
  timestamp = {2013.04.07}
}

@INPROCEEDINGS{McRorie2007,
  author = {McRorie, Margaret and Sneddon, Ian},
  title = {Real Emotion Is Dynamic and Interactive},
  booktitle = {Proceedings of the International Conference on Affective Computing
	\& Intelligent Interaction (ACII)},
  year = {2007},
  volume = {4738/2007},
  pages = {759-760},
  doi = {10.1007/978-3-540-74889-2_86},
  file = {McRorie2007.pdf:McRorie2007.pdf:PDF},
  keywords = {feeltrace, spontaneous, spontaneousvsacted, photos, psychology natural-data
	abstract-emotion-scales labels-emotion natural-important rapid head
	motion},
  owner = {tim},
  timestamp = {2010.08.23}
}

@INPROCEEDINGS{Meier1999,
  author = {Uwe Meier and Rainer Stiefelhagen and Jie Yang and Alex Waibel},
  title = {Towards Unrestricted Lipreading},
  booktitle = {Proceedings of the Second International Conference on Multimodal
	Interfaces},
  year = {1999},
  abstract = {Lip reading provides useful information in speech perception and language
	understanding, especially when the auditory speech is degraded. However,
	many current automatic lip reading systems impose some restrictions
	on users. In this paper, we present our research efforts, in the
	Interactive System Laboratory, towards unrestricted lip reading.
	We first introduce a top-down approach to automatically track and
	extract lip regions. This technique makes it possible to acquire
	visual information in real-time without limiting user's freedom of
	movement. We then discuss normalization algorithms to preprocess
	images for different lightning conditions (global illumination and
	side illumination). We also compare different visual preprocessing
	methods such as raw image, Linear Discriminant Analysis (LDA), and
	Principle Component Analysis (PCA). We demonstrate the feasibility
	of the proposed methods by development of a modular system for flexible
	human-computer interaction via both visual and acous...},
  file = {:Meier1999.ps:PostScript},
  keywords = {facial feature tracking feature-generation texture-feature},
  review = {Same lip tracker as Steifelhagen 1997. Use of neutral network MS-TDNN
	for appearance based recognition of combined audio and pose normalized
	video data. Various NN approaches are used (combine audio at high
	or low level) with various data representations (raw, LDA, PCA).
	Performance is best when visual and audio are combined at the phonetic
	level. Reading german letters.}
}

@ARTICLE{Meng2007,
  author = {Meng, A. and Ahrendt, P. and Larsen, J. and Hansen, L.K.},
  title = {Temporal Feature Integration for Music Genre Classification},
  journal = {Audio, Speech, and Language Processing, IEEE Transactions on},
  year = {2007},
  volume = {15},
  pages = {1654 -1664},
  number = {5},
  month = {july },
  abstract = {Temporal feature integration is the process of combining all the feature
	vectors in a time window into a single feature vector in order to
	capture the relevant temporal information in the window. The mean
	and variance along the temporal dimension are often used for temporal
	feature integration, but they capture neither the temporal dynamics
	nor dependencies among the individual feature dimensions. Here, a
	multivariate autoregressive feature model is proposed to solve this
	problem for music genre classification. This model gives two different
	feature sets, the diagonal autoregressive (DAR) and multivariate
	autoregressive (MAR) features which are compared against the baseline
	mean-variance as well as two other temporal feature integration techniques.
	Reproducibility in performance ranking of temporal feature integration
	methods were demonstrated using two data sets with five and eleven
	music genres, and by using four different classification schemes.
	The methods were further compared to human performance. The proposed
	MAR features perform better than the other features at the cost of
	increased computational complexity.},
  doi = {10.1109/TASL.2007.899293},
  issn = {1558-7916},
  keywords = {Computational complexity;Computational efficiency;Feature extraction;Humans;Internet;Labeling;Music
	information retrieval;Reproducibility of results;Rhythm;Testing;autoregressive
	processes;computational complexity;information retrieval;music;pattern
	classification;computational complexity;diagonal autoregressive feature
	model;feature vector;information retrieval;multivariate autoregressive
	feature model;music genre classification;temporal feature integration;time
	window;Autoregressive (AR) model;music genre classification;temporal
	feature integration;}
}

@INPROCEEDINGS{Micilotta2006,
  author = {Micilotta, A.S. and Ong, E.J. and Bowden, R.},
  title = {Real-Time Upper Body Detection and 3{D} Pose Estimation in Monoscopic
	Images},
  booktitle = {Proceedings of the 9th European Conference on Computer},
  year = {2006},
  pages = {III: 139-150},
  abstract = {This paper presents a novel solution to the difficult task of both
	detecting and estimating the 3D pose of humans in monoscopic images.
	The approach consists of two parts. Firstly the location of a human
	is identified by a probabalistic assembly of detected body parts.
	Detectors for the face, torso and hands are learnt using adaBoost.
	A pose likliehood is then obtained using an a priori mixture model
	on body configuration and possible configurations assembled from
	available evidence using RANSAC. Once a human has been detected,
	the location is used to initialise a matching algorithm which matches
	the silhouette and edge map of a subject with a 3D model. This is
	done efficiently using chamfer matching, integral images and pose
	estimation from the initial detection stage. We demonstrate the application
	of the approach to large, cluttered natural images and at near framerate
	operation (16fps) on lower resolution video streams.},
  bibsource = {http://www.visionbib.com/bibliography/motion-f739.html#TT55791},
  file = {Micilotta2006.pdf:Micilotta2006.pdf:PDF},
  keywords = {pose detection},
  review = {Uses adaboost to find 2D location of face, torso and hands. Their
	locations are compared to a model of upper body poses to find most
	likely match. Applies to both images and video.}
}

@ARTICLE{Mikels2005,
  author = {Mikels, Joseph A and Fredrickson, Barbara L and Larkin, Gregory R
	and Lindberg, Casey M and Maglio, Sam J and Reuter-Lorenz, Patricia
	A},
  title = {Emotional category data on images from the International Affective
	Picture System},
  journal = {Behavior research methods},
  year = {2005},
  volume = {37},
  pages = {626--630},
  number = {4},
  publisher = {Springer}
}

@ARTICLE{Miller1955,
  author = {Miller, G. A. and Nicely, P. E.},
  title = {An analysis of perceptual confusions among some English consonants},
  journal = {Journal of the Acoustical Society of America},
  year = {1955},
  volume = {72},
  pages = {338-352},
  abstract = {Sixteen English consonants were spoken over voice communication systems
	with frequency distortion and with random masking noise. The listeners
	were forced to guess at every sound and a count was made of all the
	different errors that resulted when one sound was confused with another.
	With noise or low‐pass filtering the confusions fall into consistent
	patterns, but with high‐pass filtering the errors are scattered quite
	randomly. An articulatory analysis of these 16 consonants provides
	a system of five articulatory features or “dimensions” that serve
	to characterize and distinguish the different phonemes: voicing,
	nasality, affrication, duration, and place of articulation. The data
	indicate that voicing and nasality are little affected and that place
	is severely affected by low‐pass and noisy systems. The indications
	are that the perception of any one of these five features is relatively
	independent of the perception of the others, so that it is as if
	five separate, simple channels were involved rather than a single
	complex channel.},
  keywords = {lipreading speechreading}
}

@ARTICLE{Mognon2010,
  author = {Mognon, A. and Jovicich, J. and Bruzzone, L. and Buiatti, M.},
  title = {ADJUST: An Automatic EEG artifact Detector based on the Joint Use
	of Spatial and Temporal features},
  journal = {Psychophysiology},
  year = {2010},
  owner = {tim},
  timestamp = {2012.12.21}
}

@ARTICLE{Mohammed05,
  author = {Mohammed, T and Campbell, R and MacSweeney, M and Milne, E and Hansen,
	P and Coleman, M},
  title = {Speechreading skill and visual movement sensitivity are related in
	deaf speechreaders},
  journal = {Perception},
  year = {2005},
  volume = {2},
  pages = {205–216},
  number = {34},
  keywords = {speechreading perception facial},
  review = {A good overview of the cognitive factors of human speech reading.
	Investigation of deaf and hearing human performance in speech reading
	tests, motion-coherence threshold (MCT) and visual form-coherence
	test (FCT). Both groups had similar performance in FCT. The deaf
	group performed better at both the speed reading and MCT. This ability
	to distinguish motion may be linked with performance in speech reading.}
}

@INPROCEEDINGS{Mok2004,
  author = {Mok, L.L. and Lau, W.H. and Leung, S.H. and Wang, S.L. and Yan, H.},
  title = {Person authentication using {ASM} based lip shape and intensity information},
  booktitle = {Proceedings of the International Conference on Image Processing},
  year = {2004},
  pages = {I: 561-564},
  abstract = {An authentication system solely based on visual lip information is
	of advantage since the uttering characteristics/manner is unique
	to the individual and difficult to imitate. This paper presents a
	study of using lip shape-based and intensity features in person authentication.
	These features are derived from a 14-point active shape model (ASM)
	lip model with the use of principal component analysis (PCA). The
	differential change of the feature parameters reflecting the uttering
	characteristics are also considered in the study. A database containing
	the visual utterance of 40 speakers has been generated and each of
	the utterances is of duration 3 seconds. The visual features are
	then extracted from this database and a hidden Markov model (HMM)
	classifier is used to perform the analysis. It is observed that the
	best authentication result is obtained when the first 8 modes of
	the intensity profile is used together with the lip shaped-based
	parameters.},
  bibsource = {http://www.visionbib.com/bibliography/people915.html#TT74194},
  keywords = {facial feature-extraction asm-features classification-hmms labels-identity},
  review = {Lip tracking using an active shape model. The first three variation
	modes are used in identification. Authentication tests were performed
	on humans uttering a single phrase.}
}

@INPROCEEDINGS{Moore2009,
  author = {Moore, Stephen and Bowden, Richard},
  title = {The Effects of Pose On Facial Expression Recognition},
  booktitle = {Proceedings of the British Machine Vision Conference},
  year = {2009},
  pages = {1-11},
  address = {London},
  abstract = {Research into facial expression recognition has predominantly been
	based upon near frontal view data. However, a recent 3D facial expression
	database (BU-3DFE database) has allowed empirical investigation of
	facial expression recognition across pose. In this paper, we investigate
	the effects of pose from frontal to profile view on facial expression
	recognition. Experiments are carried out on 100 subjects with 5 yaw
	angles over 6 prototypical expressions. Expressions have 4 levels
	of intensity from subtle to exaggerated. We evaluate features such
	as local binary patterns (LBPs) as well as various extensions of
	LBPs. In addition, a novel approach to facial expression recognition
	is proposed using local gabor binary patterns (LGBPs). Multi class
	support vector machines (SVMs) are used for classification. We investigate
	the effects of image resolution and pose on facial expression classification
	using a variety of different features.},
  file = {Moore2009.pdf:Moore2009.pdf:PDF},
  keywords = {situation-acted basic-emotions lbp-features texture-features corpus-BU-3DFE
	classification-svm labels-emotion multi-angle},
  owner = {ts00051},
  timestamp = {2009.10.26}
}

@INPROCEEDINGS{Moore07,
  author = {Stephen Moore and Richard Bowden},
  title = {Automatic Facial Expression Recognition Using Boosted Discriminatory
	Classifiers},
  booktitle = {Proceedings of the 3rd IEEE International Workshop on Analysis and
	Modeling of Faces and Gestures},
  year = {2007},
  pages = {71-83},
  file = {Moore07.pdf:Moore07.pdf:PDF},
  keywords = {texture-feature emotion recognition feature-generation basic-emotions
	labels-emotion corpus-cohn-kanade},
  review = {Contour based recognition from single frames. Chamfer edges are classified
	using boosting to detect six expressions. Detection is performed
	on static images.}
}

@PHDTHESIS{Morales2001,
  author = {Morales, Leo S.},
  title = {Assessing Patient Experiences with Assessing Healthcare in Multi-Cultural
	Settings},
  school = {RAND Graduate School},
  year = {2001},
  file = {:Morales2001.pdf:PDF},
  keywords = {translation instruments cross-cultural},
  owner = {tim},
  review = {'Cross-cultural researchers differentiate between universal or common
	meaning across cultures (“etic”) and group-specific (“emic”) constructs
	or ideas.'},
  timestamp = {2011.07.07}
}

@INPROCEEDINGS{Morency2011,
  author = {Morency, Louis-Philippe},
  title = {Computational study of human communication dynamic},
  booktitle = {Proceedings of the 2011 joint ACM workshop on Human Gesture and Behavior
	Understanding},
  year = {2011},
  pages = {13--18},
  address = {New York, NY, USA},
  publisher = {ACM},
  abstract = {Face-to-face communication is a highly dynamic process where participants
	mutually exchange and interpret linguistic and gestural signals.
	Even when only one person speaks at the time, other participants
	exchange information continuously amongst themselves and with the
	speaker through gesture, gaze, posture and facial expressions. To
	correctly interpret the high-level communicative signals, an observer
	needs to jointly integrate all spoken words, subtle prosodic changes
	and simultaneous gestures from all participants. In this paper, we
	present our ongoing research effort at USC MultiComp Lab to create
	models of human communication dynamic that explicitly take into consideration
	the multimodal and interpersonal aspects of human face-to-face interactions.
	The computational framework presented in this paper has wide applicability,
	including the recognition of human social behaviors, the synthesis
	of natural animations for robots and virtual humans, improved multimedia
	content analysis, and the diagnosis of social and behavioral disorders
	(e.g., autism spectrum disorder).},
  acmid = {2072578},
  doi = {http://doi.acm.org/10.1145/2072572.2072578},
  file = {:Morency2011.pdf:PDF},
  isbn = {978-1-4503-0998-1},
  keywords = {backchannel feedback prediction, context-based recognition, human
	communication dynamics},
  location = {Scottsdale, Arizona, USA},
  numpages = {6},
  url = {http://doi.acm.org/10.1145/2072572.2072578}
}

@ARTICLE{Morrison2007,
  author = {Edward R. Morrison and Lisa Gralewski and Neill Campbell and Ian
	S. Penton-Voaka},
  title = {Facial movement varies by sex and is related to attractiveness},
  journal = {Evolution and Human Behavior},
  year = {2007},
  volume = {28},
  pages = {186-192},
  number = {3},
  file = {:Morrison2007.pdf:PDF},
  keywords = {emotion expression gender-difference},
  owner = {tim},
  timestamp = {2011.07.08}
}

@ARTICLE{Moses-Kolko2010,
  author = {Eydie L. Moses-Kolko and Susan B. Perlman and Katherine L. Wisner
	and M.S., Jeffrey James and A. Tova Saul and Mary L. Phillips},
  title = {Abnormally Reduced Dorsomedial Prefrontal Cortical Activity and Effective
	Connectivity With Amygdala in Response to Negative Emotional Faces
	in Postpartum Depression},
  journal = {American Journal of Psychiatry},
  year = {2010},
  volume = {167},
  pages = {1373-1380},
  abstract = {Objective: Postpartum major depression is a significant public health
	problem that strikes 15% of new mothers and confers adverse consequences
	for mothers, children, and families. The neural mechanisms involved
	in postpartum depression remain unknown, but brain processing of
	affective stimuli appears to be involved in other affective disorders.
	The authors examined activity in response to negative emotional faces
	in the dorsomedial pre-frontal cortex and amygdala, key emotion regulatory
	neural regions of importance to both mothering and depression.
	
	
	Method: Postpartum healthy mothers (N=16) and unmedicated depressed
	mothers (N=14) underwent functional magnetic resonance imaging blood-oxygen-level-dependent
	acquisition during a block-designed face versus shape matching task.
	A two-way analysis of variance was performed examining main effects
	of condition and group and group-by-condition interaction on activity
	in bilateral dorsomedial prefrontal cortical and amygdala regions
	of interest.
	
	
	Results: Depressed mothers relative to healthy mothers had significantly
	reduced left dorsomedial prefrontal cortical face-related activity.
	In depressed mothers, there was also a significant negative correlation
	between left amygdala activity and postpartum depression severity
	and a significant positive correlation between right amygdala activity
	and absence of infant-related hostility. There was reliable top-down
	connectivity from the left dorsomedial prefrontal cortex to the left
	amygdala in healthy, but not depressed, mothers.
	
	
	Conclusions: Significantly diminished dorsomedial prefrontal cortex
	activity and dorsomedial prefrontal cortical-amygdala effective connectivity
	in response to negative emotional faces may represent an important
	neural mechanism, or effect, of postpartum depression. Reduced amygdala
	activity in response to negative emotional faces is associated with
	greater postpartum depression severity and more impaired maternal
	attachment processes in postpartum depressed mothers.},
  keywords = {psychological perception brain emotion},
  owner = {tim},
  timestamp = {2011.07.07}
}

@ARTICLE{Motley1986,
  author = {Motley, Michael T.},
  title = {Consciousness and intentionality in communication: A preliminary
	model and methodological approaches},
  journal = {Western Journal of Speech Communication},
  year = {1986},
  volume = {50},
  pages = {3},
  number = {1},
  doi = {10.1080/10570318609374210},
  issn = {0193-6700},
  shorttitle = {Consciousness and intentionality in communication},
  url = {http://www.informaworld.com/10.1080/10570318609374210},
  urldate = {2011-03-20}
}

@INPROCEEDINGS{Mower2009,
  author = {Mower, Emily and Angeliki Metallinou and Chi-Chun Lee and Abe Kazemzadeh
	and Carlos Busso and Sungbok Lee and Shrikanth Narayanan},
  title = {Interpreting Ambiguous Emotional Expressions},
  booktitle = {Proceedings of the International Conference on Affective Computing
	\& Intelligent Interaction (ACII)},
  year = {2009},
  address = {Amsterdam},
  month = {Sept},
  abstract = {Emotion expression is a complex process involving dependencies based
	on time, speaker, context, mood, personality, and culture. Emotion
	classification algorithms designed for real-world application must
	be able to interpret the emotional content of an utterance or dialog
	given the modulations resulting from these and other dependencies.
	Algorithmic development often rests on the assumption that the input
	emotions are uniformly recognized by a pool of evaluators. However,
	this style of consistent prototypical emotion expression often does
	not exist outside of a laboratory environment. This paper presents
	methods for interpreting the emotional content of non-prototypical
	utterances. These methods include modeling across multiple time-scales
	and modeling interaction dynamics between interlocutors. This paper
	recommends classifying emotions based on emotional profiles, or soft-labels,
	of emotion expression rather than relying on just raw acoustic features
	or categorical hard labels. Emotion expression is both interactive
	and dynamic. Consequently, to accurately recognize emotional content,
	these aspects must be incorporated during algorithmic design to improve
	classification performance.},
  file = {Mower2009.pdf:Mower2009.pdf:PDF},
  keywords = {multiannotator context-important labels-continuous corpus-IEMOCAP
	posed-data labels-emotion classification-hmms taxonomy audio-features},
  owner = {ts00051}
}

@INPROCEEDINGS{Nakatsu98,
  author = {Rhyohei Nakatsu},
  title = {Nonverbal information recognition and its application to communications},
  booktitle = {Proceedings of the 6th ACM International Conference on Multimedia},
  year = {1998},
  pages = {2-9},
  address = {New York, NY, USA},
  publisher = {ACM},
  abstract = {The development of sophisticated human interface technologies are
	highly desired for facilitating human-machine communications. The
	human interface is defined as technologies for supporting human communications
	and, therefore, should take the various aspects of communications
	into consideration. In this paper, various forms of human communications
	are examined and in particular the nonverbal aspects of communications
	are shown to be essential. Also it is reported that systematic research
	to treat nonverbal information in communications should be carried
	out. Several examples of this type of research are described},
  doi = {http://doi.acm.org/10.1145/306668.306673},
  file = {:Nakatsu98.pdf:PDF},
  isbn = {1-58113-163-1},
  keywords = {nvc hci nvc-applications nvc-important},
  location = {Bristol, United Kingdom},
  review = {Overview of role of non verbal communication. Brief description on
	variation in speech rate in human dialogue. Describes neural network
	for audio recognition of human emotion. Brief description of facial
	expression recognition using a helmet mounted camera using DCT.}
}

@ARTICLE{Natsuki2008,
  author = {Sano Natsuki and Suzuki Hideo and Koda Masato},
  title = {A Robust Ensemble Learning Using Zero-One Loss Function},
  journal = {Journal of the Operations Research Society of Japan},
  year = {2008-03},
  volume = {51},
  pages = {95-110},
  number = {1},
  issn = {04534514},
  publisher = {The Operations Research Society of Japan},
  url = {http://ci.nii.ac.jp/naid/110006632507/en/}
}

@ARTICLE{Nefian2002,
  author = {Ara V. Nefian and Luhong Liang and Xiaobo Pi and Xiaoxing Liu and
	Kevin Murphy},
  title = {Dynamic Bayesian networks for audio-visual speech recognition},
  journal = {EURASIP Journal on Applied Signal Processing},
  year = {2002},
  volume = {2002},
  pages = {1274--1288},
  number = {1},
  abstract = {The use of visual features in audio-visual speech recognition (AVSR)
	is justified by both the speech generation mechanism, which is essentially
	bimodal in audio and visual representation, and by the need for features
	that are invariant to acoustic noise perturbation. As a result, current
	AVSR systems demonstrate significant accuracy improvements in environments
	affected by acoustic noise. In this paper, we describe the use of
	two statistical models for audio-visual integration, the coupled
	HMM (CHMM) and the factorial HMM (FHMM), and compare the performance
	of these models with the existing models used in speaker dependent
	audio-visual isolated word recognition. The statistical properties
	of both the CHMM and FHMM allow to model the state asynchrony of
	the audio and visual observation sequences while preserving their
	natural correlation over time. In our experiments, the CHMM performs
	best overall, outperforming all the existing models and the FHMM.},
  address = {New York, NY, United States},
  doi = {http://dx.doi.org/10.1155/S1110865702206083},
  file = {:Nefian2002.pdf:PDF},
  issn = {1110-8657},
  keywords = {speech recognition classification-hmms lips classification-dbn texture-features
	feature-generation audio-features mouth contour},
  publisher = {Hindawi Publishing Corp.},
  review = {This paper describes visual and audio data being classified by factorial
	HMM or coupled HMM to recognise spoken number digits. The visual
	system is appearanced based using LDA on RGB pixel colour. The lip
	contour is found using binary chain encoding then smoothed. The pose
	of the mouth is determined from the lip contour and the pixels are
	normalised for pose. The normalised lip region is then split into
	8 blocks and 2D DCT is applied. This method has full source code
	available. This method could be used in comparison to our system.}
}

@ARTICLE{Negi2009,
  author = {Negi, Janak Singh},
  title = {The Role of Teachers’ Non-Verbal Communication in ELT Classroom},
  journal = {Journal of NELTA},
  year = {2009},
  volume = {14},
  pages = {101-110},
  number = {1\&2},
  month = {December},
  owner = {tim},
  timestamp = {2013.03.30}
}

@INPROCEEDINGS{Nicolaou2011,
  author = {M. A. Nicolaou and H. Gunes and M. Pantic},
  title = {Output-Associative {RVM} Regression for Dimensional and Continuous
	Emotion Prediction},
  booktitle = {Proceedings of IEEE International Conference on Automatic Face and
	Gesture Recognition},
  year = {2011},
  pages = {16--23},
  address = {Santa Barbara, CA, USA},
  month = {March},
  abstract = {Many problems in machine learning and computer vision consist of predicting
	multi-dimensional output vectors given a specific set of input features.
	In many of these problems, there exist inherent temporal and spacial
	dependencies between the output vectors, as well as repeating output
	patterns and input-output associations, that can provide more robust
	and accurate predictors when modelled properly. With this intrinsic
	motivation, we propose a novel Output-Associative Relevance Vector
	Machine (OA-RVM) regression framework that augments the traditional
	RVM regression by being able to learn non-linear input and output
	dependencies. Instead of depending solely on the input patterns,
	OA-RVM models output structure and covariances within a predefined
	temporal window, thus capturing past, current and future context.
	As a result, output patterns manifested in the training data are
	captured within a formal probabilistic framework, and subsequently
	used during inference. As a proof of concept, we target the highly
	challenging problem of dimensional and continuous prediction of emotions
	from naturalistic facial expressions. We demonstrate the advantages
	of the proposed OA-RVM regression by performing both subject-dependent
	and subject-independent experiments using the SAL database. The experimental
	results show that OA-RVM regression outperforms the traditional RVM
	and SVM regression approaches in prediction accuracy, generating
	more robust and accurate models.},
  file = {:Nicolaou2011.pdf:PDF},
  keywords = {regression sliding-window feature-generation corpus-Belfast-SAL classification-svm
	classification-rvm situation-woz abstract-emotion-scales labels-emotion
	tracker-features temporal}
}

@ARTICLE{Nicolaou2011b,
  author = {Nicolaou, M.A. and Gunes, H. and Pantic, M.},
  title = {Continuous prediction of spontaneous affect from multiple cues and
	modalities in valence-arousal space},
  journal = {Affective Computing, IEEE Transactions on},
  year = {2011},
  volume = {2},
  pages = {92-105},
  number = {2},
  file = {:Nicolaou2011b.pdf:PDF},
  publisher = {IEEE}
}

@ARTICLE{Nielsen1991,
  author = {Nielsen, Tore A. and Deslauriers, Daniel and Baylor, George W.},
  title = {Emotions in dream and waking event reports},
  journal = {Dreaming: Journal of the Association for the Study of Dreams},
  year = {1991},
  volume = {1},
  pages = {287-300},
  number = {4},
  month = {Dec},
  owner = {ts00051},
  timestamp = {2012.11.12}
}

@ARTICLE{Norman2010,
  author = {Norman, Geoff},
  title = {Likert scales, levels of measurement and the “laws” of statistics},
  journal = {Advances in Health Sciences Education},
  year = {2010},
  volume = {15},
  pages = {625-632},
  note = {10.1007/s10459-010-9222-y},
  abstract = {Reviewers of research reports frequently criticize the choice of statistical
	methods. While some of these criticisms are well-founded, frequently
	the use of various parametric methods such as analysis of variance,
	regression, correlation are faulted because: (a) the sample size
	is too small, (b) the data may not be normally distributed, or (c)
	The data are from Likert scales, which are ordinal, so parametric
	statistics cannot be used. In this paper, I dissect these arguments,
	and show that many studies, dating back to the 1930s consistently
	show that parametric statistics are robust with respect to violations
	of these assumptions. Hence, challenges like those above are unfounded,
	and parametric methods can be utilized without concern for “getting
	the wrong answer”.},
  affiliation = {McMaster University, 1200 Main St. W., Hamilton, ON L8N3Z5, Canada},
  issn = {1382-4996},
  issue = {5},
  keyword = {Humanities, Social Sciences and Law},
  publisher = {Springer Netherlands},
  url = {http://dx.doi.org/10.1007/s10459-010-9222-y}
}

@INPROCEEDINGS{Oertel2010,
  author = {Oertel, C. and F. Cummins and N. Campbell and J. Edlund and P. Wagner},
  title = {{D64}: A corpus of richly recorded conversational interaction},
  booktitle = {Proceedings of the Workshop on Multimodal Corpora: Advances in Capturing,
	Coding and Analyzing Multimodality (LREC)},
  year = {2010},
  address = {Valetta, Malta},
  abstract = {Rich non-intrusive recording of a naturalistic conversation was conducted
	in a domestic setting. Four (sometimes five) participants engaged
	in lively conversation over two 4-hour sessions on two successive
	days. Conversation was not directed, and ranged widely over topics
	both trivial and technical. The entire conversation, on both days,
	was richly recorded using 7 video cameras, 10 audio microphones,
	and the registration of 3-D head, torso and arm motion using an Optitrack
	system. To add liveliness to the conversation, several bottles of
	wine were consumed during the final two hours of recording. The resulting
	corpus will be of immediate interest to all researchers interested
	in studying naturalistic, ethologically situated, conversational
	interaction.},
  file = {:Oertel2010.pdf:PDF},
  keywords = {corpus corpus-D64 natural-data situation-casual corpus-announce},
  owner = {ts00051},
  timestamp = {2012.01.10}
}

@ARTICLE{Oikonomopoulos2011,
  author = {A. Oikonomopoulos and I. Patras and M. Pantic},
  title = {Spatiotemporal Localization and Categorization of Human Actions in
	Unsegmented Image Sequences},
  journal = {IEEE Transactions on Image Processing},
  year = {2011},
  volume = {20},
  pages = {1126--1140},
  number = {4},
  month = {April}
}

@ARTICLE{Ojala2002,
  author = {Ojala, Timo and Pietik\"{a}inen, Matti and M\"{a}enp\"{a}\"{a}, Topi},
  title = {Multiresolution Gray-Scale and Rotation Invariant Texture Classification
	with Local Binary Patterns},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
  year = {2002},
  volume = {24},
  pages = {971--987},
  number = {7},
  abstract = {Presents a theoretically very simple, yet efficient, multiresolution
	approach to gray-scale and rotation invariant texture classification
	based on local binary patterns and nonparametric discrimination of
	sample and prototype distributions. The method is based on recognizing
	that certain local binary patterns, termed "uniform," are fundamental
	properties of local image texture and their occurrence histogram
	is proven to be a very powerful texture feature. We derive a generalized
	gray-scale and rotation invariant operator presentation that allows
	for detecting the "uniform" patterns for any quantization of the
	angular space and for any spatial resolution and presents a method
	for combining multiple operators for multiresolution analysis. The
	proposed approach is very robust in terms of gray-scale variations
	since the operator is, by definition, invariant against any monotonic
	transformation of the gray scale. Another advantage is computational
	simplicity as the operator can be realized with a few operations
	in a small neighborhood and a lookup table. Experimental results
	demonstrate that good discrimination can be achieved with the occurrence
	statistics of simple rotation invariant local binary patterns},
  address = {Washington, DC, USA},
  doi = {http://dx.doi.org/10.1109/TPAMI.2002.1017623},
  file = {:Ojala2002.pdf:PDF},
  issn = {0162-8828},
  keywords = {lbp feature-generation lbp-features dct-features texture-features},
  publisher = {IEEE Computer Society}
}

@INPROCEEDINGS{Okuma2004,
  author = {Kenji Okuma and Ali Taleghani and O De Freitas and James J. Little
	and David G. Lowe},
  title = {A boosted particle filter: Multitarget detection and tracking},
  booktitle = {Proceedings of the European Conference on Computer Vision},
  year = {2004},
  pages = {28--39},
  abstract = {The problem of tracking a varying number of non-rigid objects has
	two major difficulties. First, the observation models and target
	distributions can be highly non-linear and non-Gaussian. Second,
	the presence of a large, varying number of objects creates complex
	interactions with overlap and ambiguities. To surmount these difficulties,
	we introduce a vision system that is capable of learning, detecting
	and tracking the objects of interest. The system is demonstrated
	in the context of tracking hockey players using video sequences.
	Our approach combines the strengths of two successful algorithms:
	mixture particle filters and Adaboost. The mixture particle filter
	[17] is ideally suited to multi-target tracking as it assigns a mixture
	component to each player. The crucial design issues in mixture particle
	filters are the choice of the proposal distribution and the treatment
	of objects leaving and entering the scene.},
  keywords = {tracking},
  owner = {ts00051},
  timestamp = {2009.10.26},
  vol = {3021}
}

@INPROCEEDINGS{Okwechime2011,
  author = {Okwechime, D. Ong, E-J. and Gilbert, A. and Bowden, R.},
  title = {Visualisation and Prediction of Conversation Interest through Mined
	Social Signals},
  booktitle = {IEEE International Workshop on Social Behavior Analysis},
  year = {2011},
  owner = {ts00051},
  timestamp = {2012.05.24}
}

@INPROCEEDINGS{Ong2009,
  author = {Ong, E. and Y. Lan and B. Thobald and R. Harvey and R. Bowden},
  title = {Robust Facial Feature Tracking using Multiscale Biased Linear Predictors},
  booktitle = {Proceedings of the International Conference on Computer Vision},
  year = {2009},
  file = {Ong2009.pdf:Ong2009.pdf:PDF},
  keywords = {facial feature tracker},
  owner = {ts00051},
  timestamp = {2009.10.26}
}

@INPROCEEDINGS{Ong2011,
  author = {Eng-Jon Ong and Bowden, R.},
  title = {Learning temporal signatures for Lip Reading},
  booktitle = {Computer Vision Workshops (ICCV Workshops), 2011 IEEE International
	Conference on},
  year = {2011},
  pages = {958 -965},
  month = {nov.},
  abstract = {This paper attempts to tackle the problem of lipreading by building
	visual sequence classifiers that are based on salient temporal signatures.
	The temporal signatures used in this paper allow us to capture spatio-temporal
	information that can span multiple feature dimensions with gaps in
	the temporal axis. Selecting suitable temporal signatures by exhaustive
	search is not possible given the immensely large search space. As
	an example, the temporal sequence used in this paper would require
	exhaustively evaluating 22000 temporal signatures which is simply
	not possible. To address this, a novel gradient-descent based method
	is proposed to search for a suitable candidate temporal signature.
	Crucially, this is achieved very efficiently with O(nD) complexity,
	where D is the static feature vector dimensionality and n the maximum
	length of the temporal signatures considered. We then integrate this
	temporal search method into the AdaBoost algorithm. The results are
	spatio-temporal strong classifiers that can be applied to multi-class
	recognition in the lipreading domain. We provide experimental results
	evaluating the performance of our method against existing work in
	both subject dependent and subject independent cases demonstrating
	state of the art performance. Importantly, this was also achieved
	with a small set of temporal signatures.},
  doi = {10.1109/ICCVW.2011.6130355},
  keywords = {AdaBoost algorithm;O(nD) complexity;gradient-descent based method;lip
	reading;multiclass recognition;spatio-temporal information;static
	feature vector dimensionality;temporal signature learning;visual
	sequence classifier;computational complexity;face recognition;gesture
	recognition;gradient methods;image classification;image sequences;learning
	(artificial intelligence);}
}

@INPROCEEDINGS{Ong2008,
  author = {Ong, Eng-Jon and Bowden, Richard},
  title = {Robust Lip-Tracking using Rigid Flocks of Selected Linear Predictors},
  booktitle = {Proceedings of the 8th IEEE International Conference on Automatic
	Face and Gesture Recognition},
  year = {2008},
  note = {In press.},
  file = {Ong2008.pdf:Ong2008.pdf:PDF},
  keywords = {feature tracker}
}

@ARTICLE{Ong2006,
  author = {Eng-Jon Ong and Antonio S. Micilotta and Richard Bowden and Adrian
	Hilton},
  title = {Viewpoint invariant exemplar-based 3{D} human tracking},
  journal = {Computer Vision and Image Understanding: CVIU},
  year = {2006},
  volume = {104},
  pages = {178--189},
  number = {2},
  address = {New York, NY, USA},
  doi = {http://dx.doi.org/10.1016/j.cviu.2006.08.004},
  file = {Ong2006.pdf:Ong2006.pdf:PDF},
  issn = {1077-3142},
  keywords = {tracking linear-predictor},
  publisher = {Elsevier Science Inc.},
  review = {Trains a model of transitions between clusters of exemplars in walking
	using multiple cameras. Tracking is done with one camera using a
	particle algorithm based on transitions between exemplar clusters.}
}

@BOOK{Ortony1988,
  title = {The Cognitive Structure of Emotion},
  publisher = {Cambridge University Press},
  year = {1988},
  author = {Ortony, A. and Clore, G. L. and Collins, A.},
  address = {Cambridge, UK},
  abstract = {It has long been clear that the way in which people interpret the
	world affects our emotional reactions. What has been less clear is
	exactly how such different interpretations lead to different emotions.
	This is the central question addressed by The Cognitive Structure
	of Emotions. Taking a cognitive science perspective, a systematic
	account is presented of the cognitive structures that underlie a
	wide range of different emotions. Detailed proposals about the factors
	that affect intensity are also offered. The authors propose three
	broad classes of emotions, each corresponding to a different attentional
	focus. One class consists of reactions to events, one of reactions
	to the actions of agents, and one of reactions to objects. By basing
	their analysis of the antecedents of emotions on an analysis of the
	perceived situational conditions that elicit them, the authors offer
	the prospect of accounting for variations in the emotions of different
	individuals, different cultures, and perhaps even different species.},
  keywords = {emotion cross-cultural perception emotion-taxonomy},
  owner = {ts00051},
  review = {Review at: www.aaai.org/ojs/index.php/aimagazine/article/download/926/844},
  timestamp = {2011.08.09}
}

@PHDTHESIS{Owuor2001,
  author = {Owuor, Charles Ochieng},
  title = {Implications of using Likert data in multiple regression analysis},
  school = {University of British Columbia},
  year = {2001},
  abstract = {Many of the measures obtained in educational research are Likert-type
	responses on questionnaires. These Likert-type variables are sometimes
	used in ordinary least-squares regression analysis. However, among
	the key implications of the assumptions of regression is that the
	criterion is continuous. Little research has been done to examine
	how much information is lost and how inappropriate it is to use Likert
	variables in ordinary least-squares multiple regression. Therefore,
	this study examined the effect of Likert-type responses in the criterion
	variable and predictors for various scale points, on the accuracy
	of regression models using normal and skewed observed response patterns.
	This was done for the case of three predictors and one criterion.
	Similarly, eight levels of Likert-type categorization ranging from
	two to nine scale points were considered for both predictors and
	criterion variables. It was found that the largest bias in the estimation
	of the model R-squared, the relative Pratt Index, and Pearson correlation
	coefficient occurred for two or three-point Likert scales. The bias
	did not substantially reduce any further beyond the four-point Likert
	scale. Type of correlation matrix had no effect on the model fit.
	However, skewed response distribution resulted in large biases in
	both R² and Pearson correlation, but not in Relative Pratt index,
	which was not affected by the response distribution. Practical contribution
	and significance of the study is that it has provided information
	and insight on how much information is lost due to bias, and the
	extent to which accuracy is compromised in using Likert data in linear
	regression models in education and social science research. It is
	recommended that researchers and practitioners should recognize the
	extent of the bias in ordinary least-squares regression models with
	Likert data, resulting in substantial loss of information. For variable
	importance, the relative Pratt index should be used given that it
	is robust to Likert conditions and response distributions. Finally,
	when interpreting reported regression results in the research literature
	one should recognize that the reported R-squared values are underestimated
	and that the Pearson correlations are also typically underestimated
	and sometimes substantially underestimated.},
  owner = {tim},
  timestamp = {2012.11.01}
}

@INBOOK{Pantic2009,
  chapter = {Machine analysis of facial behaviour: Naturalistic and dynamic behaviour},
  pages = {3505-3513},
  title = {Philosophical Transactions of Royal Society B},
  publisher = {Royal Society},
  year = {2009},
  editor = {Robinson, Peter and el Kaliouby, Rana},
  author = {Pantic, Maja},
  volume = {364},
  abstract = {This article introduces recent advances in the machine analysis of
	facial expressions. It describes the problem space, surveys the problem
	domain and examines the state of the art. Two recent research topics
	are discussed with particular attention: analysis of facial dynamics
	and analysis of naturalistic (spontaneously displayed) facial behaviour.
	Scientific and engineering challenges in the field in general, and
	in these specific subproblem areas in particular, are discussed and
	recommendations for accomplishing a better facial expression measurement
	technology are outlined.},
  file = {Pantic2009.pdf:Pantic2009.pdf:PDF},
  keywords = {survey-paper emotion recognition},
  owner = {ts00051},
  timestamp = {2010.01.07}
}

@ARTICLE{Pantic2008,
  author = {Pantic, Maja and Nijholt, Anton and Pentland, Alex and Huanag, Thomas
	S.},
  title = {Human-Centred Intelligent Human Computer Interaction ({HCI2}): how
	far are we from attaining it?},
  journal = {International Journal of Autonomous and Adaptive Communications Systems},
  year = {2008},
  volume = {1},
  pages = {168--187},
  number = {2},
  abstract = {A widely accepted prediction is that computing will move to the background,
	weaving itself into the fabric of our everyday living spaces and
	projecting the human user into the foreground. To realise this prediction,
	next-generation computing should develop anticipatory user interfaces
	that are human-centred, built for humans and based on naturally occurring
	multimodal human communication. These interfaces should transcend
	the traditional keyboard and mouse and have the capacity to understand
	and emulate human communicative intentions as expressed through behavioural
	cues, such as affective and social signals. This article discusses
	how far we are to the goal of human-centred computing and Human-Centred
	Intelligent Human-Computer Interaction (HCI2) that can understand
	and respond to multimodal human communication.},
  address = {Inderscience Publishers, Geneva, SWITZERLAND},
  doi = {http://dx.doi.org/10.1504/IJAACS.2008.019799},
  file = {Pantic2008.pdf:Pantic2008.pdf:PDF},
  issn = {1754-8632},
  keywords = {review-paper context-important nvc-applications},
  publisher = {Inderscience Publishers}
}

@ARTICLE{Pantic2006,
  author = {Pantic, M. and Patras, I.},
  title = {Dynamics of Facial Expression: Recognition of Facial Actions and
	Their Temporal Segments From Face Profile Image Sequences},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics},
  year = {2006},
  volume = {36},
  pages = {433-449},
  number = {2},
  month = {April},
  bibsource = {http://www.visionbib.com/bibliography/people921.html#TT74726},
  file = {Pantic2006.pdf:Pantic2006.pdf:PDF},
  keywords = {facial expression recognition tracking label-facs tracker-features
	automatic-segmentation corpus-cohn-kanade classification-rule-based},
  review = {Detection of AUs from video of face. The pose used was side profile.
	Tracking was based on an auxiliary particle filter method and colour.
	Multi-level recognition of expressions. Temporal dynamics of AUs
	used recognition.
	
	
	Interestingly they use distances between points but discretise the
	results.}
}

@INPROCEEDINGS{Pantic2000,
  author = {Maja Pantic and L\'{e}on J. M. Rothkrantz},
  title = {An Expert System for Recognition of Facial Actions and their Intensity},
  booktitle = {Proceedings of the Seventeenth National Conference on Artificial
	Intelligence and Twelfth Conference on Innovative Applications of
	Artificial Intelligence},
  year = {2000},
  pages = {1026-1033},
  publisher = {AAAI Press / The MIT Press},
  abstract = {The Facial Action Coding System (FACS) is an objective method for
	quantifying facial movement in terms of 44 component actions, i.e.
	Action Units (AUs). This system is widely used in behavioral investigations
	of emotion, cognitive process and social interaction. Highly trained
	human experts (FACS coders) presently perform the coding. This paper
	presents a system that can automatically recognize 30 AUs, their
	combinations and their intensity. The system employs a framework
	for hybrid facial feature detection and an expert system for facial
	action coding in static dual­view facial images. Per facial feature,
	multiple feature detection techniques are applied and the resulting
	redundant data is reduced so that an unequivocal facial expression
	geometry ensues. Reasoning with uncertainty is used to encode and
	quantify the encountered facial actions based on the determined expression
	geometry and the certainty of that data. Eight certified FACS coders
	tested the system. The recognition results demonstrated rather high
	concurrent validity with human coding.},
  file = {:Pantic2000.pdf:PDF},
  isbn = {0-262-51112-6},
  keywords = {labels_facs annotation detection_features point_features classification_expert_system
	emotion recognition},
  review = {System uses feature specialized detectors to find features in dual
	views of faces. The positions are classified using a rule based expert
	system referring a database of 30 FACS extreme deformations. A further
	expert system takes FACS data and classifies into one of the six
	exemplar emotions.}
}

@ARTICLE{Pantic2009b,
  author = {M Pantic and A Vinciarelli},
  title = {Implicit Human Centered Tagging},
  journal = {IEEE Signal Processing Magazine},
  year = {2009},
  volume = {26},
  pages = {173-180},
  number = {6},
  abstract = {Tagging is the annotation of multimedia data with user-specified keywords
	known as tags, with the aim of facilitating fast and accurate data
	retrieval based on these tags. In contrast to this process, also
	referred to as Explicit Tagging, Implicit Human-Centered Tagging
	(IHCT) refers to exploiting the information on user’s nonverbal reactions
	(e.g., facial expressions like smiles or head gestures like shakes)
	to multimedia data, with which he or she interacts, to assign new
	or improve the existing tags associated with the target data. Thus,
	implicit tagging allows that a data item gets tagged each time a
	user interacts with it based on the reactions of the user to the
	data (e.g., laughter when seeing a funny video), in contrast to explicit
	tagging paradigm in which a data item gets tagged only if a user
	is requested (or chooses) to associate tags with it. As nonverbal
	reactions to observed multimedia are displayed naturally and spontaneously,
	no purposeful explicit action (effort) is required from the user;
	hence, the resulting tagging process is said to be “implicit” and
	“human-centered” (in contrast to being dictated by computer and being
	“computer-centered”). Tags obtained through IHCT are expected to
	be more robust than tags associated with the data explicitly, at
	least in terms of generality and statistical reliability. To wit,
	a number of human behaviors are universally displayed and perceived
	– e.g., basic emotions like happiness, disgust and fear – and these
	could be associated to IHCT tags such as “funny” and “horror”, which
	would make sense to everybody (generality) and would be sufficiently
	represented (statistical reliability).},
  url = {http://ibug.doc.ic.ac.uk/media/uploads/documents/SPM-PanticVinciarelli-CAMERA.pdf}
}

@INPROCEEDINGS{Patras2004,
  author = {Ioannis Patras and Maja Pantic},
  title = {Particle Filtering with Factorized Likelihoods for Tracking Facial
	Features},
  booktitle = {Proceedings of the Sixth IEEE International Conference on Automatic
	Face and Gesture Recognition},
  year = {2004},
  pages = {97-104},
  address = {Seoul, Korea},
  month = {May 17-19},
  publisher = {IEEE Computer Society},
  abstract = {In the recent years particle filtering has been the dominant paradigm
	for tracking facial and body features, recognizing temporal events
	and reasoning in uncertainty. A major problem associated with it
	is that its performance deteriorates drastically when the dimensionality
	of the state space is high. In this paper, we address this problem
	when the state space can be partitioned in groups of random variables
	whose likelihood can be independently evaluated. We introduce a novel
	proposal density which is the product of the marginal posteriors
	of the groups of random variables. The proposed method requires only
	that the interdependencies between the groups of random variables
	(i.e. the priors) can be evaluated and not that a sample can be drawn
	from them. We adapt our scheme to the problem of multiple template-based
	tracking of facial features. We propose a color-based observation
	model that is invariant to changes in illumination intensity. We
	experimentally show that our algorithm clearly outperforms multiple
	independent template tracking schemes and auxiliary particle filtering
	that utilizes priors.},
  citedby = {0},
  cites = {0},
  doi = {http://csdl.computer.org/comp/proceedings/fgr/2004/2122/00/21220097abs.htm},
  file = {:Patras2004.pdf:PDF},
  researchr = {http://researchr.org/publication/PatrasP04}
}

@INBOOK{Pearce1987,
  chapter = {Conceptual migrations: Understanding travelers tales for cross-cultural
	understanding},
  pages = {20-40},
  title = {Cross-cultural Adaptation},
  publisher = {Sage},
  year = {1987},
  editor = {Kim, Y. Y. and Gudykunst, W. B.},
  author = {Pearce, W. B. and Kang, K},
  volume = {IX},
  address = {Beverly Hills, CA},
  owner = {tim},
  timestamp = {2013.03.30}
}

@ARTICLE{Pentland2007,
  author = {Pentland, A.},
  title = {Social Signal Processing [Exploratory DSP]},
  journal = {Signal Processing Magazine, IEEE},
  year = {2007},
  volume = {24},
  pages = {108 -111},
  number = {4},
  month = {july },
  abstract = {Face-to-face communication conveys social context as well as words.
	It is this social signaling that allows new information to be smoothly
	integrated into a shared, group-wide understanding. Social signaling
	includes signals of interest, determination, friendliness, boredom,
	and other "attitudes" toward a social situation. Psychologists speculate
	that social signaling may have evolved as a way to establish hierarchy
	and group cohesion because social signaling functions as a subconscious
	discussion about relationships, resources, risks, and rewards. In
	many situations the nonlinguistic signals that serve as the basis
	for this social discussion are just as important as conscious content
	for determining human behavior. In what follows we discuss challenges
	in exploratory processing of social signals and tools that allow
	us to predict human behavior and sometimes exceed even expert human
	capabilities. These tools potentially permit computer and communications
	systems to support social and organizational roles instead of viewing
	the individual as an isolated entity. Example applications include
	automatically patching people into socially important conversations,
	instigating conversations among people in order to build a more solid
	social network, and reinforcing family ties.},
  doi = {10.1109/MSP.2007.4286569},
  issn = {1053-5888},
  keywords = {boredom;determination;expert human capabilities;family ties;group-wide
	understanding;human behavior;interest;nonlin- guistic signals;social
	context;social interaction;social network;social signal processing;social
	signaling;socially important conversations;subconscious discussion;psychology;signal
	processing;speech processing;}
}

@ARTICLE{Perlman2009,
  author = {Perlman, Susan B. AND Morris, James P. AND Vander Wyk, Brent C. AND
	Green, Steven R. AND Doyle, Jaime L. AND Pelphrey, Kevin A.},
  title = {Individual Differences in Personality Predict How People Look at
	Faces},
  journal = {PLoS ONE},
  year = {2009},
  volume = {4},
  pages = {e5952},
  number = {6},
  month = {06},
  abstract = {<sec> <title>Background</title> <p>Determining the ways in which personality
	traits interact with contextual determinants to shape social behavior
	remains an important area of empirical investigation. The specific
	personality trait of neuroticism has been related to characteristic
	negative emotionality and associated with heightened attention to
	negative, emotionally arousing environmental signals. However, the
	mechanisms by which this personality trait may shape social behavior
	remain largely unspecified.</p> </sec><sec> <title>Methodology/Principal
	Findings</title> <p>We employed eye tracking to investigate the relationship
	between characteristics of visual scanpaths in response to emotional
	facial expressions and individual differences in personality. We
	discovered that the amount of time spent looking at the eyes of fearful
	faces was positively related to neuroticism.</p> </sec><sec> <title>Conclusions/Significance</title>
	<p>This finding is discussed in relation to previous behavioral research
	relating personality to selective attention for trait-congruent emotional
	information, neuroimaging studies relating differences in personality
	to amygdala reactivity to socially relevant stimuli, and genetic
	studies suggesting linkages between the serotonin transporter gene
	and neuroticism. We conclude that personality may be related to interpersonal
	interaction by shaping aspects of social cognition as basic as eye
	contact. In this way, eye gaze represents a possible behavioral link
	in a complex relationship between genes, brain function, and personality.</p>
	</sec>},
  doi = {10.1371/journal.pone.0005952},
  keywords = {emotion perception personal-differences psychological personality
	gaze},
  publisher = {Public Library of Science},
  url = {http://dx.doi.org/10.1371%2Fjournal.pone.0005952}
}

@INBOOK{Perner1999,
  chapter = {Theory of Mind},
  pages = {205-230},
  title = {Developmental psychology: Achievements \& prospects},
  publisher = {Psychology Press},
  year = {1999},
  editor = {Bennett, M.},
  author = {Perner, J.},
  address = {Hove, East Sussex},
  owner = {tim},
  timestamp = {2012.06.16}
}

@INPROCEEDINGS{Petridis2010,
  author = {Petridis, S. and Asghar, A. and Pantic, M.},
  title = {Classifying laughter and speech using audio-visual feature prediction},
  booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech,
	and Signal Processing},
  year = {2010},
  address = {Dallas, USA},
  month = {March},
  abstract = {In this study, a system that discriminates laughter from speech by
	modelling the relationship between audio and visual features is presented.
	The underlying assumption is that this relationship is different
	between speech and laughter. Neural networks are trained which learn
	the audio-to-visual and visual-to-audio features mapping for both
	classes. Classification of a new frame is performed via prediction.
	All the networks produce a prediction of the expected audio / visual
	features and the network with the best prediction, i.e., the model
	which best describes the audiovisual feature relationship, provides
	its label to the input frame. When trained on a simple dataset and
	tested on a hard dataset, the proposed approach outperforms audiovisual
	feature-level fusion, resulting in a 10.9% and 6.4% absolute increase
	in the F1 rate for laughter and classification rate, respectively.
	This indicates that classification based on prediction can produce
	a good model even when the available dataset is not challenging enough.},
  file = {:Petridis2010.pdf:PDF},
  keywords = {classification-nn labels-laughter audio-features multimodal corpus-ami
	situation-meeting feature-generation corpus-Belfast-SAL transfer},
  owner = {tim},
  timestamp = {2010.08.25}
}

@INPROCEEDINGS{Petridis2009,
  author = {Petridis, S. and Gunes, H. and Kaltwang, S. and Pantic, M.},
  title = {Static vs. Dynamic Modelling of Human Nonverbal Behaviour from Multiple
	Cues and Modalities},
  booktitle = {Proceedings of the ACM Interational Conference on Multimodal Interfaces},
  year = {2009},
  address = {Cambridge, USA},
  month = {November},
  abstract = {Human nonverbal behavior recognition from multiple cues and modalities
	has attracted a lot of interest in recent years. Despite the interest,
	many research questions, including the type of feature representation,
	choice of static vs. dynamic classification schemes, the number and
	type of cues or modalities to use, and the optimal way of fusing
	these, remain open research questions. This paper compares frame-based
	vs. window-based feature representation and employs static vs. dynamic
	classification schemes for two distinct problems in the field of
	automatic human nonverbal behavior analysis: multicue discrimination
	between posed and spontaneous smiles from facial expressions, head
	and shoulder movements, and audio-visual discrimination between laughter
	and speech. Single cue and single modality results are compared to
	multicue and multimodal results by employing Neural Networks, Hidden
	Markov Models (HMMs), and 2-and 3-chain coupled HMMs. Subject independent
	experimental evaluation shows that: 1) both for static and dynamic
	classification, fusing data coming from multiple cues and modalities
	proves useful to the overall task of recognition, 2) the type of
	feature representation appears to have a direct impact on the classification
	performance, and 3) static classification is comparable to dynamic
	classification both for multicue discrimination between posed and
	spontaneous smiles, and audio-visual discrimination between laughter
	and speech.},
  file = {Petridis2009.pdf:Petridis2009.pdf:PDF},
  keywords = {labels-laughter nvc feature-generation classification-neural classification-hmms
	feature-fusion features-important audio-features posed-data natural-data
	multimodal temporal},
  owner = {ts00051},
  timestamp = {2010.01.15}
}

@ARTICLE{Petridis2011,
  author = {S. Petridis and M. Pantic},
  title = {Audiovisual Discrimination Between Speech and Laughter: Why and When
	Visual Information Might Help},
  journal = {IEEE Transactions on Multimedia},
  year = {2011},
  volume = {13},
  pages = {216--234},
  number = {2},
  month = {April}
}

@INPROCEEDINGS{Petridis2008,
  author = {Petridis, Stavros and Pantic, Maja},
  title = {Audiovisual Laughter Detection Based on Temporal Features},
  booktitle = {Proceedings of the 10th International Conference on Multimodal Interfaces},
  year = {2008},
  pages = {37--44},
  address = {New York, NY, USA},
  publisher = {ACM},
  abstract = {Previous research on automatic laughter detection has mainly been
	focused on audio-based detection. In this study we present an audio-visual
	approach to distinguishing laughter from speech based on temporal
	features and we show that integrating the information from audio
	and video channels leads to improved performance over single-modal
	approaches. Static features are extracted on an audio/video frame
	basis and then combined with temporal features extracted over a temporal
	window, describing the evolution of static features over time. The
	use of several different temporal features has been investigated
	and it has been shown that the addition of temporal information results
	in an improved performance over utilizing static information only.
	It is common to use a fixed set of temporal features which implies
	that all static features will exhibit the same behaviour over a temporal
	window. However, this does not always hold and we show that when
	AdaBoost is used as a feature selector, different temporal features
	for each static feature are selected, i.e., the temporal evolution
	of each static feature is described by different statistical measures.
	When tested on 96 audiovisual sequences, depicting spontaneously
	displayed (as opposed to posed) laughter and speech episodes, in
	a person independent way the proposed audiovisual approach achieves
	an F1 rate of over 89%.},
  doi = {http://doi.acm.org/10.1145/1452392.1452402},
  file = {Petridis2008.pdf:Petridis2008.pdf:PDF},
  isbn = {978-1-60558-198-9},
  keywords = {feature-generation clip-statistics temporal feature-generation labels-laughter
	feature-fusion audio-features multimodal tracker-features feature-selection},
  location = {Chania, Crete, Greece},
  owner = {ts00051},
  timestamp = {2009.10.26}
}

@INPROCEEDINGS{Peyras2008,
  author = {Peyras, Julien and Adrien Bartoli and Samir Khoualed},
  title = {Pools of {A}{A}{M}s: Towards Automatically Fitting any Face Image},
  booktitle = {Proceedings of the Nineth British Machine Vision Conference},
  year = {2008},
  address = {Leeds, UK},
  month = {September},
  abstract = {Fitting a single generic AAM on an unseen face (that is not in the
	training set) under any pose and expression is very difficult. The
	variability of the data is so high that the fitting process usually
	gets stuck into one of the numerous local minima. We show that a
	solution to this problem consists to separate the variability sources.
	We build a pool of specialized AAMs. Each AAM is trained over multiple
	identities, all shown under the same pose and expression. We then
	retain the AAM that shows the smallest residual error when fitted
	to the input image. The fitting obtained in this manner is very accurate
	on unseen faces. The ultimate goal is to automatically train a person-specific
	AAM. In addition, the pool of specialized AAMs allows us to recognize
	the face pose and expression at each frame of the video with good
	performances. The proposed method has potential applications in Human
	Computer Interaction and driving surveillance, to name just but a
	few.},
  file = {:Peyras2008.pdf:PDF},
  keywords = {face tracking head-model aam},
  owner = {ts00051},
  timestamp = {2009.10.26}
}

@INPROCEEDINGS{Pfister2011,
  author = {Tomas Pfister and Xiaobai Li and Guoying Zhao and Matti Pietik\"ainen},
  title = {Recognising Spontaneous Facial Micro-expressions},
  booktitle = {Proceedings of the International Conference on Computer Vision},
  year = {2011},
  abstract = {Facial micro-expressions are rapid involuntary facial expressions
	which reveal suppressed affect. To the best knowledge of the authors,
	there is no previous work that successfully recognises spontaneous
	facial micro-expressions. In this paper we show how a temporal interpolation
	model together with the first comprehensive spontaneous microexpression
	corpus enable us to accurately recognise these very short expressions.
	We designed an induced emotion
	
	suppression experiment to collect the new corpus using a high-speed
	camera. The system is the first to recognise spontaneous facial micro-expressions
	and achieves very promising results that compare favourably with
	the human micro-expression detection accuracy.},
  file = {:Pfister2011.pdf:PDF},
  keywords = {situation-video-watching classification-mkl classification-tree asm-features
	tracker-features alignment corpus-YorkDDT label-emotion feature-generation},
  poster = {http://tomas.pfister.fi/files/pfister11microexpressions-poster.pdf},
  video = {http://youtu.be/E-b49e0wLgg},
  webpdf = {http://tomas.pfister.fi/files/pfister11microexpressions.pdf}
}

@INPROCEEDINGS{Pfister2011b,
  author = {Tomas Pfister and Xiaobai Li and Guoying Zhao and Matti Pietik\"ainen},
  title = {Differentiating Spontaneous From Posed Facial Expressions Within
	a Generic Facial Expression Recognition Framework},
  booktitle = {Proceedings of the Workshop on Socially Intelligent Surveillance
	and Monitoring},
  year = {2011},
  abstract = {In this paper we propose the first method known to the authors that
	successfully differentiates spontaneous from posed facial expressions
	using a realistic training corpus. We propose a new spatiotemporal
	local texture descriptor (CLBP-TOP) that outperforms other descriptors.
	We demonstrate that our temporal interpolation and visual/near-infrared
	fusion methods improve the differentiation performance. Finally,
	we propose a new generic facial expression recognition framework
	that subdivides the facial expression recognition problem into a
	cascade of smaller tasks that are simpler to tackle. The system is
	the first to differentiate spontaneous from posed facial expressions
	with a realistic corpus and achieves promising results.},
  file = {:Pfister2011b.pdf:PDF},
  keywords = {feature-generation texture-feature classification-mkl classification-svm
	natural-important basic-emotions classification-genetic decision-fusion},
  webpdf = {http://tomas.pfister.fi/files/pfister2011differentiating.pdf}
}

@INPROCEEDINGS{Poggi2011,
  author = {Poggi, I. and D'Errico, F},
  title = {Social Signals: A Psychological Perspective},
  booktitle = {In Proceedings of Computer Analysis of Human Behavior},
  year = {2011},
  pages = {185-225},
  owner = {tim},
  timestamp = {2012.10.04}
}

@ARTICLE{Poggi2011b,
  author = {Poggi, Isabella and D’Errico, Francesca and Vincze, Laura},
  title = {Agreement and its Multimodal Communication in Debates: A Qualitative
	Analysis},
  journal = {Cognitive Computation},
  year = {2011},
  volume = {3},
  pages = {466-479},
  note = {10.1007/s12559-010-9068-x},
  abstract = {The paper defines the notion of agreement from a cognitive point of
	view and analyses types of agreement signals in TV debates. Agreement
	is defined as a relation of identity, similarity or congruence between
	the opinions of two or more persons, by contrasting it with confirmation
	and admission, and the connected notions of proposal, assessment
	and opinion are overviewed. Research is then presented on the multimodal
	signals of agreement in debates from the Canal 9 and the AMI corpora;
	different ways to express agreement are singled out in extensive
	discourse, single words and body signals, and analysed through an
	annotation scheme of multimodal data. Different types of agreement
	are illustrated, including true, indirect and apparent agreement.},
  affiliation = {Roma Tre University, Via del Castro Pretorio 20, 00185 Rome, Italy},
  issn = {1866-9956},
  issue = {3},
  keyword = {Biomedical and Life Sciences},
  publisher = {Springer New York},
  url = {http://dx.doi.org/10.1007/s12559-010-9068-x}
}

@INBOOK{Poh2010,
  chapter = {Multimodal Information Fusion},
  pages = {153-169},
  title = {Multimodal Signal Processing: Theory and Applications for Human-Computer
	Interaction},
  publisher = {Academic Press},
  year = {2010},
  editor = {Thiran, Jean-Philippe and Ferran Marqués and Hervé Bourlard},
  author = {Poh, Norman and Kittler, Josef},
  address = {London},
  file = {Poh2010.pdf:Poh2010.pdf:PDF},
  owner = {tim},
  timestamp = {2012.10.24}
}

@INPROCEEDINGS{Potamianos2001,
  author = {G. Potamianos and J. Luettin and C. Neti},
  title = {Hierarchical discriminant features for audio-visual LVCSR},
  booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech
	and Signal Processing},
  year = {2001},
  volume = {1},
  pages = {165-168},
  address = {Salt Lake City, Utah, USA},
  abstract = {We propose the use of a hierarchical, two-stage discriminant transformation
	for obtaining audio-visual features that improve automatic speech
	recognition. Linear discriminant analysis (LDA), followed by a maximum
	likelihood linear transform (MLLT) is first applied on MFCC based
	audio-only features, as well as on visualonly features, obtained
	by a discrete cosine transform of the video region of interest. Subsequently,
	a second stage of LDA and MLLT is applied on the concatenation of
	the resulting single modality features. The obtained audio-visual
	features are used to train a traditional HMM based speech recognizer.
	Experiments on the IBM ViaVoice TM audio-visual database demonstrate
	that the proposed feature fusion method improves speaker-independent,
	large vocabulary, continuous speech recognition for both clean and
	noisy audio conditions considered. A 24% relative word error rate
	reduction over an audio-only system is achieved in the latter case.},
  keywords = {multimodal-features texture-feature feature-generation speech recognition
	classification-hmms},
  review = {Audiovisual detection of speech using LDA and MLLT on audio and video
	DCT separately, concatenating the result and then applying LDT and
	MLLT. An alternative hierarchical combination of the data performaned
	dimensional reduction on the visual channel because it contained
	less information than the audio for this task. The Viavoice dataset
	was used with 10400 word vocabulary. This method had better performance
	than audio only recognition.}
}

@INPROCEEDINGS{Potamianos2003,
  author = {Potamianos, G. and Neti, C.},
  title = {Audio-visual speech recognition in challenging environments},
  booktitle = {Proceedings of the European Conference on Speech Communication and
	Technology},
  year = {2003},
  pages = {1293-1296},
  address = {Geneva, Switzerland},
  abstract = {Visual speech information is known to improve accuracy and noise robustness
	of automatic speech recognizers. However, to-date, all audio-visual
	ASR work has concentrated on “visually clean” data with limited variation
	in the speaker’s frontal pose, lighting, and background. In this
	paper, we investigate audio-visual ASR in two practical environments
	that present significant challenges to robust visual processing:
	(a) Typical offices, where data are recorded by means of a portable
	PC equipped with an inexpensive web camera, and (b) automobiles,
	with data collected at three approximate speeds. The performance
	of all components of a state-of-the-art audio-visual ASR system is
	reported on these two sets and benchmarked against “visually clean”
	data recorded in a studio-like environment. Not surprisingly, both
	audio- and visual-only ASR degrade, more than doubling their respective
	word error rates. Nevertheless, visual speech remains beneficial
	to ASR.},
  keywords = {speechreading head-pose situation-office situation-driving},
  review = {Combining audio and visual speech recognition tested in challenging
	environments. The visual processing channel is: find ROI of lower
	face, find 100 highest energy DCT components, use LDA to find 30
	components, then Maximum likelihood linear transformation (MLLT).
	This is combined with the audio information at the feature level
	or decision level.}
}

@MISC{Proyas2004,
  author = {Proyas (Director), Alex},
  title = {I, Robot},
  howpublished = {[Motion picture]},
  year = {2004},
  note = {Twentieth Century-Fox Film Corporation},
  keywords = {nvc-applications nvc-important},
  owner = {ts00051},
  review = {Sonny: What does this action signify?
	
	[winks]
	
	Sonny: As you walked in the room, when you looked at the other human.
	What does it mean?
	
	[winks]
	
	Detective Del Spooner: It's a sign of trust. It's a human thing. You
	wouldn't understand.},
  timestamp = {2009.11.30}
}

@INPROCEEDINGS{Raddick2010,
  author = {{Raddick}, J. and {Bracey}, G.~L. and {Gay}, P.~L.},
  title = {Motivations of Citizen Scientists Participating in Galaxy Zoo: A
	More Detailed Look},
  booktitle = {American Astronomical Society Meeting Abstracts \#215},
  year = {2010},
  volume = {42},
  series = {Bulletin of the American Astronomical Society},
  pages = {509},
  month = jan,
  adsnote = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl = {http://adsabs.harvard.edu/abs/2010AAS...21546704R},
  keywords = {crowdsourcing tools annotation}
}

@INBOOK{Ramseyer2008,
  chapter = {Synchrony in dyadic psychotherapy sessions},
  pages = {329-347},
  title = {Simultaneity: Temporal structures and observer perspectives.},
  publisher = {World Scientific},
  year = {2008},
  editor = {S. Vrobel, O. E. Rössler, \& T. Marks-Tarlow},
  author = {Ramseyer, F. and Tschacher, W.},
  address = {Singapore},
  owner = {tim},
  timestamp = {2012.10.31}
}

@ARTICLE{Rasmussen1989,
  author = {Rasmussen, Jeffrey L.},
  title = {Analysis of Likert-scale data: A reinterpretation of Gregoire and
	Driver.},
  journal = {Psychological Bulletin},
  year = {1989},
  volume = {105},
  pages = {167-170},
  number = {1},
  month = {Jan},
  doi = {10.1037/0033-2909.105.1.167},
  owner = {tim},
  timestamp = {2012.11.01}
}

@INPROCEEDINGS{Ray2001,
  author = {Ray, Soumya and Page, David},
  title = {Multiple instance regression},
  booktitle = {Proceedings of the 18th International Conference on Machine Learning},
  year = {2001},
  pages = {425--432},
  publisher = {Morgan Kaufmann},
  file = {Ray2001.pdf:Ray2001.pdf:PDF},
  keywords = {supervised learning bag instance regression regression-method}
}

@PHDTHESIS{Reidsma2008Thesis,
  author = {Reidsma, Dennis},
  title = {Annotations and subjective machines of annotators, embodied agents,
	users, and other humans},
  school = {University of Twente},
  year = {2008},
  address = {Enschede},
  month = {October},
  abstract = {The usual practice in assessing whether a multimodal annotated corpus
	is fit for purpose is to calculate the level of inter-annotator agreement,
	and when it exceeds a certain fixed threshold the data is considered
	to be of tolerable quality. There are two problems with this approach.
	Firstly, it depends on the assumption that any disagreement in the
	data is not systematic. This assumption may not always be warranted.
	Secondly, the approach is not well suited for annotations that are
	subjective to a certain degree. In that case annotator disagreement
	is (partly) an inherent property of the annotation, expressing something
	about the level of intersubjectivity between annotators in how they
	interpret certain communicative behavior versus the amount of idiosyncrasy
	in their judgements with respect to this behavior. This thesis addresses
	both problems. In the theoretical part, it is shown that when disagreement
	is systematic, obtaining a certain level of inter-annotator agreement
	may not be a guarantee for the data being fit for purpose. Simulations
	are used to investigate the effect of systematic disagreement on
	the relation between the level of inter-annotator agreement and the
	validity of machine-learning results obtained on the data. In the
	practical part, two new methods are explored for working with data
	that has been annotated with a low level of inter-annotator agreement.
	One method is aimed at finding a subset of the annotations that has
	been annotated more reliably, in a way that makes it possible to
	determine for new, unseen data whether it should belong to this subset
	? and therefore, whether a classifier trained on this more reliable
	subset is qualified to make a judgement for the new data. The other
	method is designed to use machine learning for explicitly modeling
	the overlap and disjunctions in subjective judgements of different
	annotators. Both methods put together should in theory make it possible
	to build classifiers that, when deployed in a practical application,
	yield decisions that make sense for the human end user of the application,
	who indeed also may have his or her own way of interpreting the communicative
	behavior that is subjected to the classifier.},
  file = {Reidsma2008Thesis.pdf:Reidsma2008Thesis.pdf:PDF},
  keywords = {nvc-annotation perception annotation corpus-ami},
  publisher = {University of Twente},
  series = {CTIT Dissertation Series 08-121},
  url = {http://doc.utwente.nl/59870/}
}

@INPROCEEDINGS{Reidsma2008b,
  author = {Reidsma, Dennis and Rieks op den Akker},
  title = {Exploiting ‘Subjective’ Annotations},
  booktitle = {Proceedings of the Workshop on Human Judgements in Computational
	Linguistic},
  year = {2008},
  address = {Manchester, UK},
  month = {August},
  file = {:Reidsma2008b.pdf:PDF},
  keywords = {nvc-annotation multiannotator human-performance corpus-ami},
  owner = {ts00051},
  review = {Trains a system with one set of annotators and applies it to another
	set},
  timestamp = {2012.01.13}
}

@INPROCEEDINGS{Reidsma2008,
  author = {Reidsma, Dennis and Heylen, Dirk and op den Akker, H. J. A.},
  title = {On the Contextual Analysis of Agreement Scores},
  booktitle = {Proceedings of the Workshop on Multimodal Corpora (LREC)},
  year = {2008},
  editor = {Martin, Jean-Claude and Paggio, Patrizia and Kipp, Michael and Heylen,
	Dirk},
  pages = {52--55},
  month = {May},
  publisher = {ELRA},
  abstract = {Annotators of multimodal corpora rely on a combination of audio and
	video features to assign labels to the events observed. The reliability
	of annotations may be influenced by the presences or absence of certain
	key features. For practical applications it can be useful to know
	what circumstances determined fluctuations in the interannotator
	agreement. In this paper we consider the case of annotations of addressing
	on the AMI corpus.},
  file = {Reidsma2008.pdf:Reidsma2008.pdf:PDF},
  keywords = {cat-1: Infrastructure, data collection (incl. scenarios) and data
	management (annotation and standardization), cat-4: Multimodal structure
	and content analysis, rep-2 human-performance nvc-annotation perception
	corpus-ami},
  location = {Marrakech, Morrocco},
  owner = {ts00051},
  review = {AMIDA publication number 99},
  timestamp = {2009.10.26}
}

@INPROCEEDINGS{Reidsma2010,
  author = {Dennis {Reidsma} and Anton {Nijholt} and Wolfgang {Tschacher} and
	Fabian {Ramseyer}},
  title = {Measuring Multimodal Synchrony for Human-Computer Interaction},
  booktitle = {Proceedings of the International Conference on CYBERWORLDS 2010},
  year = {2010},
  editor = {A. {Sourin}},
  pages = {67--71},
  address = {Los Alamitos},
  month = {October},
  publisher = {IEEE Computer Society Press},
  note = {Synchrony, nonverbal communication, measurement, virtual humans,
	HCI},
  abstract = {Nonverbal synchrony is an important and natural element in human-human
	interaction. It can also play various roles in human-computer interaction.
	In particular this is the case in the interaction between humans
	and the virtual humans that inhabit our cyberworlds. Virtual humans
	need to adapt their behavior to the behavior of their human interaction
	partners in order to maintain a natural and continuous interaction
	synchronization. This paper surveys approaches to modeling synchronization
	and applications where this modeling is important. Apart from presenting
	this framework, we also present a quantitative method for measuring
	the level of nonverbal synchrony in an interaction and observations
	on future research that allows embedding such methods in models of
	interaction behavior of virtual humans.},
  url = {http://doc.utwente.nl/74052/}
}

@ARTICLE{Revelle2009,
  author = {Revelle, William and Zinbarg, Richard},
  title = {Coefficients Alpha, Beta, Omega, and the glb: Comments on Sijtsma},
  journal = {Psychometrika},
  year = {2009},
  volume = {74},
  pages = {145-154},
  note = {10.1007/s11336-008-9102-z},
  abstract = {There are three fundamental problems in Sijtsma (Psychometrika, 2008
	): (1) contrary to the name, the glb is not the greatest lower bound
	of reliability but rather is systematically less than ω t (McDonald,
	Test theory: A unified treatment, Erlbaum, Hillsdale, 1999 ), (2)
	we agree with Sijtsma that when considering how well a test measures
	one concept, α is not appropriate, but recommend ω t rather than
	the glb, and (3) the end user needs procedures that are readily available
	in open source software.},
  affiliation = {Northwestern University Department of Psychology Evanston IL USA},
  issn = {0033-3123},
  issue = {1},
  keyword = {Behavioral Science},
  publisher = {Springer New York},
  url = {http://dx.doi.org/10.1007/s11336-008-9102-z}
}

@ARTICLE{Richardson2007,
  author = {Richardson, Daniel C. and Dale, Rick and Kirkham, Natasha Z.},
  title = {The Art of Conversation Is Coordination: Common Ground and the Coupling
	of Eye Movements During Dialogue},
  journal = {Psychological Science},
  year = {2007},
  volume = {18},
  pages = {407-413(7)},
  month = {May},
  abstract = {When two people discuss something they can see in front of them, what
	is the relationship between their eye movements? We recorded the
	gaze of pairs of subjects engaged in live, spontaneous dialogue.
	Cross-recurrence analysis revealed a coupling between the eye movements
	of the two conversants. In the first study, we found their eye movements
	were coupled across several seconds. In the second, we found that
	this coupling increased if they both heard the same background information
	prior to their conversation. These results provide a direct quantification
	of joint attention during unscripted conversation and show that it
	is influenced by knowledge in the common ground.},
  doi = {doi:10.1111/j.1467-9280.2007.01914.x},
  file = {:Richardson2007.pdf:PDF},
  keywords = {gaze coupled perception},
  url = {http://www.ingentaconnect.com/content/bpl/psci/2007/00000018/00000005/art00009}
}

@INPROCEEDINGS{Riddick2009,
  author = {Riddick, M. Jordan and Georgia Bracey and Pamela Gay and Chris J.
	Lintott and Kevin Scawinski and Alex Szalay and Jan Vanderberg},
  title = {Galaxy Zoo: Exploring the Motivations of Citizen Science Volunteers},
  booktitle = {Astronomy Education Review},
  year = {2009},
  volume = {9},
  number = {1},
  keywords = {annotation crowdsourcing},
  owner = {ts00051},
  timestamp = {2012.01.13}
}

@INPROCEEDINGS{Rose2006,
  author = {Rose, N.},
  title = {A Comparison of Single and Multi-Class Classifiers for Facial Expression
	Classification},
  booktitle = {Computational Intelligence for Modelling, Control and Automation,
	2006 and International Conference on Intelligent Agents, Web Technologies
	and Internet Commerce, International Conference on},
  year = {2006},
  pages = {175},
  month = {Nov. 28-Dec. 1},
  abstract = {This paper compares various Kernel, neural network and statistical
	approaches for the task of novel facial classification. In addition
	to multi-class classifiers, it explores the use of the single-class
	methods known as autoassociators, including the recently proposed
	Kernel Autoassociator which reconstructs feature vectors from Kernel
	feature space to input space. Comparisons are made using feature
	vectors composed of image pixel values, as well as image points convolved
	with Gabor filters. Results show an advantage to using multi-class
	methods, with linear disriminant analysis and Kernel based approaches
	providing the best results.},
  doi = {10.1109/CIMCA.2006.1},
  keywords = {Gabor filters;facial expression classification;kernel autoassociator;kernel
	feature space;multi-class classifiers;neural network;Gabor filters;face
	recognition;neural nets;pattern classification;}
}

@ARTICLE{Rosenblum1996,
  author = {Rosenblum, M. and Yacoob, Y. and Davis, L.S.},
  title = {Human expression recognition from motion using a radial basis function
	network architecture},
  journal = {IEEE Transactions on Neural Networks},
  year = {1996},
  volume = {7},
  pages = {1121-1138},
  number = {5},
  month = {September},
  abstract = {In this paper a radial basis function network architecture is developed
	that learns the correlation of facial feature motion patterns and
	human expressions. We describe a hierarchical approach which at the
	highest level identifies expressions, at the mid level determines
	motion of facial features, and at the low level recovers motion directions.
	Individual expression networks were trained to recognize the “smile”
	and “surprise” expressions. Each expression network was trained by
	viewing a set of sequences of one expression for many subjects. The
	trained neural network was then tested for retention, extrapolation,
	and rejection ability. Success rates were 88% for retention, 88%
	for extrapolation, and 83% for rejection},
  doi = {10.1109/72.536309},
  file = {:Rosenblum1996.pdf:PDF},
  issn = {1045-9227},
  keywords = {correlation;extrapolation;facial feature motion patterns;hierarchical
	approach;human expression recognition;radial basis function network
	architecture;rejection ability;retention;smile;surprise;correlation
	methods;face recognition;feedforward neural nets;motion estimation;
	basic-emotions; feature-generation; optical-flow-features; posed-data;
	classification-neural}
}

@INBOOK{Rozelle2006,
  chapter = {Non-verbal behavior as communication},
  pages = {67-102},
  title = {The Handbook of Communication Skills},
  publisher = {Routledge},
  year = {2006},
  editor = {Owen Hargie},
  author = {Rozelle, Richard M. and Daniel Druckman and James C. Baxter},
  month = {1st July},
  owner = {tim},
  timestamp = {2013.03.30}
}

@INPROCEEDINGS{Rudovic2011,
  author = {O. Rudovic and M. Pantic},
  title = {Shape-constrained Gaussian Process Regression for Facial-point-based
	Head-pose Normalization},
  booktitle = {Proceedings of IEEE Int’l Conf. on Computer Vision (ICCV 2011)},
  year = {2011},
  pages = {1495--1502},
  month = {November}
}

@INPROCEEDINGS{Rudovic2010,
  author = {O. Rudovic and I. Patras and M. Pantic},
  title = {Regression-based multi-view facial expression recognition},
  booktitle = {Proceedings of Int'l Conf. Pattern Recognition (ICPR'10)},
  year = {2010},
  pages = {4121--4124},
  address = {Istanbul, Turkey},
  month = {August}
}

@INPROCEEDINGS{Rudovic2012,
  author = {O. Rudovic and V. Pavlovic and M. Pantic},
  title = {Kernel Conditional Ordinal Random Fields for Temporal Segmentation
	of Facial Action Units},
  booktitle = {Proceedings of the 12th European Conference on Computer Vision (ECCV-W'12).
	Florence, Italy},
  year = {2012},
  month = {October}
}

@CONFERENCE{Rudovic2012cvpr,
  author = {Rudovic, Ognjen and Pavlovic, Vladimir and Pantic, Maja},
  title = {{M}ulti-output {L}aplacian {D}ynamic {O}rdinal {R}egression for {F}acial
	{E}xpression {R}ecognition and {I}ntensity {E}stimation},
  booktitle = {Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)},
  year = {2012}
}

@ARTICLE{Saeys2007,
  author = {Saeys, Yvan and Inza, I\~{n}aki and Larra\~{n}aga, Pedro},
  title = {A review of feature selection techniques in bioinformatics},
  journal = {Bioinformatics},
  year = {2007},
  volume = {23},
  pages = {2507--2517},
  number = {19},
  month = sep,
  acmid = {1349169},
  address = {Oxford, UK},
  doi = {10.1093/bioinformatics/btm344},
  file = {:Saeys2007.pdf:PDF},
  issn = {1367-4803},
  issue_date = {September 2007},
  numpages = {11},
  publisher = {Oxford University Press},
  url = {http://dx.doi.org/10.1093/bioinformatics/btm344}
}

@INPROCEEDINGS{Saitoh2005,
  author = {Saitoh, T. and Konishi, R.},
  title = {Lip Reading Based on Sampled Active Contour Model},
  booktitle = {Proceedings of the International Conference on Image Analysis and
	Recognition},
  year = {2005},
  pages = {507-515},
  abstract = {This paper describes a model-based method for detecting lip region
	from image sequences. Our approach is by Sampled Active Contour Model
	(S-ACM). The original S-ACM has the problem which can’t expand. To
	overcome this problem, we propose the elastic S-ACM. Moreover, based
	on the extracted lip contour, the effective delta radius features
	are fed to the word HMM. We recorded ten words that uses for the
	wheelchair control, and obtained a recognition rate of 89% with twelve
	features.},
  bibsource = {http://www.visionbib.com/bibliography/people915.html#TT74192},
  keywords = {speechreading asm-features classification-hmms lipreading}
}

@ARTICLE{Sammon1969,
  author = {Sammon, J.W.},
  title = {A nonlinear mapping for data structure analysis},
  journal = {IEEE Transactions on Computers},
  year = {1969},
  volume = {18},
  pages = {401-409},
  keywords = {dimensional-reduction unsupervised},
  owner = {ts00051},
  timestamp = {2012.01.17}
}

@INPROCEEDINGS{Sandbach2011,
  author = {G. Sandbach and S. Zafeiriou and M. Pantic and D. Rueckert},
  title = {A Dynamic Approach to the Recognition of 3D Facial Expressions and
	Their Temporal Models},
  booktitle = {Proceedings of IEEE International Conference on Automatic Face and
	Gesture Recognition (FG'11), Special Session: 3D Facial Behavior
	Analysis and Understanding},
  year = {2011},
  pages = {406--413},
  address = {Santa Barbara, CA, USA},
  month = {March}
}

@ARTICLE{Sauter2009,
  author = {Sauter, Disa A. and Eisner, Frank and Ekman, Paul and Scott, Sophie
	K.},
  title = {Cross-cultural recognition of basic emotions through nonverbal emotional
	vocalizations},
  journal = {Proceedings of the National Academy of Sciences},
  year = {2009},
  abstract = {Emotional signals are crucial for sharing important information, with
	conspecifics, for example, to warn humans of danger. Humans use a
	range of different cues to communicate to others how they feel, including
	facial, vocal, and gestural signals. We examined the recognition
	of nonverbal emotional vocalizations, such as screams and laughs,
	across two dramatically different cultural groups. Western participants
	were compared to individuals from remote, culturally isolated Namibian
	villages. Vocalizations communicating the so-called “basic emotions”
	(anger, disgust, fear, joy, sadness, and surprise) were bidirectionally
	recognized. In contrast, a set of additional emotions was only recognized
	within, but not across, cultural boundaries. Our findings indicate
	that a number of primarily negative emotions have vocalizations that
	can be recognized across cultures, while most positive emotions are
	communicated with culture-specific signals.},
  doi = {10.1073/pnas.0908239106},
  file = {:Sauter2009.pdf:PDF},
  keywords = {audio-features cross-cultural perception},
  owner = {ts00051},
  timestamp = {2010.01.28}
}

@ARTICLE{Savitzky1964,
  author = {Savitzky, Abraham and Golay, M. J. E.},
  title = {Smoothing and Differentiation of Data by Simplified Least Squares
	Procedures},
  journal = {Analytical Chemistry},
  year = {1964},
  volume = {36},
  pages = {1627-1639},
  number = {8},
  doi = {10.1021/ac60214a047},
  eprint = {http://pubs.acs.org/doi/pdf/10.1021/ac60214a047},
  keywords = {tools filtering feature-generation},
  url = {http://pubs.acs.org/doi/abs/10.1021/ac60214a047}
}

@ARTICLE{Savran2012,
  author = {Arman Savran and Bulent Sankur and M. Taha Bilge},
  title = {Regression-based intensity estimation of facial action units},
  journal = {Image and Vision Computing},
  year = {2012},
  volume = {30},
  pages = {774 - 784},
  number = {10},
  note = {<ce:title>3D Facial Behaviour Analysis and Understanding</ce:title>},
  abstract = {Facial Action Coding System (FACS) is the de facto standard in the
	analysis of facial expressions. FACS describes expressions in terms
	of the configuration and strength of atomic units called Action Units:
	AUs. FACS defines 44 AUs and each AU intensity is defined on a nonlinear
	scale of five grades. There has been significant progress in the
	literature on the detection of AUs. However, the companion problem
	of estimating the AU strengths has not been much investigated. In
	this work we propose a novel AU intensity estimation scheme applied
	to 2D luminance and/or 3D surface geometry images. Our scheme is
	based on regression of selected image features. These features are
	either non-specific, that is, those inherited from the AU detection
	algorithm, or are specific in that they are selected for the sole
	purpose of intensity estimation. For thoroughness, various types
	of local 3D shape indicators have been considered, such as mean curvature,
	Gaussian curvature, shape index and curvedness, as well as their
	fusion. The feature selection from the initial plethora of Gabor
	moments is instrumented via a regression that optimizes the AU intensity
	predictions. Our AU intensity estimator is person-independent and
	when tested on 25 AUs that appear singly or in various combinations,
	it performs significantly better than the state-of-the-art method
	which is based on the margins of SVMs designed for AU detection.
	When evaluated comparatively, one can see that the 2D and 3D modalities
	have relative merits per upper face and lower face AUs, respectively,
	and that there is an overall improvement if 2D and 3D intensity estimations
	are used in fusion.},
  doi = {10.1016/j.imavis.2011.11.008},
  issn = {0262-8856},
  keywords = {Action unit intensity estimation},
  url = {http://www.sciencedirect.com/science/article/pii/S0262885611001326}
}

@ARTICLE{Scholkopf2000,
  author = {Sch\"{o}lkopf, B. and Smola, A. and Williamson, R. and Bartlett,
	P. L.},
  title = {New Support Vector Algorithms},
  journal = {Neural Computation},
  year = {2000},
  volume = {12},
  pages = {1207-1245},
  file = {:Scholkopf2000.pdf:PDF},
  keywords = {nu svr regression supervised machine learning method},
  owner = {ts00051},
  timestamp = {2010.02.22}
}

@INPROCEEDINGS{Schapire1998,
  author = {Schapire, R. E. and Singer, Y.},
  title = {Improved boosting algorithms using confidence-rated predictions},
  booktitle = {Proceedings of the Eleventh Annual Conference on Computational Learning
	Theory},
  year = {1998},
  pages = {80–91},
  abstract = {We describe several improvements to Freund and Schapire‘s AdaBoost
	boosting algorithm, particularly in a setting in which hypotheses
	may assign confidences to each of their predictions. We give a simplified
	analysis of AdaBoost in this setting, and we show how this analysis
	can be used to find improved parameter settings as well as a refined
	criterion for training weak hypotheses. We give a specific method
	for assigning confidences to the predictions of decision trees, a
	method closely related to one used by Quinlan. This method also suggests
	a technique for growing decision trees which turns out to be identical
	to one proposed by Kearns and Mansour. We focus next on how to apply
	the new boosting algorithms to multiclass classification problems,
	particularly to the multi-label case in which each example may belong
	to more than one class. We give two boosting methods for this problem,
	plus a third method based on output coding. One of these leads to
	a new method for handling the single-label case which is simpler
	but as effective as techniques suggested by Freund and Schapire.
	Finally, we give some experimental results comparing a few of the
	algorithms discussed in this paper.},
  file = {Schapire1998.pdf:Schapire1998.pdf:PDF},
  keywords = {supervised boost machine learning multiclass classification classification-method},
  owner = {tim},
  timestamp = {2011.09.29}
}

@INBOOK{Scherer1999,
  chapter = {Appraisal theory},
  pages = {637-663},
  title = {Handbook of cognition and emotion},
  publisher = {Wiley},
  year = {1999},
  editor = {T. Dalgleish and M. Power},
  author = {Scherer, K. R.},
  address = {Chichester},
  abstract = {A central tenet of the appraisal theory is the claim that emotions
	are elicited and differentiated on the basis of a person's subjective
	evaluation of the personal significance of a situation, object, or
	event. During the last 20 yrs, a number of authors (e.g., B. Weiner,
	1986) have suggested that the nature of an emotional reaction can
	best be predicted on the basis of the individual's subjective appraisal
	of an antecedent event. Appraisal theory currently does not have
	any rivals. This is not surprising given that present day appraisal
	theories can be considered the culminating formalization of 2 centuries
	of philosophical notions that have always insisted on significance
	evaluation as the core process of emotional reaction. In this chapter
	appraisal theories are reviewed and distinguished based on 4 major
	strands of theoretical approaches to appraisal. These approaches
	can be characterized by the nature of the appraisal dimensions postulated
	by the respective theorists: criteria, attributions, themes, or meanings.
	Other topics discussed include the role of theoretical predictions,
	levels and the process of appraisal, links between appraisal and
	other components of emotion, universality vs cultural specificity
	of appraisal, and individual differences and pathology in appraisal.},
  keywords = {appraisal theory nvc taxonomy emotion cross-cultural},
  owner = {ts00051},
  timestamp = {2011.12.22}
}

@INPROCEEDINGS{Cowie2000,
  author = {Marc Schr\"{o}der and R. Cowie and E. Douglas-Cowie and S. Savvidou
	and E. McMahon and M. Sawey},
  title = {'{FEELTRACE}': An Instrument for Recording Perceived Emotion in Real
	Time},
  booktitle = {Proceedings of the ISCA Workshop on Speech and Emotion: A Conceptual
	Framework for Research},
  year = {2000},
  pages = {19-24},
  address = {Belfast},
  publisher = {Textflow},
  abstract = {FEELTRACE is an instrument developed to let observers track the emotional
	content of a stimulus as they perceive it over time, allowing the
	emotional dynamics of speech episodes to be examined. It is based
	on activation-evaluation space, a representation derived from psychology.
	The activation dimension measures how dynamic the emotional state
	is; the evaluation dimension is a global measure of the positive
	or negative feeling associated with the state. Research suggests
	that the space is naturally circular, i.e. states which are at the
	limit of emotional intensity define a circle, with alert neutrality
	at the centre. To turn those ideas into a recording tool, the space
	was represented by a circle on a computer screen, and observers described
	perceived emotional state by moving a pointer (in the form of a disc)
	to the appropriate point in the circle, using a mouse. Prototypes
	were tested, and in the light of results, refinements were made to
	ensure that outputs were as consistent and meaningful as possible.
	They include colour coding the pointer in a way that users readily
	associate with the relevant emotional state; presenting key emotion
	words as `landmarks' at the strategic points in the space; and developing
	an induction procedure to introduce observers to the system. An experiment
	assessed the reliability of the developed system. Stimuli were 16
	clips from TV programs, two showing relatively strong emotions in
	each quadrant of activationevaluation space, each paired with one
	of the same person in a relatively neural state. 24 raters took part.
	Differences between clips chosen to contrast were statistically robust.
	Results were plotted in activation-evaluation space as ellipses,
	each with its centre at the mean co-ordinates for the clip, and its
	width proportional t...},
  file = {:Cowie2000.pdf:PDF},
  keywords = {emotion annotation abstract-emotion-scales labels-temporal}
}

@INPROCEEDINGS{Schroder2011,
  author = {Schroder, M. and S. Pammi and H. Gunes and M. Pantic and M. Valstar
	and R. Cowie and G. McKeown and D. Heylen and M. ter Maat and F.
	Eyben and B. Schuller and M. Wollmer and E. Bevacqua and C. Pelachaud
	and E. de Sevin},
  title = {Come and Have an Emotional Workout with Sensitive Artificial Listeners!},
  booktitle = {Proceedings of IEEE International Conference on Automatic Face and
	Gesture Recognition},
  year = {2011},
  address = {Santa Barbara, California, USA},
  owner = {ts00051},
  timestamp = {2012.03.15}
}

@ARTICLE{Schuller2011,
  author = {Björn Schuller and Anton Batliner and Stefan Steidl and Dino Seppi},
  title = {Recognising realistic emotions and affect in speech: State of the
	art and lessons learnt from the first challenge},
  journal = {Speech Communication},
  year = {2011},
  volume = {53},
  pages = {1062 - 1087},
  number = {9–10},
  note = {<ce:title>Sensing Emotion and Affect - Facing Realism in Speech Processing</ce:title>},
  abstract = {More than a decade has passed since research on automatic recognition
	of emotion from speech has become a new field of research in line
	with its ‘big brothers’ speech and speaker recognition. This article
	attempts to provide a short overview on where we are today, how we
	got there and what this can reveal us on where to go next and how
	we could arrive there. In a first part, we address the basic phenomenon
	reflecting the last fifteen years, commenting on databases, modelling
	and annotation, the unit of analysis and prototypicality. We then
	shift to automatic processing including discussions on features,
	classification, robustness, evaluation, and implementation and system
	integration. From there we go to the first comparative challenge
	on emotion recognition from speech – the INTERSPEECH 2009 Emotion
	Challenge, organised by (part of) the authors, including the description
	of the Challenge’s database, Sub-Challenges, participants and their
	approaches, the winners, and the fusion of results to the actual
	learnt lessons before we finally address the ever-lasting problems
	and future promising attempts.},
  doi = {10.1016/j.specom.2011.01.011},
  issn = {0167-6393},
  keywords = {Emotion},
  url = {http://www.sciencedirect.com/science/article/pii/S0167639311000185}
}

@ARTICLE{Schutte1998,
  author = {Nicola S. Schutte and John M. Malouff and Lena E. Hall and Donald
	J. Haggerty and Joan T. Cooper and Charles J. Golden and Liane Dornheim},
  title = {Development and validation of a measure of emotional intelligence},
  journal = {Personality and Individual Differences},
  year = {1998},
  volume = {25},
  pages = {167 - 177},
  number = {2},
  abstract = {This series of studies describes the development of a measure of emotional
	intelligence based on the model of emotional intelligence developed
	by Salovey and Mayer [Salovey, P. &amp; Mayer, J. D. (1990). Emotional
	intelligence. Imagination, Cognition and Personality, 9, 185–211.].
	A pool of 62 items represented the different dimensions of the model.
	A factor analysis of the responses of 346 participants suggested
	the creation of a 33-item scale. Additional studies showed the 33-item
	measure to have good internal consistency and testretest reliability.
	Validation studies showed that scores on the 33-item measure 1. (a)
	correlated with eight of nine theoretically related constructs, including
	alexithymia, attention to feelings, clarity of feelings, mood repair,
	optimism and impulse control; 2. (b) predicted first-year college
	grades; 3. (c) were significantly higher for therapists than for
	therapy clients or for prisoners; 4. (d) were significantly higher
	for females than males, consistent with prior findings in studies
	of emotional skills; 5. (e) were not related to cognitive ability
	and 6. (f) were associated with the openness to experience trait
	of the big five personality dimensions.},
  doi = {10.1016/S0191-8869(98)00001-4},
  issn = {0191-8869},
  url = {http://www.sciencedirect.com/science/article/pii/S0191886998000014}
}

@INPROCEEDINGS{Seguier2002,
  author = {Seguier, R. and Cladel, N. and Foucher, C. and Mercier, D.},
  title = {Lipreading with Spiking Neurons: One Pass Learning},
  booktitle = {Proceedings of the 10th International Conference in Central Europe
	on Computer Graphics, Visualization and Computer Vision},
  year = {2002},
  pages = {397},
  address = {Plzen},
  bibsource = {http://www.visionbib.com/bibliography/people915.html#TT74207},
  file = {:Seguier2002.pdf:PDF},
  keywords = {classification-neural lipreading feature-generation facial texture-feature},
  review = {A system implementing spiking neuron (STAN) is user specifically trained
	for each word in the vocabulary. STANs operate on PCA of gray scale
	image. Expanding the vocabulary might be difficult without extensive
	training.}
}

@ARTICLE{Sekiyama2003,
  author = {Sekiyama, K. and Kanno, I. and Miura, S. and Sugita, Y.},
  title = {Auditory-visual speech perception examined by {fMRI} and {PET}},
  journal = {Neuroscience Research},
  year = {2003},
  volume = {47},
  pages = {277-287},
  keywords = {speech perception brain}
}

@ARTICLE{Senechal2012,
  author = {Senechal, T. and Rapp, V. and Salam, H. and Seguier, R. and Bailly,
	K. and Prevost, L.},
  title = {Facial Action Recognition Combining Heterogeneous Features via Multikernel
	Learning},
  journal = {Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions
	on},
  year = {2012},
  volume = {42},
  pages = {993 -1005},
  number = {4},
  month = {aug. },
  abstract = {This paper presents our response to the first international challenge
	on facial emotion recognition and analysis. We propose to combine
	different types of features to automatically detect action units
	(AUs) in facial images. We use one multikernel support vector machine
	(SVM) for each AU we want to detect. The first kernel matrix is computed
	using local Gabor binary pattern histograms and a histogram intersection
	kernel. The second kernel matrix is computed from active appearance
	model coefficients and a radial basis function kernel. During the
	training step, we combine these two types of features using the recently
	proposed SimpleMKL algorithm. SVM outputs are then averaged to exploit
	temporal information in the sequence. To evaluate our system, we
	perform deep experimentation on several key issues: influence of
	features and kernel function in histogram-based SVM approaches, influence
	of spatially independent information versus geometric local appearance
	information and benefits of combining both, sensitivity to training
	data, and interest of temporal context adaptation. We also compare
	our results with those of the other participants and try to explain
	why our method had the best performance during the facial expression
	recognition and analysis challenge.},
  doi = {10.1109/TSMCB.2012.2193567},
  issn = {1083-4419},
  keywords = {SimpleMKL algorithm;action unit detection;active appearance model
	coefficient;facial action recognition;facial emotion recognition;facial
	expression recognition;facial image;geometric local appearance information;heterogeneous
	feature;histogram intersection kernel;histogram-based SVM;kernel
	function;kernel matrix;local Gabor binary pattern histogram;multikernel
	learning;multikernel support vector machine;radial basis function
	kernel;sensitivity;temporal context adaptation;emotion recognition;face
	recognition;learning (artificial intelligence);support vector machines;}
}

@INPROCEEDINGS{Seppi2008,
  author = {Seppi, Dino and Batliner, Anton and Björn Schuller and Stefan Steidl
	and Thurid Vogt and Laurence Devillers and Laurence Vidrascu and
	Noam Amir and Vered Aharonson and Fondazione Bruno Kessler},
  title = {Patterns, Prototypes, Performance: Classifying Emotional User States},
  booktitle = {Proceedings of the International Conference on Spoken Language Processing
	(Interspeech)},
  year = {2008},
  address = {Brisbane, Australia},
  file = {Seppi2008.pdf:Seppi2008.pdf:PDF},
  keywords = {feature-generation classification-svm situation-woz multiannotator
	labels-emotion},
  owner = {tim},
  review = {nonprototypical},
  timestamp = {2010.06.22}
}

@ARTICLE{Shan2009,
  author = {Shan, Caifeng and Gong, Shaogang and McOwan, Peter W.},
  title = {Facial expression recognition based on Local Binary Patterns: A comprehensive
	study},
  journal = {Image and Vision Computing},
  year = {2009},
  volume = {27},
  pages = {803-816},
  number = {6},
  abstract = {Automatic facial expression analysis is an interesting and challenging
	problem, and impacts important applications in many areas such as
	human–computer interaction and data-driven animation. Deriving an
	effective facial representation from original face images is a vital
	step for successful facial expression recognition. In this paper,
	we empirically evaluate facial representation based on statistical
	local features, Local Binary Patterns, for person-independent facial
	expression recognition. Different machine learning methods are systematically
	examined on several databases. Extensive experiments illustrate that
	LBP features are effective and efficient for facial expression recognition.
	We further formulate Boosted-LBP to extract the most discriminant
	LBP features, and the best recognition performance is obtained by
	using Support Vector Machine classifiers with Boosted-LBP features.
	Moreover, we investigate LBP features for low-resolution facial expression
	recognition, which is a critical problem but seldom addressed in
	the existing work. We observe in our experiments that LBP features
	perform stably and robustly over a useful range of low resolutions
	of face images, and yield promising performance in compressed low-resolution
	video sequences captured in real-world environments.},
  address = {Newton, MA, USA},
  doi = {http://dx.doi.org/10.1016/j.imavis.2008.08.005},
  file = {Shan2009.pdf:Shan2009.pdf:PDF},
  issn = {0262-8856},
  keywords = {feature-generation facial expression recognition lbp-features classification-svm
	classification-lda posed-data classification-linear-programming corpus-pets
	corpus-mmi corpus-jaffe boost features-important},
  publisher = {Butterworth-Heinemann}
}

@ARTICLE{Shaver1987,
  author = {Shaver, P. and Schwartz, J. and Kirson, D. and O'connor, C.},
  title = {Emotion knowledge: further exploration of a prototype approach.},
  journal = {Journal of personality and social psychology},
  year = {1987},
  volume = {52},
  pages = {1061},
  number = {6},
  publisher = {American Psychological Association}
}

@INPROCEEDINGS{SheermanChase2013,
  author = {Sheerman-Chase, Tim and Eng-Jon Ong and Richard Bowden},
  title = {Non-linear Predictors for Facial feature Tracking across Pose and
	Expression},
  booktitle = {IEEE Conference on Automatic Face and Gesture Recognition, Shanghai},
  year = {2013},
  note = {(In press)},
  owner = {tim},
  timestamp = {2012.11.11}
}

@INPROCEEDINGS{SheermanChase2011,
  author = {Sheerman-Chase, Tim and Ong, Eng-Jon and Bowden, Richard},
  title = {Cultural Factors in the Regression of Non-verbal Communication Perception},
  booktitle = {Proceedings of the Workshop on Human Interaction in Computer Vision},
  year = {2011},
  address = {Barcelona},
  month = {Nov},
  abstract = {Recognition of NVC is important for understanding human communication
	and designing user centric user interfaces. Cultural differences
	affect the expression and perception of NVC but no previous automatic
	system considers these cultural differences. Annotation data for
	the LILiR TwoTalk corpus, containing dyadic (two person) conversations,
	was gathered using Internet crowdsourcing, with a significant quantity
	collected from India, Kenya and the UK. Many studies have investigated
	cultural differences based on human observations but this has not
	been addressed in the context of automatic emotion or NVC recognition.
	Perhaps not surprisingly, testing an automatic system on data that
	is not culturally representative of the training data is seen to
	result in low performance. We address this problem by training and
	testing our system on a specific culture to enable better modeling
	of the cultural differences in NVC perception. The system uses linear
	predictor tracking, with features generated based on distances between
	pairs of trackers. The annotations indicated the strength of the
	NVC which enables the use of nu-SVR to perform the regression.},
  file = {:SheermanChase2011.pdf:PDF},
  keywords = {feature selection boost adaboost nvc affect natural conversation},
  timestamp = {2011.09.06},
  url = {http://personal.ee.surrey.ac.uk/Personal/T.Sheerman-chase/}
}

@INPROCEEDINGS{SheermanChase2009,
  author = {Sheerman-Chase, Tim and Ong, Eng-Jon and Bowden, Richard},
  title = {Feature Selection of Facial Displays for Detection of Non Verbal
	Communication in Natural Conversation},
  booktitle = {Proceedings of the IEEE International Workshop on Human-Computer
	Interaction},
  year = {2009},
  address = {Kyoto},
  month = {Oct},
  abstract = {Recognition of human communication has previously focused on deliberately
	acted emotions or in structured or artificial social contexts. This
	makes the result hard to apply to realistic social situations. This
	paper describes the recording of spontaneous human communication
	in a specific and common social situation: conversation between two
	people. The clips are then annotated by multiple observers to reduce
	individual variations in interpretation of social signals. Temporal
	and static features are generated from tracking using heuristic and
	algorithmic methods. Optimal features for classifying examples of
	spontaneous communication signals are then extracted by AdaBoost.
	The performance of the boosted classifier is comparable to human
	performance for some communication signals, even on this challenging
	and realistic data set.},
  file = {SheermanChase2009.pdf:SheermanChase2009.pdf:PDF},
  keywords = {feature selection boost adaboost nvc affect natural conversation},
  owner = {ts00051},
  timestamp = {2009.10.26}
}

@INPROCEEDINGS{SheermanChase2009b,
  author = {Sheerman-Chase, Tim and Ong, Eng-Jon and Bowden, Richard},
  title = {Online Learning of Robust Facial Feature Trackers},
  booktitle = {Proceedings of the 3rd IEEE On-line Learning for Computer Vision
	Workshop},
  year = {2009},
  address = {Kyoto},
  month = {Oct},
  abstract = {This paper presents a head pose and facial feature estimation technique
	that works over a wide range of pose variations without a priori
	knowledge of the appearance of the face. Using simple LK trackers,
	head pose is estimated by Levenberg-Marquardt (LM) pose estimation
	using the feature tracking as constraints. Factored sampling and
	RANSAC are employed to both provide a robust pose estimate and identify
	tracker drift by constraining outliers in the estimation process.
	The system provides both a head pose estimate and the position of
	facial features and is capable of tracking over a wide range of head
	poses.},
  file = {SheermanChase2009b.pdf:SheermanChase2009b.pdf:PDF},
  keywords = {head pose online ransac lk tracking},
  owner = {ts00051},
  timestamp = {2009.10.26}
}

@ARTICLE{Sijtsma2009,
  author = {Klaas Sijtsma},
  title = {On the Use, the Misuse, and the Very Limited Usefulness of Cronbach’s
	Alpha},
  journal = {Psychometrika},
  year = {2009},
  volume = {74},
  pages = {107-120},
  number = {1},
  month = {March},
  abstract = {No abstract is available for this item.},
  keywords = {Cronbach’s alpha; internal consistency; reliability; unidimensionality},
  url = {http://ideas.repec.org/a/spr/psycho/v74y2009i1p107-120.html}
}

@ARTICLE{Sijtsma2009b,
  author = {Klaas Sijtsma},
  title = {Reliability Beyond Theory and Into Practice},
  journal = {Psychometrika},
  year = {2009},
  volume = {74},
  pages = {169-173},
  number = {1},
  month = {March},
  abstract = {No abstract is available for this item.},
  keywords = {common factor reliability; Cronbach’s alpha; structural equation modeling
	of reliability},
  url = {http://ideas.repec.org/a/spr/psycho/v74y2009i1p169-173.html}
}

@INPROCEEDINGS{Smith2005,
  author = {Smith, Paul and da Vitoria Lobo, Niels and Shah, Mubarak},
  title = {Temporal Boost for Event Recognition},
  booktitle = {Proceedings of the 10th IEEE International Conference on Computer
	Vision},
  year = {2005},
  month = {October},
  organization = {IEEE},
  citeulike-article-id = {361641},
  file = {Smith2005.pdf:Smith2005.pdf:PDF},
  keywords = {video-attention classification-boost pixel-features posed-data situation-acted
	activity recognition},
  posted-at = {2005-10-22 15:06:26},
  priority = {4}
}

@ARTICLE{SobolShikler2010,
  author = {Sobol-Shikler, Tal and Robinson, Peter},
  title = {Classification of complex information: inference of co-occurring
	affective states from their expressions in speech},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
  year = {2010},
  volume = {32},
  pages = {1284-1297},
  number = {7},
  month = {July},
  abstract = {We present a classification algorithm for inferring affective states
	(emotions, mental states, attitudes, and the like) from their nonverbal
	expressions in speech. It is based on the observations that affective
	states can occur simultaneously and different sets of vocal features,
	such as intonation and speech rate, distinguish between nonverbal
	expressions of different affective states. The input to the inference
	system was a large set of vocal features and metrics that were extracted
	from each utterance. The classification algorithm conducted independent
	pairwise comparisons between nine affective-state groups. The classifier
	used various subsets of metrics of the vocal features and various
	classification algorithms for different pairs of affective-state
	groups. Average classification accuracy of the 36 pairwise machines
	was 75 percent, using 10-fold cross validation. The comparison results
	were consolidated into a single ranked list of the nine affective-state
	groups. This list was the output of the system and represented the
	inferred combination of co-occurring affective states for the analyzed
	utterance. The inference accuracy of the combined machine was 83
	percent. The system automatically characterized over 500 affective
	state concepts from the Mind Reading database. The inference of co-occurring
	affective states was validated by comparing the inferred combinations
	to the lexical definitions of the labels of the analyzed sentences.
	The distinguishing capabilities of the system were comparable to
	human performance.},
  file = {SobolShikler2010.pdf:SobolShikler2010.pdf:PDF},
  keywords = {labels-emotion prototypical situation-acted corpus-mind-reading audio-features
	feature-generation classification-svm classification-tree},
  owner = {tim},
  timestamp = {2010.08.25}
}

@ARTICLE{Sokolova2006,
  author = {Sokolova, Marina and Japkowicz, Nathalie and Szpakowicz, Stan},
  title = {Beyond accuracy, f-score and roc: a family of discriminant measures
	for performance evaluation},
  journal = {Performance Evaluation},
  year = {2006},
  volume = {4304},
  pages = {1015–1021},
  number = {c},
  editor = {Sattar, Abdul and Kang, Byeong HoEditors},
  file = {:Sokolova2006.pdf:PDF},
  keywords = {performance metrics},
  publisher = {Springer},
  url = {http://www.springerlink.com/index/m35h804627066125.pdf}
}

@INPROCEEDINGS{Starner1995,
  author = {Starner, T. and Pentland, A.},
  title = {Real-Time {American} Sign Language Recognition From Video Using Hidden
	{Markov} Models},
  booktitle = {Proceedings of the International Symposium on Computer Vision},
  year = {1995},
  pages = {5B Systems and Applications},
  abstract = {Hidden Markov models (HMM’s) have been used prominently and successfully
	in speech recognition and, more recently, in handwriting recognition.
	Consequently, they seem ideal for visual recognition of complex,
	structured hand gestures such as are found in sign language. We describe
	a real-time HMM-based system for recognizing sentence level American
	Sign Language (ASL) which attains a word accuracy of 99.2% without
	explicitly modeling the fingers.},
  file = {:Starner1995.pdf:PDF},
  keywords = {sign hmm gesture feature-generation tracking-features classification-hmms},
  url = {citeseer.ist.psu.edu/starner96realtime.html}
}

@INPROCEEDINGS{Stiefelhagen97,
  author = {Stiefelhagen, Rainer and Meier, Uwe and Yang, Jie},
  title = {Real-time lip-tracking for lipreading},
  booktitle = {Proceedings of Eurospeech},
  year = {1997},
  abstract = {This paper presents a new approach to lip tracking for lipreading.
	Instead of only tracking features on lips, we propose to track lips
	along with other facial features such as pupils and nostril. In the
	new approach, the face is first located in an image using a stochastic
	skin-color model, the eyes, lip-corners and nostrils are then located
	and tracked inside the facial region. The new approach can effectively
	improve the robustness of lip-tracking and simplify automatic detection
	and recovery of tracking failure. The feasibility of the proposed
	approach has been demonstrated by implementation of a lip tracking
	system. The system has been tested by a database that contains 900
	image sequences of different speakers spelling words. The system
	has successfully extract lip regions from the image sequences to
	obtain training data for the audio-visual speech recognition system.
	The system has been also applied to extract the lip region in real-time
	from live video images to obtain the visua...},
  keywords = {speechreading lip tracking facial colour-model},
  review = {Lip corner tracking using geometry of face and shadows under typical
	lighting. The face is located using a 2D gaussian model of normalised
	skin colour. Eyes are located using grey scale shadow with an iterative
	threshold and looking for a pair of shadows. Lip corners found by
	horizontal integral projection. Pose is estimated from landmarks.
	Tracking is assumed lost of the predicted positions of landmarks
	are far away from projection based on face pose.}
}

@BOOK{Stubbs1983,
  title = {Discourse Analysis: the sociolinguistic analysis of natural language},
  publisher = {Basil Blackwell},
  year = {1983},
  author = {Stubbs, Michael},
  address = {Oxford},
  keywords = {unclassified}
}

@INPROCEEDINGS{Sun2011,
  author = {Xiaofan {Sun} and Jeroen {Lichtenauer} and Michel {Valstar} and Anton
	{Nijholt} and Maja {Pantic}},
  title = {A Multimodal Database for Mimicry Analysis},
  booktitle = {Affective Computing and Intelligent Interaction, Part I},
  year = {2011},
  editor = {Sidney {D'Mello} and Arthur {Graesser} and Bj\"orn {Schuller} and
	Jean-Claude {Martin}},
  volume = {6974},
  series = {Lecture Notes in Computer Science },
  pages = {367--376},
  address = {Berlin, Germany},
  month = {October},
  publisher = {Springer Verlag},
  abstract = {In this paper we introduce a multi-modal database for the analysis
	of human interaction, in particular mimicry, and elaborate on the
	theoretical hypotheses of the relationship between the occurrence
	of mimicry and human affect. The recorded experiments are designed
	to explore this relationship. The corpus is recorded with 18 synchronised
	audio and video sensors, and is annotated for many different phenomena,
	including dialogue acts, turn-taking, affect, head gestures, hand
	gestures, body movement and facial expression. Recordings were made
	of two experiments: a discussion on a political topic, and a role-playing
	game. 40 participants were recruited, all of whom selfreported their
	felt experiences. The corpus will be made available to the scientific
	community.},
  file = {:Sun2011.pdf:PDF},
  keywords = {Mimicry database, interaction scenario, synchronized multi-sensor
	recording, annotation, social signal processing, affective computing},
  url = {http://doc.utwente.nl/78299/}
}

@INPROCEEDINGS{Sun2012,
  author = {Xiaofan {Sun} and Anton {Nijholt} and Maja {Pantic}},
  title = {Towards Mimicry Recognition during Human Interactions: Automatic
	Feature Selection and Representation},
  booktitle = {Intelligent Technologies for Interactive Entertainment},
  year = {2012},
  editor = {Antonio {Camurri} and Cristina {Costa}},
  volume = {78},
  series = {Lecture Notes of the Institute for Computer Sciences, Social-Informatics
	and Telecommunications Engineering},
  pages = {160--169},
  address = {Heidelberg, Germany},
  month = {September},
  publisher = {Springer Verlag},
  abstract = {During face-to-face interpersonal interaction people have a tendency
	to mimic each other, that is, they change their own behaviors to
	adjust to the behavior expressed by a partner. In this paper we describe
	how behavioral information expressed between two interlocutors can
	be used to detect and identify mimicry and improve recognition of
	interrelationship and affect between them in a conversation. To automatically
	analyze how to extract and integrate this behavioral information
	into a mimicry detection framework for improving affective computing,
	this paper addresses the main challenge: mimicry representation in
	terms of optimal behavioral feature extraction and automatic integration.},
  keywords = {Mimicry representation, human-human interaction, human behavior analysis,
	motion energy},
  url = {http://doc.utwente.nl/81853/}
}

@INPROCEEDINGS{Sun2004,
  author = {Sun, Yafei and Sebe, Nicu and Lew, Michael and Gevers, Theo},
  title = {Authentic Emotion Detection in Real-Time Video},
  booktitle = {Proceedings of the International Workshop on Human Computer Interaction},
  year = {2004},
  volume = {3058},
  pages = {94-104},
  address = {Prague},
  abstract = {There is a growing trend toward emotional intelligence in human-computer
	interaction paradigms. In order to react appropriately to a human,
	the computer would need to have some perception of the emotional
	state of the human. We assert that the most informative channel for
	machine perception of emotions is through facial expressions in video.
	One current difficulty in evaluating automatic emotion detection
	is that there are currently no international databases which are
	based on authentic emotions. The current facial expression databases
	contain facial expressions which are not naturally linked to the
	emotional state of the test subject. Our contributions in this work
	are twofold: First, we create the first authentic facial expression
	database where the test subjects are showing the natural facial expressions
	based upon their emotional state. Second, we evaluate the several
	promising machine learning algorithms for emotion detection which
	include techniques such as Bayesian Networks, SVMs, and Decision
	trees.},
  file = {:Sun2004.pdf:PDF},
  keywords = {situation-woz facial emotion recognition natural-data label-self-assessment
	model-features classification-bn classification-tree labels-emotion
	classification-svm},
  owner = {ts00051},
  review = {Illicit emotions using videos. Many types of classifier used. Ekman
	style classes. Real time.},
  timestamp = {2010.02.19}
}

@ARTICLE{Sung2008,
  author = {Sung, Jaewon and Kanade, Takeo and Kim, Daijin},
  title = {Pose Robust Face Tracking by Combining Active Appearance Models and
	Cylinder Head Models},
  journal = {Int. J. Comput. Vision},
  year = {2008},
  volume = {80},
  pages = {260--274},
  number = {2},
  month = nov,
  acmid = {1412676},
  address = {Hingham, MA, USA},
  doi = {10.1007/s11263-007-0125-1},
  issn = {0920-5691},
  issue_date = {November 2008},
  keywords = {2D+3D active appearance models, Active appearance models, Face tracking},
  numpages = {15},
  publisher = {Kluwer Academic Publishers},
  url = {http://dx.doi.org/10.1007/s11263-007-0125-1}
}

@INPROCEEDINGS{Tang2007,
  author = {Fangqi Tang and Benzai Deng},
  title = {Facial Expression Recognition using {AAM} and Local Facial Features},
  booktitle = {Proceedings of the Third International Conference on Natural Computation},
  year = {2007},
  volume = {2},
  pages = {632-635},
  month = {August},
  abstract = {A new technique for facial expression recognition is proposed, which
	uses Active Appearance Model (AAM) to extract facial feature points
	and combines useful local shape features to form a classifier. To
	enhance performance of AAM, we use Adaboost to locate eye position
	to initialize AAM. After extraction of facial feature points, we
	analyze local facial changes and use some simple features to form
	an effective classifier. At last, we demonstrate our approach by
	experiments.},
  doi = {10.1109/ICNC.2007.373},
  file = {:Tang2007.pdf:PDF},
  keywords = {emotion recognition, face recognition, feature extractionAdaboost,
	active appearance model, facial expression recognition, facial feature
	point extraction, local facial features feature-generation aam-features
	tracker-features labels-facs corpus-cmu-pittsburgh texture-feature
	heuristic posed-data emotion recognition},
  review = {Use of AAMs for facial expression detection. Instead of using eigenvectors
	of feature points, they use proportions of the mouth, eyes, lips,
	etc to classify emotions (neutral, fear, happy, surprise).}
}

@INPROCEEDINGS{Tarasov2010,
  author = {Tarasov, A. and Cullen, C. and Delany, S},
  title = {Using Crowdsourcing for labeling emotional speech assets},
  booktitle = {Proceedings of the W3C Workshop on Emotion ML},
  year = {2010},
  address = {Paris, France},
  file = {:Tarasov2010.pdf:PDF},
  keywords = {crowdsourcing annotation tools},
  owner = {ts00051},
  timestamp = {2011.12.21}
}

@INPROCEEDINGS{Tax2011,
  author = {David Tax and Emile Hendriks and Valstar, Michel F. and Pantic, Maja},
  title = {The detection of concept frames using Clustering Multi-Instance Learning},
  booktitle = {Proceedings of International Conference on Pattern Recognition},
  year = {2010},
  pages = {2917-2921},
  abstract = {The classification of sequences requires the combination of information
	from different time points. In this paper the detection of facial
	expressions is considered. Experiments on the detection of certain
	facial muscle activations in videos show that it is not always required
	to model the sequences fully, but that the presence of specific frames
	(the concept frame) can be sufficient for a reliable detection of
	certain facial expression classes. For the detection of these concept
	frames a standard classifier is often sufficient, although a more
	advanced clustering approach performs better in some cases.},
  file = {Tax2011.pdf:Tax2011.pdf:PDF},
  keywords = {corpus-mmi mil labels-facs tracker-features face alignment classification-mil
	classification-hmms classification-crf feature-generation}
}

@ARTICLE{Terracciano2003,
  author = {Terracciano, A and Merritt, M and Zonderman, AB and Evans, MK},
  title = {Personality traits and sex differences in emotions recognition among
	{African} {Americans} and {Caucasians}},
  journal = {Annals of New York Academy of Sciences},
  year = {2003},
  volume = {1000},
  pages = {309-312},
  abstract = {This study investigated the role of personality traits and sex differences
	in emotion recognition. In several studies using samples with mostly
	young Caucasian and Asian students, Matsumoto et al.1 found strong
	evidence that recognition of emotional expression in faces was related
	to Openness to Experience and, to a lesser extent, Conscientiousness.
	Openness is one of the major dimensions of the five-factor model2
	(FFM) of personality that might play an important role in the recognition
	of emotion. Open individuals tend to be intellectually curious, imaginative,
	and sensitive to aesthetics and inner feelings. The present study
	seeks to replicate Matsumoto et al. and extend the findings to an
	older African American and an older Caucasian sample. Furthermore,
	this study tests whether the relation between personality traits
	and emotion recognition can be replicated with a purely verbal task.
	Finally, the hypothesis that women tend to be better than men in
	decoding facial expressions of emotion will be tested.},
  file = {:Terracciano2003.pdf:PDF},
  keywords = {emotion recognition human-performance perception psychology cross-cultural
	gender-difference},
  owner = {ts00051},
  timestamp = {2011.12.20}
}

@ARTICLE{Terzopoulos1993,
  author = {D. Terzopoulos and K. Waters},
  title = {Analysis and Synthesis of Facial Image Sequences Using Physical and
	Anatomical Models},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
  year = {1993},
  volume = {15},
  pages = {569-579},
  number = {6},
  abstract = {An approach to the analysis of dynamic facial images for the purposes
	of estimating and resynthesizing dynamic facial expressions is presented.
	The approach exploits a sophisticated generative model of the human
	face originally developed for realistic facial animation. The face
	model which may be simulated and rendered at interactive rates on
	a graphics workstation, incorporates a physics-based synthetic facial
	tissue and a set of anatomically motivated facial muscle actuators.
	The estimation of dynamical facial muscle contractions from video
	sequences of expressive human faces is considered. An estimation
	technique that uses deformable contour models (snakes) to track the
	nonrigid motions of facial features in video images is developed.
	The technique estimates muscle actuator controls with sufficient
	accuracy to permit the face model to resynthesize transient expressions},
  address = {Washington, DC, USA},
  doi = {http://dx.doi.org/10.1109/34.216726},
  file = {:/home/ts00051/Dropbox/docs/papers/Terzopoulos1993.pdf:PDF},
  issn = {0162-8828},
  keywords = {model, expression, synthesis, face-scanning},
  publisher = {IEEE Computer Society}
}

@ARTICLE{Thirumalai2003,
  author = {Thirumalai, M. S.},
  title = {Understanding Non-verbal Behavior},
  journal = {Language in India},
  year = {2003},
  volume = {3},
  number = {9},
  month = {September},
  keywords = {nvc-definition translation},
  owner = {tim},
  review = {Not peer reviewed?},
  timestamp = {2011.07.07}
}

@TECHREPORT{Tomasi1991,
  author = {Tomasi, Carlo and Kanade, Takeo},
  title = {Detection and Tracking of Point Features},
  institution = {Carnegie Mellon University Technical Report CMU-CS-91-132},
  year = {1991},
  month = {April},
  file = {:/home/ts00051/Dropbox/docs/papers/Tomasi1991.ps.gz:PostScript},
  keywords = {tracker entropy good-to-track lk},
  owner = {ts00051},
  timestamp = {2012.01.04}
}

@ARTICLE{Torkkola2003,
  author = {Torkkola, Kari},
  title = {{Feature extraction by non parametric mutual information maximization}},
  journal = {J. Mach. Learn. Res.},
  year = {2003},
  volume = {3},
  pages = {1415--1438},
  address = {Cambridge, MA, USA},
  citeulike-article-id = {1305175},
  citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=944981},
  issn = {1533-7928},
  keywords = {feature\_selection},
  posted-at = {2007-05-18 12:45:57},
  priority = {2},
  publisher = {MIT Press},
  url = {http://portal.acm.org/citation.cfm?id=944981}
}

@INPROCEEDINGS{Treiblmaier2011,
  author = {Treiblmaier, Horst and Peter Filzmoser},
  title = {Benefits from Using Continuous Rating Scales in Online Survey Research},
  booktitle = {International Conference on Information Systems (ICIS)},
  year = {2011},
  owner = {tim},
  timestamp = {2013.03.31}
}

@PHDTHESIS{Truong2009,
  author = {K.P. Truong},
  title = {How does real affect affect affect recognition in speech?},
  school = {University of Twente},
  year = {2009},
  address = {Enschede},
  month = {September},
  abstract = {The automatic analysis of affect is a relatively new and challenging
	multidisciplinary research area that has gained a lot of interest
	over the past few years. The research and development of affect recognition
	systems has opened many opportunities for improving the interaction
	between man and machine. Although affect can be expressed through
	multimodal means like hand gestures, facial expressions, and body
	postures, this dissertation has focused on speech (i.e., vocal expressions)
	as the main carrier of affect. Speech carries a lot of ?hidden? information.
	By hearing a voice only, humans can guess who is speaking, what language
	he/she is speaking (or accent or dialect), what age he/she is etc.
	The goal of automatic speech recognition (ASR) is to recognize what
	is said. In automatic speech-based emotion recognition, the goal
	is to recognize how something is said. In this work, several experiments
	are described which were carried out to investigate how affect can
	be automatically recognized in speech. One of the first steps in
	developing speech-based affect recognizers involves finding a spontaneous
	speech corpus that is labeled with emotions. Machine learning techniques,
	that are often used to build these recognizers, require these data
	to learn how to associate specific speech features (e.g., pitch,
	energy) with certain emotions. However, collecting and labeling real
	affective speech data has appeared to be difficult. Efforts to collecting
	affective speech data in the field have been described in this work.
	As an alternative, speech corpora that contain acted emotional speech
	(actors are asked to portray certain emotions) have often been used.
	Advantages of these corpora are that the recording conditions can
	be controlled, the emotions portrayed can be clearly associated with
	an emotion label, the costs and effort required to collect such corpora
	are relatively low, and the recordings are usually made available
	to the research community. In this work, an acted emotional speech
	corpus (containing basic, universal emotions like Anger, Boredom,
	Disgust, Fear, appiness, Neutral, and Sadness) was used to explore
	and apply recognition techniques and evaluation frameworks, adopted
	from similar research areas like automatic speaker and language recognition,
	to automatic emotion recognition. Recognizers were evaluated in a
	detection framework, and an evaluation for handling so-called ?out-of-set?
	emotions (unknown emotions that were not present in the training
	data, but which can occur in real-life situations) was presented.
	Partly due to lack of standardization and shared databases, the evaluation
	of affect recognizers remains somewhat problematic. While evaluation
	is an important aspect in development, it has been a relatively underexposed
	topic of investigation in the emotion research community. The main
	objections against the use of acted emotional speech corpora are
	that the expressions are not ?real? but rather portrayals of prototype
	emotions (and hence, expressed rather exaggeratedly), and the emotions
	portrayed do not often occur in real life situations. Therefore,
	in this work, spontaneous data has also been used and methods were
	developed to recognize spontaneous, vocal expressions of affect,
	like laughter. The task of the laughter detector was to recognize
	audible laughter in meeting speech data. Using a combination of Gaussian
	Mixture Models (GMMs)and Support Vector Machines (SVMs), and a combination
	of prosodic and spectral speech features, relatively low error rates
	between 3%?12% were achieved. Although the detector did not interpret
	the affective meaning of the laughter, the detection of laughter
	alone was informative enough. Part of these findings were used to
	build a so-called ?Affective Mirror? that successfully elicited and
	recognized laughter with different user groups. Other speech phenomena
	related to vocal expressions of affect, also in the context of meeting
	speech data, are the expressions of opinions and sentiments. In this
	work, it was assumed that opinions are expressed differently from
	factual statements in terms of tone of voice, and the words used.
	Classification experiments were carried out to find the best combination
	of lexical and prosodic features for the discrimination between subjective
	and non-subjective clauses. As lexical features, word-level, phone-level,
	and character-level n-grams were used. The experiments showed that
	a combination of all features yields the best performances, and that
	the prosodic features were the weakest of all features investigated.
	In addition, a second task was formulated, namely the discrimination
	between positive subjective clauses and negative subjective clauses.
	Similar results for this task were found. The relatively high error
	rates for both tasks, Cdet = 26%?30%, indicat that these are more
	difficult recognition problems than that of laughter: the relation
	between prosodic and lexical features, and subjectivity and polarity
	(i.e., positive vs. negative), is not as clear as is in the case
	of laughter. As an intermediate between real affective expressions
	and acted expressions, elicited affective expressions were employed
	in this dissertation in several human perception and classification
	experiments. To this end, a multimodal corpus with elicited affect
	was recorded. Affective vocal and facial expressions were elicited
	via a multiplayer first-person shooter video game (Unreal Tournament)
	that was manipulated by the experimenter. These expressions were
	captured by close-talk microphones and high-quality webcams, and
	were afterwards rated by the players themselves on Arousal (active-passive)
	and Valence (positive-negative) scales. After post-processing the
	data, perception and classification experiments were carried out
	on this data. The first experiment carried out with this unique kind
	of data tried to answer the question how the level of agreement between
	observers on the perceived emotion is affected when audio-only, video-only,
	audiovisual, or audiovisual + context information clips containing
	affective expressions are shown. The observers were asked to rate
	each clip on Arousal and Valence scales. The results showed that
	the agreement among human observers was highest when audiovisual
	clips were shown. Furthermore, the observers reached higher agreement
	on Valence judgments than Arousal judgments. Additionally, the results
	indicated that the ?self?-ratings of the gamers themselves differed
	somewhat from the ?observed?-ratings of the human observers. This
	finding was further investigated in a second xperiment. Six raters
	re-annotated a substantial part of the corpus. The results confirmed
	that there is a discrepancy between what the ?self?-raters (i.e.,
	the gamers themselves) experienced/felt and what observers perceive
	based on the gamers? vocal and facial expressions. This finding has
	consequences for the development of automatic affect analyzers that
	use these ratings: the goal of affect analyzers can be to recognize
	?felt? affect, or to recognize ?observed/perceived? affect. Two different
	types of speech-based affect recognizers were developed in parallel
	to recognize either ?felt? or ?perceived? affect on continuous Arousal
	and Valence scales. The results showed that ?felt? emotions are much
	harder to predict than ?perceived? emotions. Although these recognizers
	performed moderately from a classification perspective, the recognizers
	did not perform too bad in comparison to human performance. The recognizers
	developed depend much on how the affect data is rated by humans;
	if this data reflects moderate human judgments of affect, then it
	can be difficult for the machine to perform well (in an absolute
	sense). The work presented in this dissertation shows that the automatic
	recognition of affect in speech is complicated by the fact that real
	affect, as encountered in reallife situations, is a very complex
	phenomenon that sometimes cannot be described straightforwardly in
	ways that can be useful for computer scientists (who would like to
	build affect recognizers). The use of real affect data has led to
	the development of recognizers that are more targeted toward affect-related
	expressions. Laughter and subjectivity are examples of such affect-related
	expressions. The Arousal and Valence descriptors offer a nice way
	to describe the meaning of these affective expressions. The relatively
	high error rates obtained for Arousal and Valence prediction, suggest
	that the acoustic correlates used in this research only partly capture
	the characteristics of real affective speech. The search for stronger
	acoustic correlates or vocal profiles for specific emotions continues.
	This search is partly complicated by the ?noise? that comes with
	real affect which remains a challenge for the research community
	working toward automatic affect analyzers.},
  file = {Truong2009.pdf:Truong2009.pdf:PDF},
  keywords = {emotion taxonomy abstract-emotion-scales audio-features feature-generation
	corpus-list posed-data detection corpus-berlin feeltrace labels-emotion
	classification-svm situation-meeting natural-data labels-laughter},
  number = {CTIT Ph.D.-thesis series No. 09-152},
  publisher = {University of Twente},
  url = {http://doc.utwente.nl/67865/}
}

@ARTICLE{Tsiamyrtzis07,
  author = {P. Tsiamyrtzis and J. Dowdall and D. Shastri and I. T. Pavlidis and
	M. G. Frank and P. Ekman},
  title = {Imaging Facial Physiology for the Detection of Deceit},
  journal = {International Journal of Computer Vision (IJCV)},
  year = {2007},
  volume = {71},
  pages = {197--214},
  number = {2},
  abstract = {Previous work has demonstrated the correlation of increased blood
	perfusion in the orbital muscles and stress levels for human beings.
	It has also been suggested that this periorbital perfusion can be
	quantified through the processing of thermal video. The idea has
	been based on the fact that skin temperature is heavily modulated
	by superficial blood flow. Proof of this concept was established
	for two different types of stress inducing experiments: startle experiments
	and mock-crime polygraph interrogations. However, the polygraph interrogation
	scenarios were simplistic and highly constrained. In the present
	paper, we report results derived from a large and realistic mock-crime
	interrogation experiment. The interrogation is free flowing and no
	restrictions have been placed on the subjects. Additionally, we propose
	a new methodology to compute the mean periorbital temperature signal.
	The present approach addresses the deficiencies of the earlier methodology
	and is capable of coping with the challenges posed by the realistic
	setting. Specifically, it features a tandem CONDENSATION tracker
	to register the periorbital area in the context of a moving face.
	It operates on the raw temperature signal and tries to improve the
	information content by suppressing the noise level instead of amplifying
	the signal as a whole. Finally, a pattern recognition method classifies
	stressful (Deceptive) from non-stressful (Non-Deceptive) subjects
	based on a comparative measure between the entire interrogation signal
	(baseline) and a critical subsection of it (transient response).
	The successful classification rate is 87.2% for 39 subjects. This
	is on par with the success rate achieved by highly trained psycho-physiological
	experts and opens the way for automating lie detection in realistic
	settings.},
  address = {Hingham, MA, USA},
  doi = {http://dx.doi.org/10.1007/s11263-006-6106-y},
  issn = {0920-5691},
  keywords = {nvc-applications tracker-features labels-deception feature-generation
	polygraph},
  publisher = {Kluwer Academic Publishers},
  review = {Tracking of faces in thermal IR video using the condensation algorithm.
	The face temperature of the face was then used as a detector of stress
	level for the detection of lies.}
}

@INPROCEEDINGS{Tu2005,
  author = {Tu, Zhuowen},
  title = {Probabilistic Boosting-Tree: Learning Discriminative Models for Classification,
	Recognition, and Clustering},
  booktitle = {Proceedings of the 10th IEEE International Conference on Computer
	Vision},
  year = {2005},
  pages = {1589--1596},
  address = {Washington, DC, USA},
  publisher = {IEEE Computer Society},
  abstract = {In this paper, a new learning framework–probabilistic boosting-tree
	(PBT), is proposed for learning two-class and multi-class discriminative
	models. In the learning stage, the probabilistic boosting-tree automatically
	constructs a tree in which each node combines a number of weak classifiers
	(evidence, knowledge) into a strong classifier (a conditional posterior
	probability). It approaches the target posterior distribution by
	data augmentation (tree expansion) through a divide-and-conquer strategy.
	In the testing stage, the conditional probability is computed at
	each tree node based on the learned classifier, which guides the
	probability propagation in its sub-trees. The top node of the tree
	therefore outputs the overall posterior probability by integrating
	the probabilities gathered from its sub-trees. Also, clustering is
	naturally embedded in the learning phase and each sub-tree represents
	a cluster of certain level. The proposed framework is very general
	and it has interesting connections to a number of existing methods
	such as the � £ algorithm, decision tree algorithms, generative models,
	and cascade approaches. In this paper, we show the applications of
	PBT for classification, detection, object recognition. We have also
	applied the framework in segmentation.},
  doi = {http://dx.doi.org/10.1109/ICCV.2005.194},
  file = {Tu2005.pdf:Tu2005.pdf:PDF},
  isbn = {0-7695-2334-X-02},
  keywords = {machine learning technique classification-method}
}

@PHDTHESIS{Turkmani2008,
  author = {Turkmani, Aseel},
  title = {Visual Analysis of Viseme Dynamics},
  school = {University of Surrey},
  year = {2008},
  keywords = {speechreading lipreading facial animation}
}

@INPROCEEDINGS{Tzimiropoulos2011,
  author = {G. Tzimiropoulos and S. Zafeiriou and M. Pantic},
  title = {Robust and Efficient Parametric Face Alignment},
  booktitle = {Proceedings of IEEE International Conference on Computer Vision},
  year = {2011},
  pages = {1847--1854},
  month = {November},
  abstract = {We propose a correlation-based approach to parametric object alignment
	particularly suitable for face analysis applications which require
	efficiency and robustness against occlusions and illumination changes.
	Our algorithm registers two images by iteratively maximizing their
	correlation coefficient using gradient ascent. We compute this correlation
	coefficient from complex gradients which capture the orientation
	of image structures rather than pixel intensities. The maximization
	of this gradient correlation coefficient results in an algorithm
	which is as computationally efficient as ℓ2 norm-based algorithms,
	can be extended within the inverse compositional framework (without
	the need for Hessian recomputation) and is robust to outliers. To
	the best of our knowledge, no other algorithm has been proposed so
	far having all three features. We show the robustness of our algorithm
	for the problem of face alignment in the presence of occlusions and
	non-uniform illumination changes. The code that reproduces the results
	of our paper can be found at http://ibug.doc.ic.ac.uk/resources.}
}

@INPROCEEDINGS{Vakayallapati2011,
  author = {Vakayallapati, H.D. and Anne, K.R and Kyamakya, K.},
  title = {Emotion Recognition from Decision Level Fusion of Visual and Acoustic
	Features using Hausdorff Classifier},
  booktitle = {Fifth International Conference on Information Processing (ICIP-2011)},
  year = {2011},
  owner = {tim},
  timestamp = {2012.10.22}
}

@PHDTHESIS{Valstar2008,
  author = {Valstar, Michel},
  title = {Timing is everything, A spatio-temporal approach to the analysis
	of facial actions},
  school = {Imperial College},
  year = {2008},
  file = {:Valstar2008.pdf:PDF},
  owner = {tim},
  timestamp = {2012.05.10}
}

@ARTICLE{Valstar2012b,
  author = {Valstar, M.F. and Mehu, M. and Bihan Jiang and Pantic, M. and Scherer,
	K.},
  title = {Meta-Analysis of the First Facial Expression Recognition Challenge},
  journal = {Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions
	on},
  year = {2012},
  volume = {42},
  pages = {966 -979},
  number = {4},
  month = {aug. },
  abstract = {Automatic facial expression recognition has been an active topic in
	computer science for over two decades, in particular facial action
	coding system action unit (AU) detection and classification of a
	number of discrete emotion states from facial expressive imagery.
	Standardization and comparability have received some attention; for
	instance, there exist a number of commonly used facial expression
	databases. However, lack of a commonly accepted evaluation protocol
	and, typically, lack of sufficient details needed to reproduce the
	reported individual results make it difficult to compare systems.
	This, in turn, hinders the progress of the field. A periodical challenge
	in facial expression recognition would allow such a comparison on
	a level playing field. It would provide an insight on how far the
	field has come and would allow researchers to identify new goals,
	challenges, and targets. This paper presents a meta-analysis of the
	first such challenge in automatic recognition of facial expressions,
	held during the IEEE conference on Face and Gesture Recognition 2011.
	It details the challenge data, evaluation protocol, and the results
	attained in two subchallenges: AU detection and classification of
	facial expression imagery in terms of a number of discrete emotion
	categories. We also summarize the lessons learned and reflect on
	the future of the field of facial expression recognition in general
	and on possible future challenges in particular.},
  doi = {10.1109/TSMCB.2012.2200675},
  issn = {1083-4419},
  keywords = {AU;automatic facial expression recognition;challenge data;computer
	science;discrete emotion categories;discrete emotion states;evaluation
	protocol;facial action coding system action unit;facial expression
	databases;facial expressive imagery;first facial expression recognition
	challenge;gesture recognition;meta analysis;emotion recognition;face
	recognition;gesture recognition;}
}

@INPROCEEDINGS{Valstar2011,
  author = {Valstar, Michel F. and Bihan Jiang and Marc M\'{e}hu and Pantic,
	Maja and Klaus Scherer},
  title = {The First Facial Expression Recognition and Analysis Challenge},
  booktitle = {Proceedings of the IEEE International Conference on Automatic Face
	and Gesture Recognition},
  year = {2011},
  file = {:Valstar2011.pdf:PDF},
  keywords = {FERA2011 posed-data corpus-GEMEP-FERA},
  owner = {tim},
  timestamp = {2011.11.13}
}

@INPROCEEDINGS{Valstar2011slides,
  author = {Valstar, Michel F. and M\'{e}hu, Marc and Marcello Mortillaro and
	Pantic, Maja and Klaus Scherer},
  title = {Meta-analysis of Challenge Slides},
  booktitle = {First Facial Expression Recognition and Analysis Challenge ({FERA}2011)},
  year = {2011},
  owner = {ts00051},
  timestamp = {2012.02.23}
}

@ARTICLE{Valstar2012,
  author = {Valstar, Michel F. and Pantic, Maja},
  title = {Fully Automatic Recognition of the Temporal Phases of Facial Actions},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics},
  year = {2012},
  volume = {42},
  pages = {28--43},
  number = {1},
  month = {February},
  doi = {10.1109/TSMCB.2011.2163710},
  file = {:Valstar2012.pdf:PDF},
  issn = {1083-4419},
  keywords = {Gabor-feature-based boosted classifiers;GentleBoost;action units;decision
	making;facial behavior;facial fiducial point localization;facial
	muscle action temporal phase automatic recognition;facial point detector;factorized
	likelihoods;hidden Markov models;image sequence;particle filtering;support
	vector machines;temporal activation models;face recognition;hidden
	Markov models;image classification;image sequences;particle filtering
	(numerical methods);support vector machines;}
}

@INPROCEEDINGS{Valstar2010,
  author = {Valstar, M F and M Pantic},
  title = {Induced Disgust, Happiness and Surprise: an Addition to the MMI Facial
	Expression Database},
  booktitle = {Proceedings of Int'l Conf. Language Resources and Evaluation, Workshop
	on EMOTION},
  year = {2010},
  pages = {65-70},
  address = {Malta},
  month = {May},
  url = {http://ibug.doc.ic.ac.uk/media/uploads/documents/EMOTION-2010-ValstarPantic-CAMERA.pdf}
}

@INPROCEEDINGS{Valstar2006,
  author = {Valstar, Michel F. and Pantic, Maja and Zara Ambadar and Jeffrey
	F. Cohn},
  title = {Spontaneous vs. Posed Facial Behavior: Automatic Analysis of Brow
	Actions},
  booktitle = {Proceedings of the 8th International Conference on Multimodal interfaces},
  year = {2006},
  pages = {162--170},
  address = {New York, NY, USA},
  publisher = {ACM},
  abstract = {Past research on automatic facial expression analysis has focused
	
	mostly on the recognition of prototypic expressions of discrete
	
	emotions rather than on the analysis of dynamic changes over
	
	time, although the importance of temporal dynamics of facial
	
	expressions for interpretation of the observed facial behavior has
	
	been acknowledged for over 20 years. For instance, it has been
	
	shown that the temporal dynamics of spontaneous and volitional
	
	smiles are fundamentally different from each other. In this work,
	
	we argue that the same holds for the temporal dynamics of brow
	
	actions and show that velocity, duration, and order of occurrence
	
	of brow actions are highly relevant parameters for distinguishing
	
	posed from spontaneous brow actions. The proposed system for
	
	discrimination between volitional and spontaneous brow actions
	
	is based on automatic detection of Action Units (AUs) and their
	
	temporal segments (onset, apex, offset) produced by movements
	
	of the eyebrows. For each temporal segment of an activated AU,
	
	we compute a number of mid-level feature parameters including
	
	the maximal intensity, duration, and order of occurrence. We use
	
	Gentle Boost to select the most important of these parameters.
	
	The selected parameters are used further to train Relevance
	
	Vector Machines to determine per temporal segment of an
	
	activated AU whether the action was displayed spontaneously or
	
	volitionally. Finally, a probabilistic decision function determines
	
	the class (spontaneous or posed) for the entire brow action. When
	
	tested on 189 samples taken from three different sets of
	
	spontaneous and volitional facial data, we attain a 90.7% correct
	
	recognition rate.},
  doi = {http://doi.acm.org/10.1145/1180995.1181031},
  file = {Valstar2006.pdf:Valstar2006.pdf:PDF},
  isbn = {1-59593-541-X},
  keywords = {natural-important feature-selection gentleboost classification-rvm
	tracker-features clip-statistics label-posed posed-data natural-data},
  location = {Banff, Alberta, Canada},
  review = {By tracking 8 landmarks on the face, AUs were determined (duration
	and intensity). The aim was to distinguish between voluntary muscle
	movement and genuine emotions. The system selected relevant features
	of AUs using gentle boost and trained RVMs on those features to classify
	AUs. Note: this may have an impact if people in the dataset attempt
	to change their face movements.}
}

@BOOK{Vapnik1998,
  title = {Statistical Learning Theory},
  publisher = {John Wiley},
  year = {1998},
  author = {Vapnik, V},
  address = {New York, NY},
  keywords = {svm support vector machine learning supervised classification-svm
	classification-method},
  owner = {ts00051},
  timestamp = {2009.10.28}
}

@ARTICLE{Varshney1997,
  author = {Varshney, P.K.},
  title = {Multisensor data fusion},
  journal = {Electronics Communication Engineering Journal},
  year = {1997},
  volume = {9},
  pages = {245 -253},
  number = {6},
  month = {dec},
  abstract = {Multisensor data fusion refers to the acquisition, processing and
	synergistic combination of information gathered by various knowledge
	sources and sensors to provide a better understanding of a phenomenon.
	It is a fascinating and rapidly evolving field that has generated
	a lot of excitement in the research and development community. These
	concepts are being applied to a wide variety of fields such as military
	command and control, robotics, image processing, air traffic control,
	medical diagnostics, pattern recognition and environmental monitoring.
	This paper presents a brief overview of the field and illustrates
	its potential by means of two examples},
  doi = {10.1049/ecej:19970602},
  issn = {0954-0695},
  keywords = {acquisition;air traffic control;environmental monitoring;image processing;knowledge
	sources;medical diagnostics;military command and control;multisensor
	data fusion;pattern recognition;processing;research and development
	community;robotics;air traffic control;command and control systems;image
	processing;patient diagnosis;pattern recognition;robots;sensor fusion;}
}

@ARTICLE{Vassallo2009,
  author = {Vassallo, Suzane and Cooper, Sian L. and Douglas, Jacinta M.},
  title = {Visual scanning in the recognition of facial affect: Is there an
	observer sex difference?},
  journal = {Journal of Vision},
  year = {2009},
  volume = {9},
  pages = {1-10},
  number = {3},
  month = {3},
  abstract = {This investigation assessed whether differences exist in the way males
	and females overtly orient their visual attention to salient facial
	features while viewing static emotional facial expressions. Eye movements
	were recorded while fifty healthy participants (23 males, 27 females)
	viewed a series of six universal facial expressions. Groups were
	compared with respect to accuracy and reaction time in emotional
	labeling. The number and duration of foveal fixations to four predefined
	facial areas of interest (AOIs)—each eye, nose, mouth—were also recorded.
	There were no significant group differences with respect to accuracy
	(p = 0.997), though females were significantly faster than males
	in correctly identifying expressions (p = 0.047). Analysis of the
	visual scan path revealed that while both groups spent more time
	and looked more frequently at the eye region, males spent significantly
	more time viewing the nose and mouth. The duration and number of
	fixations made to the nose were significantly greater in males (p
	< 0.05). This study is the first to show reaction time differences
	between the sexes across a range of universal emotions. Further,
	this is the first work to suggest the orienting of attention to the
	lower part of the face, especially the nose, appears to differentiate
	the sexes.},
  eprint = {http://journalofvision.org/9/3/11/Vassallo-2009-jov-9-3-11.pdf},
  issn = {1534-7362},
  keywords = {reaction time, facial expression, eye movements, ocular fixation,
	nonverbal communication, emotions basic-emotions gender-difference
	gaze perception},
  url = {http://journalofvision.org/9/3/11/}
}

@INBOOK{Verderber2007,
  chapter = {Communicating Through Non-verbal Behaviours},
  pages = {77},
  title = {Communicate!},
  publisher = {Cengage Learning Editores},
  year = {2007},
  author = {Verderber, Rudolph F. and Verderber , Kathleen S.},
  keywords = {nvc nvc-definition taxonomy},
  owner = {ts00051},
  timestamp = {2012.01.10}
}

@ARTICLE{Verduyn2009,
  author = {Verduyn, Philippe and Delvaux, Ellen and Van Coillie, Hermina and
	Tuerlinckx, Francis and Van Mechelen, Iven},
  title = {Predicting the duration of emotional experience: Two experience sampling
	studies},
  journal = {Emotion},
  year = {2009},
  volume = {9},
  pages = {83-91},
  number = {1},
  abstract = {The authors present 2 studies to explain the variability in the duration
	of emotional experience. Participants were asked to report the duration
	of their fear, anger, joy, gratitude, and sadness episodes on a daily
	basis. Information was further collected with regard to potential
	predictor variables at 3 levels: trait predictors, episode predictors,
	and moment predictors. Discrete-time survival analyses revealed that,
	for all 5 emotions under study, the higher the importance of the
	emotion-eliciting situation and the higher the intensity of the emotion
	at onset, the longer the emotional experience lasts. Moreover, a
	reappearance, either physically or merely mentally, of the eliciting
	stimulus during the emotional episode extended the duration of the
	emotional experience as well. These findings display interesting
	links with predictions within N. H. Frijda's theory of emotion, with
	the phenomenon of reinstatement (as studied within the domain of
	learning psychology), and with the literature on rumination.},
  keywords = {emotion duration psychological},
  owner = {ts00051},
  timestamp = {2010.03.01}
}

@INPROCEEDINGS{Vinciarelli2009,
  author = {A. Vinciarelli and A. Dielmann and S. Favre and H. Salamin},
  title = {{Canal9: A database of political debates for analysis of social interactions}},
  booktitle = {Affective Computing and Intelligent Interaction},
  year = {2009},
  doi = {10.1109/ACII.2009.5349466},
  masid = {5406390}
}

@ARTICLE{Vinciarelli2008,
  author = {Vinciarelli, A. and Pantic, M. and Bourlard, H.},
  title = {Social Signal Processing: Survey of an Emerging Domain},
  journal = {Image and Vision Computing},
  year = {2008},
  month = {December},
  citeulike-article-id = {3782978},
  citeulike-linkout-0 = {http://dx.doi.org/10.1016/j.imavis.2008.11.007},
  citeulike-linkout-1 = {http://linkinghub.elsevier.com/retrieve/pii/S0262885608002485},
  doi = {10.1016/j.imavis.2008.11.007},
  file = {Vinciarelli2008.pdf:Vinciarelli2008.pdf:PDF},
  issn = {02628856},
  keywords = {nvc taxonomy review-paper nvc-applications corpus-list natural-important},
  posted-at = {2008-12-12 17:48:38},
  priority = {2},
  url = {http://dx.doi.org/10.1016/j.imavis.2008.11.007}
}

@ARTICLE{Viola2002,
  author = {Viola, P. and Jones, M.},
  title = {Robust real-time object detection},
  journal = {International Journal of Computer Vision},
  year = {2002},
  volume = {57},
  pages = {137--154},
  number = {2},
  owner = {ts00051},
  timestamp = {2012.04.30}
}

@INPROCEEDINGS{Viola2006,
  author = {Viola, Paul and Platt, John C. and Zhang, Cha},
  title = {Multiple Instance Boosting for Object Detection},
  booktitle = {Advances in Neural Information Processing Systems},
  year = {2006},
  pages = {1417-1426},
  month = {January},
  file = {Viola2007.pdf:Viola2007.pdf:PDF},
  keywords = {mil instance bag boost classification-method machine learning method
	supervised},
  owner = {ts00051},
  timestamp = {2011.09.16}
}

@INPROCEEDINGS{Visser1999,
  author = {Michiel Visser and Mannes Poel and Anton Nijholt},
  title = {Classifying Visemes for Automatic Lipreading},
  booktitle = {Proceedings of the Second International Workshop on Text, Speech
	and Dialogue},
  year = {1999},
  pages = {349-352},
  address = {London, UK},
  publisher = {Springer-Verlag},
  abstract = {Automatic lipreading is automatic speech recognition that uses only
	visual information. The relevant data in a video signal is isolated
	and features are extracted from it. From a sequence of feature vectors,
	where every vector represents one video image, a sequence of higher
	level semantic elements is formed. These semantic elements are “visemes
	” the visual equivalent of “phonemes ” The developed prototype uses
	a Time Delayed Neural Network to classify the visemes.},
  file = {:Visser1999.pdf:PDF},
  isbn = {3-540-66494-7},
  keywords = {speechreading lipreading feature-generation classification-neural
	feature-extraction texture-feature},
  review = {Lip features were extracted using a neural network (I think). Lip
	positions were processed by a time delay neural network to classify
	visemes. The average detection rate of the correct viseme was only
	0.226 due to confusion between the first 7 visemes (there were 14
	total).}
}

@INCOLLECTION{Vogt1996,
  author = {M. Vogt},
  title = {Fast matching of a dynamic lip model to color video sequences under
	regular illumination conditions},
  booktitle = {Speechreading by Humans and Machines},
  publisher = {Springer Verlag},
  year = {1996},
  editor = {David G. Stork and Marcus E. Hennecke},
  volume = {volume 150 of NATO ASI Series},
  series = {Series F: Computer and Systems Sciences},
  pages = {399-40},
  address = {Berlin},
  keywords = {facial tracking lip contour},
  review = {Lip tracking using an energy minimization of a lip model based on
	bezier curves. The minimization is performed by simulated annealing
	on a sobel filtered ROI. The ROI is selected using a LUT in HSI colour
	space. Teeth visibility is detected using neural network.}
}

@ARTICLE{Vuust2010,
  author = {Vuust, Peter and Gebauer, Line and Hansen, Niels Chr. and J{\o}rgensen,
	Stine Ramsgaard and M{\o}ller, Arne and Linnet, Jakob},
  title = {Personality influences career choice: sensation seeking in professional
	musicians},
  journal = {Music Education Research},
  year = {2010},
  volume = {12},
  pages = {219-230},
  number = {2},
  abstract = {Despite the obvious importance of deciding which career to pursue,
	little is known about the influence of personality on career choice.
	Here we investigated the relation between sensation seeking, a supposedly
	innate personality trait, and career choice in classical and 'rhythmic'
	students at the academies of music in Denmark. We compared data from
	groups of 59 classical and 36 'rhythmic' students, who completed
	a psychological test battery comprising the Zuckerman Sensation Seeking
	Scale, the Spielberger State-Trait Anxiety Inventory, as well as
	information about demographics and musical background. 'Rhythmic'
	students had significantly higher sensation seeking scores than classical
	students, predominantly driven by higher boredom susceptibility.
	Classical students showed significantly higher levels of state anxiety,
	when imagining themselves just before entering the stage for an important
	concert. The higher level of anxiety related to stage performance
	in classical musicians was not attributed to group differences in
	trait anxiety, but is presumably a consequence of differences in
	musical rehearsing and performance practices of the two styles of
	music. The higher sensation seeking scores observed in 'rhythmic'
	students, however, suggests that personality is associated with musical
	career choice.},
  doi = {10.1080/14613801003746584},
  eprint = {http://www.tandfonline.com/doi/pdf/10.1080/14613801003746584},
  keywords = {perception psychological personal-differences},
  url = {http://www.tandfonline.com/doi/abs/10.1080/14613801003746584}
}

@INPROCEEDINGS{Wollmer2008,
  author = {W\"{o}llmer, Martin and Eyben, Florian and Reiter, Stephan and Schuller,
	Bj\"{o}rn and Cox, Cate and Douglas-Cowie, Ellen and Cowie, Roddy},
  title = {Abandoning emotion classes - towards continuous emotion recognition
	with modelling of long-range dependencies},
  booktitle = {Proceedings of the International Conference on Spoken Language Processing
	(Interspeech)},
  year = {2008},
  pages = {597--600},
  citeulike-article-id = {9595657},
  file = {:Wollmer2008.pdf:PDF},
  keywords = {affect, continuous classification-svm classification-neural classification-neural
	audio-features feature-generation multiannotator clip-statistics},
  location = {Brisbane, Australia},
  posted-at = {2011-07-30 11:18:39},
  priority = {2}
}

@INPROCEEDINGS{Wollmer2009,
  author = {W\"{o}llmer, Martin and Eyben, Florian and Schuller, Bj\"{o}rn and
	Douglas-Cowie, Ellen and Roddy Cowie},
  title = {Data-driven Clustering in Emotional Space for Affect Recognition
	Using Discriminatively Trained {LSTM} Networks},
  booktitle = {Proceedings of the International Conference on Spoken Language Processing
	(Interspeech)},
  year = {2009},
  series = {ISSN 1990-9772},
  pages = {1595-1598},
  address = {Brighton, UK},
  abstract = {In today’s affective databases speech turns are often labelled on
	a continuous scale for emotional dimensions such as valence or arousal
	to better express the diversity of human affect. However, applications
	like virtual agents usually map the detected emotional user sate
	to rough classes in order to reduce the multiplicity of emotion dependent
	system responses. Since these classes often do not optimally reflect
	emotions that typically occur in a given application, this paper
	investigates data-driven clustering of emotional space to find class
	divisions that better match the training data and the area of application.
	Thereby we consider the Belfast Sensitive Artificial Listener database
	and TV talkshow data from the VAM corpus. We show that a discriminatively
	trained Long Short-Term Memory (LSTM) recurrent neural net that explicitly
	learns clusters in emotional space and additionally models context
	information outperforms both, Support Vector Machines and a Regression-LSTM
	net.},
  file = {:Wollmer2009.pdf:PDF},
  keywords = {multiannotator corpus-Belfast-SAL corpus-vam taxonomy unsupervised
	classes classification-svm audio-features classification-neural feature-generation},
  owner = {ts00051},
  review = {Uses unsupervised clustering to form data driven labels},
  timestamp = {2010.01.15}
}

@INPROCEEDINGS{Wagstaff2007,
  author = {Wagstaff, K. L. and Lane},
  title = {Salience assignment for multiple-instance regression},
  booktitle = {Proceedings of the Workshop on Constrained Optimization and Structured
	Output Spaces},
  year = {2007},
  address = {Covallis, OR},
  abstract = {We present a Multiple-Instance Learning (MIL) algorithm for determining
	the salience of each item in each bag with respect to the bag's real-valued
	label. We use an alternating-projections constrained optimization
	approach to simultaneously learn a regression model and estimate
	all salience values. We evaluate this algorithm on a significant
	real-world problem, crop yield modeling, and demonstrate that it
	provides more extensive, intuitive, and stable salience models than
	Primary-Instance Regression, which selects a single relevant item
	from each bag.},
  keywords = {mil instance bag boost machine learning method kernel supervised regression-method},
  owner = {ts00051},
  timestamp = {2011.09.08}
}

@INPROCEEDINGS{Wagstaff2008,
  author = {Kiri L. Wagstaff and Terran Lane and Alex Roper},
  title = {Multiple-Instance Regression with Structured Data},
  booktitle = {Proceedings of the 4th International Workshop on Mining Complex Data},
  year = {2008},
  volume = {0},
  pages = {291-300},
  address = {Los Alamitos, CA, USA},
  month = {December},
  publisher = {IEEE Computer Society},
  doi = {http://doi.ieeecomputersociety.org/10.1109/ICDMW.2008.31},
  file = {Wagstaff2008.pdf:Wagstaff2008.pdf:PDF},
  isbn = {978-0-7695-3503-6},
  keywords = {mil instance bag regression supervised kernel method regression-method}
}

@INPROCEEDINGS{Wang2006,
  author = {Jun Wang and Lijun Yin and Xiaozhou Wei and Yi Sun},
  title = {3{D} facial expression recognition based on primitive surface feature
	distribution},
  booktitle = {Proceedings of the Conference on Computer Vision and Pattern Recognition},
  year = {2006},
  pages = {1399--1406},
  abstract = {The creation of facial range models by 3D imaging systems has led
	to extensive work on 3D face recognition [19]. However, little work
	has been done to study the usefulness of such data for recognizing
	and understanding facial expressions. Psychological research shows
	that the shape of a human face, a highly mobile facial surface, is
	critical to facial expression perception. In this paper, we investigate
	the importance and usefulness of 3D facial geometric shapes to represent
	and recognize facial expressions using 3D facial expression range
	data. We propose a novel approach to extract primitive 3D facial
	expression features, and then apply the feature distribution to classify
	the prototypic facial expressions. In order to validate our proposed
	approach, we have conducted experiments for person-independent facial
	expression recognition using our newly created 3D facial expression
	database. We also demonstrate the advantages of our 3D geometric
	based approach over 2D texture based approaches in terms of various
	head poses.},
  keywords = {labels-emotion feature-generation feature-3D feature-texture}
}

@INPROCEEDINGS{Wang2000,
  author = {Wang, Jun and Jean-Daniel Zucker and Jean-daniel Zucker},
  title = {Solving the Multiple-Instance Problem: A Lazy Learning Approach},
  booktitle = {Proceedings of the 17th International Conference on Machine Learning},
  year = {2000},
  pages = {1119--1125},
  publisher = {Morgan Kaufmann},
  abstract = {As opposed to traditional supervised learning, multiple-instance learning
	concerns the problem of classifying a bag of instances, given bags
	that are labeled by a teacher as being overall positive or negative.
	Current research mainly concentrates on adapting traditional concept
	learning to solve this problem. In this paper we investigate the
	use of lazy learning and Hausdorff distance to approach the multiple-instance
	problem. We present two variants of the K-nearest neighbor algorithm,
	called Bayesian-KNN and Citation-KNN, solving the multiple-instance
	problem. Experiments on the Drug discovery benchmark data show that
	both algorithms are competitive with the best ones conceived in the
	concept learning framework. Further work includes exploring of a
	combination of lazy and eager multiple-instance problem classifiers.},
  file = {Wang2000.pdf:Wang2000.pdf:PDF},
  keywords = {Citation, kNN, Citation-kNN supervised learning mil machine method
	classification-method},
  owner = {ts00051},
  timestamp = {2010.03.04}
}

@OTHER{Weisstein2008,
  author = {Weisstein, Eric W.},
  comment = {http://mathworld.wolfram.com/CorrelationCoefficient.html},
  keywords = {pearson},
  lastchecked = {Aug 22, 2008},
  note = {From MathWorld--A Wolfram Web Resource. http://mathworld.wolfram.com/CorrelationCoefficient.html},
  title = {Correlation Coefficient},
  url = {\url{http://mathworld.wolfram.com/CorrelationCoefficient.html}}
}

@ARTICLE{Welch1947,
  author = {Welch, B. L.},
  title = {The generalization of "Student’s” problem when several different
	population variances are involved},
  journal = {Biometrika},
  year = {1947},
  volume = {34},
  pages = {28–35},
  owner = {tim},
  timestamp = {2012.10.23}
}

@ARTICLE{White1989,
  author = {White, Sheida},
  title = {Backchannels across Cultures: A Study of {Americans} and {Japanese}},
  journal = {Language in Society},
  year = {1989},
  volume = {18},
  pages = {59--76},
  number = {1},
  abstract = {The frequency of listener responses, called backchannels, was studied
	in English conversations within and across two cultural groups: Americans
	from the midwestern United States and Japanese who were born and
	raised in Japan. The findings reveal that backchannels of several
	types are displayed far more frequently by Japanese listeners. This
	appears to be related to the greater use by Japanese of certain discourse
	constructions that favor backchannels, and to the Japanese culture.
	The Japanese listening style remains unchanged in cross-cultural
	conversations, but Americans alter listening style in the direction
	of their nonnative interlocutors. The study found no evidence for
	the hypothesis that backchanneling conventions that are not shared
	contribute to negative personality attributions or stereotyping.},
  copyright = {Copyright © 1989 Cambridge University Press},
  issn = {00474045},
  jstor_articletype = {primary_article},
  jstor_formatteddate = {Mar., 1989},
  keywords = {nvc back-channel cross-cultural mutual backchannel},
  publisher = {Cambridge University Press},
  url = {http://www.jstor.org/stable/4168001}
}

@ARTICLE{Wiener1972,
  author = {Wiener, M. and Devoe, S. and Rubinow, S., and Geller, J.},
  title = {Nonverbal behavior and nonverbal communication},
  journal = {Psychological Review},
  year = {1972},
  volume = {79},
  pages = {185-21},
  number = {4},
  owner = {tim},
  timestamp = {2013.03.30}
}

@ARTICLE{Wolpert1996,
  author = {Wolpert, David},
  title = {The Lack of A Priori Distinctions between Learning Algorithms},
  journal = {Neural Computation},
  year = {1996},
  pages = {1341-1390},
  keywords = {machine learning tools},
  owner = {ts00051},
  timestamp = {2012.02.10}
}

@TECHREPORT{Wolpert1995,
  author = {Wolpert, D.H. and Macready, W.G.},
  title = {No Free Lunch Theorems for Search},
  institution = {Santa Fe Institute},
  year = {1995},
  number = {Technical Report SFI-TR-95-02-010},
  owner = {ts00051},
  timestamp = {2012.02.22}
}

@ARTICLE{Tingfan2012,
  author = {Tingfan Wu and Butko, N.J. and Ruvolo, P. and Whitehill, J. and Bartlett,
	M.S. and Movellan, J.R.},
  title = {Multilayer Architectures for Facial Action Unit Recognition},
  journal = {Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions
	on},
  year = {2012},
  volume = {42},
  pages = {1027 -1038},
  number = {4},
  month = {aug. },
  abstract = {In expression recognition and many other computer vision applications,
	the recognition performance is greatly improved by adding a layer
	of nonlinear texture filters between the raw input pixels and the
	classifier. The function of this layer is typically known as feature
	extraction. Popular filter types for this layer are Gabor energy
	filters (GEFs) and local binary patterns (LBPs). Recent work [1]
	suggests that adding a second layer of nonlinear filters on top of
	the first layer may be beneficial. However, it is unclear what is
	the best architecture of layers and selection of filters. In this
	paper, we present a thorough empirical analysis of the performance
	of single-layer and dual-layer texture-based approaches for action
	unit recognition. For the single hidden layer case, GEFs perform
	consistently better than LBPs, which may be due to their robustness
	to jitter and illumination noise as well as to their ability to encode
	texture at multiple resolutions. For dual-layer case, we confirm
	that, while small, the benefit of adding this second layer is reliable
	and consistent across data sets. Interestingly for this second layer,
	LBPs appear to perform better than GEFs.},
  doi = {10.1109/TSMCB.2012.2195170},
  issn = {1083-4419},
  keywords = {GEF;Gabor energy filters;LBPs;action unit recognition;computer vision
	applications;dual layer texture based approaches;expression recognition;facial
	action unit recognition;feature extraction;illumination noise;local
	binary patterns;multilayer architectures;nonlinear texture filters;raw
	input pixels;recognition performance;single hidden layer;single layer
	texture based approaches;Gabor filters;computer vision;face recognition;feature
	extraction;image motion analysis;image texture;}
}

@INPROCEEDINGS{Xiangmin2007,
  author = {Xiang-min, Xu and Mao Yun-feng and Xiong Jia-ni and Zhou Feng-le},
  title = {Classification Performance Comparison between RVM and SVM},
  booktitle = {Anti-counterfeiting, Security, Identification, 2007 IEEE International
	Workshop on},
  year = {2007},
  pages = {208 -211},
  month = {april},
  abstract = {Both relevant vector machine and support vector machine are newly
	promoted pattern recognition algorithms. An extensive relevant literature
	displays that they have become hot topics in the field of machine
	learning. Due to the difference of their mechanism, little research
	is done to compare their performance. This paper experimentally compared
	several features of RVM with SVM which can characterize the classification
	performance on the basis of deeply understanding their algorithms.
	The results show that RVM is almost equal to SVM on training efficiency
	and classification accuracy, but as to sparse property, generalization
	ability and decision speed, RVM performs better. So it is recommended
	to study RVM deeply and extend its application areas further.},
  doi = {10.1109/IWASID.2007.373728},
  keywords = {RVM;SVM;classification performance comparison;pattern recognition
	algorithms;relevant vector machine;support vector machine;pattern
	recognition;support vector machines;}
}

@ARTICLE{Xiao2003,
  author = {Xiao, Jing and Tsuyoshi Moriyama and Takeo Kanade and Jeffrey Cohn},
  title = {Robust Full-Motion Recovery of Head by Dynamic Templates and Re-registration
	Techniques},
  journal = {International Journal of Imaging Systems and Technology},
  year = {2003},
  volume = {13},
  pages = {85-94},
  month = {September},
  abstract = {This paper presents a method to recover the full-motion (3 rotations
	and 3 translations) of the head from an input video using a cylindrical
	head model. Given an initial reference template of the head image
	and the corresponding head pose, the head model is created and full
	head motion is recovered automatically. The robustness of the approach
	is achieved by a combination of three techniques. First, we use the
	iteratively re-weighted least squares (IRLS) technique in conjunction
	with the image gradient to accommodate non-rigid motion and occlusion.
	Second, while tracking, the templates are dynamically updated to
	diminish the effects of self-occlusion and gradual lighting changes
	and to maintain accurate tracking even when the face moves out of
	view of the camera. Third, to minimize error accumulation inherent
	in the use of dynamic templates, we re-register images to a reference
	template whenever head pose is close to that in the template. The
	performance of the method, which runs in real time, was evaluated
	in three separate experiments using image sequences (both synthetic
	and real) for which ground truth head motion was known. The real
	sequences included pitch and yaw as large as 40?and 75? respectively.
	The average recovery accuracy of the 3D rotations was about 3? In
	a further test, the method was used as part of a facial expression
	analysis system intended for use with spontaneous facial behavior
	in which moderate head motion is common. Image data consisted of
	1-minute of video from each of 10 subjects while engaged in a 2-person
	interview. The method successfully stabilized face and eye images
	allowing for 98% accuracy in automatic blink recognition.},
  keywords = {head-pose head-model},
  owner = {ts00051},
  timestamp = {2009.10.26}
}

@INCOLLECTION{Yang2009b,
  author = {Yang, Jimei and Liao, Shengcai and Li, Stan Z.},
  title = {Automatic Partial Face Alignment in NIR Video Sequences},
  booktitle = {Advances in Biometrics},
  publisher = {Springer Berlin Heidelberg},
  year = {2009},
  editor = {Tistarelli, Massimo and Nixon, MarkS.},
  volume = {5558},
  series = {Lecture Notes in Computer Science},
  pages = {249-258},
  doi = {10.1007/978-3-642-01793-3_26},
  isbn = {978-3-642-01792-6},
  keywords = {Face Alignment; Partial Faces; SIFT; MBGC},
  url = {http://dx.doi.org/10.1007/978-3-642-01793-3_26}
}

@INPROCEEDINGS{Yang2009c,
  author = {Peng Yang and Qingshan Liu and Metaxas, D.N.},
  title = {RankBoost with l1 regularization for facial expression recognition
	and intensity estimation},
  booktitle = {Computer Vision, 2009 IEEE 12th International Conference on},
  year = {2009},
  pages = {1018 -1025},
  month = {29 2009-oct. 2},
  abstract = {Most previous facial expression analysis works only focused on expression
	recognition. In this paper, we propose a novel framework of facial
	expression analysis based on the ranking model. Different from previous
	works, it not only can do facial expression recognition, but also
	can estimate the intensity of facial expression, which is very important
	to further understand human emotion. Although it is hard to label
	expression intensity quantitatively, the ordinal relationship in
	temporal domain is actually a good relative measurement. Based on
	this observation, we convert the problem of intensity estimation
	to a ranking problem, which is modeled by the RankBoost. The output
	ranking score can be directly used for intensity estimation, and
	we also extend the ranking function for expression recognition. To
	further improve the performance, we propose to introduce l1 based
	regularization into the Rankboost. Experiments on the Cohn-Kanade
	database show that the proposed method has a promising performance
	compared to the state-of-the-art.},
  doi = {10.1109/ICCV.2009.5459371},
  issn = {1550-5499},
  keywords = {Cohn-Kanade database;RankBoost;facial expression intensity estimation;facial
	expression recognition;human emotion;ranking model;emotion recognition;face
	recognition;}
}

@INPROCEEDINGS{Yang2011,
  author = {Yang, Songfan and Bhanu, B.},
  title = {Facial expression recognition using emotion avatar image},
  booktitle = {Proceedings of the International Conference on Automatic Face \&
	Gesture Recognition and Workshops},
  year = {2011},
  pages = {866 - 871},
  address = {Santa Barbara, CA},
  file = {:Yang2011.pdf:PDF},
  keywords = {FERA2011 corpus-GEMEP-FERA single-frame exemplar temporal avatar person-normalisation
	texture-feature posed-data classification-svm temporal},
  owner = {tim},
  timestamp = {2011.11.13}
}

@ARTICLE{Yang2009,
  author = {Yong Yang and Guoyin Wang and Hao Kong},
  title = {Self-Learning Facial Emotional Feature Selection Based on Rough Set
	Theory},
  journal = {Mathematical Problems in Engineering},
  year = {2009},
  note = {Article ID 802932},
  abstract = {Emotion recognition is very important for human-computer intelligent
	interaction. It is generally performed on facial or audio information
	by artificial neural network, fuzzy set, support vector machine,
	hidden Markov model, and so forth. Although some progress has already
	been made in emotion recognition, several unsolved issues still exist.
	For example, it is still an open problem which features are the most
	important for emotion recognition. It is a subject that was seldom
	studied in computer science. However, related research works have
	been conducted in cognitive psychology. In this paper, feature selection
	for facial emotion recognition is studied based on rough set theory.
	A self-learning attribute reduction algorithm is proposed based on
	rough set and domain oriented data-driven data mining theory. Experimental
	results show that important and useful features for emotion recognition
	can be identified by the proposed method with a high recognition
	rate. It is found that the features concerning mouth are the most
	important ones in geometrical features for facial emotion recognition.},
  doi = {10.1155/2009/802932},
  file = {Yang2009.pdf:Yang2009.pdf:PDF},
  keywords = {feature-generation features-important feature-selection tracker-features
	heuristic geometric corpus-CKACFE corpus-jaffe corpus-CQUPT},
  owner = {ts00051},
  timestamp = {2009.10.26}
}

@BOOK{Yarbus1967,
  title = {Eye Movement and Vision},
  publisher = {Plenum Press, New York},
  year = {1967},
  author = {Yarbus, A. L.},
  note = {Translated by Basil Haigh},
  keywords = {gaze},
  review = {Influencial early book on eye motion}
}

@INPROCEEDINGS{Yngve1970,
  author = {Yngve, V},
  title = {On getting a word in edgeways},
  booktitle = {Papers from the Sixth Regional Meeting of the Chicago Linguistic
	Society},
  year = {1970},
  address = {Chicago},
  publisher = {Chicago Linguistic Society},
  abstract = {First used of the term "backchannel" in linguistics(?)},
  keywords = {unclassified},
  owner = {ts00051},
  timestamp = {2009.11.18}
}

@ARTICLE{Zara2007,
  author = {Zara, A and Maffiolo, V and Martin, J and Devillers, L},
  title = {Collection and annotation of a corpus of human-human multimodal interactions:
	Emotion and others anthropomorphic characteristics},
  journal = {Affective Computing and Intelligent Interaction},
  year = {2007},
  pages = {464--475},
  abstract = {In order to design affective interactive systems, experimental grounding
	is required for studying expressions of emotion during interaction.
	In this paper, we present the EmoTaboo protocol for the collection
	of multimodal emotional behaviours occurring during human-human interactions
	in a game context. First annotations revealed that the collected
	data contains various multimodal expressions of emotions and other
	mental states. In order to reduce the influence of language via a
	predetermined set of labels and to take into account differences
	between coders in their capacity to verbalize their perception, we
	introduce a new annotation methodology based on 1) a hierarchical
	taxonomy of emotion-related words, and 2) the design of the annotation
	interface. Future directions include the implementation of such an
	annotation tool and its evaluation for the annotation of multimodal
	interactive and emotional behaviours. We will also extend our first
	annotation scheme to several other characteristics interdependent
	of emotions. Springer-Verlag Berlin Heidelberg 2007.},
  file = {:Zara2007.pdf:PDF},
  keywords = {emotaboo, nvc-annotation, nvc, situation-game labels-meaning labels-emotion
	corpus-emotaboo corpus-announce emotion-taxonomy natural-data},
  publisher = {Springer},
  url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-38049034874&partnerID=40&md5=6aa5847e4756c1a1340b05f75aacfaa7}
}

@ARTICLE{Zeng2006,
  author = {Zeng, Z. and Fu, Y. and Roisman, G.I. and Wen, Z. and Hu, Y. and
	Huang, T.S.},
  title = {Spontaneous Emotional Facial Expression Detection},
  journal = {Journal of Multimedia},
  year = {2006},
  volume = {1},
  pages = {1-8},
  number = {5},
  abstract = {Change in a speaker’s emotion is a fundamental component in human
	communication. Automatic recognition of spontaneous emotion would
	significantly impact human-computer interaction and emotion-related
	studies in education, psychology and psychiatry. In this paper, we
	explore methods for detecting emotional facial expressions occurring
	in a realistic human conversation setting—the Adult Attachment Interview
	(AAI). Because non-emotional facial expressions have no distinct
	description and are expensive to model, we treat emotional facial
	expression detection as a one-class classification problem, which
	is to describe target objects (i.e., emotional facial expressions)
	and distinguish them from outliers (i.e., non-emotional ones). Our
	preliminary experiments on AAI data suggest that one-class classification
	methods can reach a good balance between cost (labeling and computing)
	and recognition performance by avoiding non-emotional expression
	labeling and modeling.},
  file = {:Zeng2006.pdf:PDF},
  keywords = {situation-interview labels-emotion head-model alignment texture-feature
	classification-svdd},
  owner = {ts00051},
  timestamp = {2009.10.26}
}

@ARTICLE{Zeng2009,
  author = {Zhihong Zeng and Pantic, M. and Roisman, G.I. and Huang, T.S.},
  title = {A Survey of Affect Recognition Methods: Audio, Visual, and Spontaneous
	Expressions},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2009},
  volume = {31},
  pages = {39-58},
  number = {1},
  month = {jan. },
  abstract = {Automated analysis of human affective behavior has attracted increasing
	attention from researchers in psychology, computer science, linguistics,
	neuroscience, and related disciplines. However, the existing methods
	typically handle only deliberately displayed and exaggerated expressions
	of prototypical emotions despite the fact that deliberate behaviour
	differs in visual appearance, audio profile, and timing from spontaneously
	occurring behaviour. To address this problem, efforts to develop
	algorithms that can process naturally occurring human affective behaviour
	have recently emerged. Moreover, an increasing number of efforts
	are reported toward multimodal fusion for human affect analysis including
	audiovisual fusion, linguistic and paralinguistic fusion, and multi-cue
	visual fusion based on facial expressions, head movements, and body
	gestures. This paper introduces and surveys these recent advances.
	We first discuss human emotion perception from a psychological perspective.
	Next we examine available approaches to solving the problem of machine
	understanding of human affective behavior, and discuss important
	issues like the collection and availability of training and test
	data. We finally outline some of the scientific and engineering challenges
	to advancing human affect sensing technology.},
  doi = {10.1109/TPAMI.2008.52},
  issn = {0162-8828},
  keywords = {affect recognition methods;audio expressions;human affect analysis;human
	affective behavior;human emotion perception;multimodal fusion;spontaneous
	expressions;visual expressions;behavioural sciences;emotion recognition;human
	factors;psychology;Affect;Algorithms;Artificial Intelligence;Emotions;Facial
	Expression;Monitoring, Physiologic;Pattern Recognition, Automated;Sound
	Spectrography;}
}

@ARTICLE{Zhang2001,
  author = {Zhang, Q. and Goldman, S. A.},
  title = {{EM}-{DD}: An improved multiple-instance learning technique},
  journal = {Neural Information Processing Systems (NIPS)},
  year = {2001},
  volume = {14},
  file = {Zhang2001.ps:Zhang2001.ps:PostScript},
  keywords = {mil expectation-maximization supervised classification machine learning
	classification-method},
  owner = {ts00051},
  timestamp = {2010.03.04}
}

@INPROCEEDINGS{Zhang2004,
  author = {Wenchao Zhang and Shiguang Shan and Wen Gao and Yizheng Chang and
	Bo Cao and Peng Yang},
  title = {Information fusion in face identification},
  booktitle = {Pattern Recognition, 2004. ICPR 2004. Proceedings of the 17th International
	Conference on},
  year = {2004},
  volume = {3},
  pages = { 950 - 953 Vol.3},
  month = {aug.},
  abstract = { Information fusion of multi-modal biometrics has attracted much attention
	in recent years. However, this paper focuses on the information fusion
	in single models, that is, the face biometric. Two different representation
	methods, gray level intensity and Gabor feature, are exploited for
	fusion. We study the fusion problem in face recognition at both the
	face representation level and the confidence level. At the representation
	level, both the PCA feature fusion and the LDA feature fusion are
	considered, while at the confidence level, the sum rule and the product
	rule are investigated. We show through experiments on FERET face
	database and our own face database that appropriate information fusion
	can improve the performance of face recognition and verification.
	This suggests that gray level intensity and Gabor feature compensate
	for each other, based on the feasible fusion.},
  doi = {10.1109/ICPR.2004.1334686},
  issn = {1051-4651 },
  keywords = { FERET face database; Gabor feature method; PCA feature fusion; face
	biometrics; face identification; face recognition; face representation
	level; face verification; gray level intensity method; information
	fusion; linear discriminant analysis feature fusion; multimodal biometrics;
	biometrics (access control); face recognition; feature extraction;
	image representation; principal component analysis; visual databases;}
}

@ARTICLE{Zhou2004,
  author = {Zhou, Shaohua K. and Chellappa, R. and Moghaddam, B.},
  title = {Visual tracking and recognition using appearance-adaptive models
	in particle filters},
  journal = {IEEE Transactions on Image Processing},
  year = {2004},
  volume = {13},
  pages = {1491--1506},
  number = {11},
  abstract = {We present an approach that incorporates appearance-adaptive models
	in a particle filter to realize robust visual tracking and recognition
	algorithms. Tracking needs modeling interframe motion and appearance
	changes, whereas recognition needs modeling appearance changes between
	frames and gallery images. In conventional tracking algorithms, the
	appearance model is either fixed or rapidly changing, and the motion
	model is simply a random walk with fixed noise variance. Also, the
	number of particles is typically fixed. All these factors make the
	visual tracker unstable. To stabilize the tracker, we propose the
	following modifications: an observation model arising from an adaptive
	appearance model, an adaptive velocity motion model with adaptive
	noise variance, and an adaptive number of particles. The adaptive-velocity
	model is derived using a first-order linear predictor based on the
	appearance difference between the incoming observation and the previous
	particle configuration. Occlusion analysis is implemented using robust
	statistics. Experimental results on tracking visual objects in long
	outdoor and indoor video sequences demonstrate the effectiveness
	and robustness of our tracking algorithm. We then perform simultaneous
	tracking and recognition by embedding them in a particle filter.
	For recognition purposes, we model the appearance changes between
	frames and gallery images by constructing the intra- and extrapersonal
	spaces. Accurate recognition is achieved when confronted by pose
	and view variations.},
  citeulike-article-id = {1596389},
  keywords = {deterministic, probabilistic, tracking},
  owner = {ts00051},
  posted-at = {2007-08-27 14:35:22},
  priority = {0},
  timestamp = {2009.10.26},
  urlzzz = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1344039}
}

@INPROCEEDINGS{Zhu2009b,
  author = {Jianke Zhu and Luc Van Gool and Hoi, S.C.H.},
  title = {Unsupervised face alignment by robust nonrigid mapping},
  booktitle = {Computer Vision, 2009 IEEE 12th International Conference on},
  year = {2009},
  pages = {1265 -1272},
  month = {29 2009-oct. 2},
  abstract = {We propose a novel approach to unsupervised facial image alignment.
	Differently from previous approaches, that are confined to affine
	transformations on either the entire face or separate patches, we
	extract a nonrigid mapping between facial images. Based on a regularized
	face model, we frame unsupervised face alignment into the Lucas-Kanade
	image registration approach. We propose a robust optimization scheme
	to handle appearance variations. The method is fully automatic and
	can cope with pose variations and expressions, all in an unsupervised
	manner. Experiments on a large set of images showed that the approach
	is effective.},
  doi = {10.1109/ICCV.2009.5459325},
  issn = {1550-5499},
  keywords = {Lucas-Kanade image registration;affine transformations;robust nonrigid
	mapping;robust optimization scheme;unsupervised facial image alignment;face
	recognition;feature extraction;image registration;transforms;}
}

@ARTICLE{Zhu2006,
  author = {Ji Zhu and Ji Zhu and Saharon Rosset and Hui Zou and Trevor Hastie},
  title = {Multi-class {AdaBoost}},
  year = {2005},
  abstract = {Boosting has been a very successful technique for solving the two-class
	classification problem.In going from two-class to multi-class classification,
	most algorithms have been restricted toreducing the multi-class classification
	problem to multiple two-class problems. In this paper, we propose
	a new algorithm that naturally extends the original AdaBoost algorithm
	to the multiclass case without reducing it to multiple two-class
	problems. Similar to AdaBoost in the twoclass case, this new algorithm
	combines weak classifiers and only requires the performance of each
	weak classifier be better than random guessing (rather than 1/2).
	We further provide a statistical justification for the new algorithm
	using a novel multi-class exponential loss function and forward stage-wise
	additive modeling. As shown in the paper, the new algorithm is extremely
	easy to implement and is highly competitive with the best currently
	available multi-class classification methods.},
  keywords = {Boosting, Exponential loss, Multi-class classification, Stagewise
	modeling machine learning classification-method boost},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.62.512}
}

@INPROCEEDINGS{Zhu2012,
  author = {Xiangxin Zhu and Ramanan, D.},
  title = {Face detection, pose estimation, and landmark localization in the
	wild},
  booktitle = {Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference
	on},
  year = {2012},
  pages = {2879 -2886},
  month = {june},
  abstract = {We present a unified model for face detection, pose estimation, and
	landmark estimation in real-world, cluttered images. Our model is
	based on a mixtures of trees with a shared pool of parts; we model
	every facial landmark as a part and use global mixtures to capture
	topological changes due to viewpoint. We show that tree-structured
	models are surprisingly effective at capturing global elastic deformation,
	while being easy to optimize unlike dense graph structures. We present
	extensive results on standard face benchmarks, as well as a new #x201C;in
	the wild #x201D; annotated dataset, that suggests our system advances
	the state-of-the-art, sometimes considerably, for all three tasks.
	Though our model is modestly trained with hundreds of faces, it compares
	favorably to commercial systems trained with billions of examples
	(such as Google Picasa and face.com).},
  doi = {10.1109/CVPR.2012.6248014},
  issn = {1063-6919},
  keywords = {Google Picasa;computer vision;face detection;face.com;global elastic
	deformation;in-the-wild annotated dataset;landmark estimation;landmark
	localization;pose estimation;topological changes;tree-structured
	models;computer vision;face recognition;object detection;pose estimation;trees
	(mathematics);}
}

@INPROCEEDINGS{Zhu2009,
  author = {Zhu, Yunfeng and Fernando De la Torre and Jeffrey F. Cohn and Yu-Jin
	Zhang},
  title = {Dynamic Cascades with Bidirectional Bootstrapping for Spontaneous
	Facial Action Unit Detection},
  booktitle = {Proceedings of the International Conference on Affective Computing
	\& Intelligent Interaction (ACII)},
  year = {2009},
  address = {Amsterdam},
  month = {Sept},
  abstract = {A relatively unexplored problem in facial expression analysis is how
	to select the positive and negative samples with which to train classifiers
	for expression recognition. Typically, for each action unit (AU)
	or other expression, the peak frames are selected as positive class
	and the negative samples are selected from other AUs. This approach
	suffers from at least two drawbacks. One, because many state of the
	art classifiers, such as Support Vector Machines (SVMs), fail to
	scale well with increases in the number of training samples (e.g.
	for the worse case in SVM), it may be infeasible to use all potential
	training data. Two, it often is unclear how best to choose the positive
	and negative samples. If we only label the peaks as positive samples,
	a large imbalance will result between positive and negative samples,
	especially for infrequent AU. On the other hand, if all frames from
	onset to offset are labeled as positive, many may differ minimally
	or not at all from the negative class. Frames near onsets and offsets
	often differ little from those that precede them. In this paper,
	we propose Dynamic Cascades with Bidirectional Bootstrapping (DCBB)
	to address these issues. DCBB optimally selects positive and negativeclass
	samples in training sets. In experimental evaluations in non-posed
	video from the RU-FACS Database, DCBB yielded improved performance
	for action unit recognition relative to alternative approaches.},
  file = {:Zhu2009.pdf:PDF},
  keywords = {classification-svm labels-facs amm-features face alignment texture-feature},
  owner = {ts00051},
  review = {Extension to boosting to choose subset of training data to produce
	optimal classifier using bootstrapping. Used on AU detection on posed
	data.}
}

@INPROCEEDINGS{Zhu2004,
  author = {Zhu, Zhiwei and Ji, Qiang},
  title = {Real Time 3D Face Pose Tracking From an Uncalibrated Camera},
  booktitle = {Proceedings of the First IEEE Workshop on Face Processing in Video},
  year = {2004},
  address = {Washington, DC},
  month = {June},
  abstract = {We propose a new near-real time technique for 3D face pose tracking
	from a monocular image sequence obtained from an uncalibrated camera.
	The basic idea behind our approach is that instead of treating 2D
	face detection and 3D face pose estimation separately, we perform
	simultaneous 2D face detection and 3D face pose tracking. Specifically,
	3D face pose at a time instant is constrained by the face dynamics
	using Kalman Filtering and by the face appearance in the image. The
	use of Kalman Filtering limits possible 3D face poses to a small
	range while the best matching between the actual face image and the
	projected face image allows to pinpoint the exact 3D face pose. Face
	matching is formulated as an optimization problem so that the exact
	face location and 3D face pose can be estimated e#ciently. Another
	major feature of our approach lies in the use of active IR illumination,
	which allows to robustly detect eyes. The detected eyes can in turn
	constrain the face in the image and regularize the 3D face pose,
	therefore the tracking drift issue can be avoided and the processing
	can speedup. Finally, the face model is dynamically updated to account
	for variations in face appearances caused by face pose, face expression,
	illumination and the combination of them. Compared with},
  keywords = {head-pose estimation},
  review = {Use of active IR sensor to locate eye positions and to use this to
	contrain the search for face pose. Pose estimate is combined with
	previous frame pose using a Kalman filter. Approach is robust to
	lighting changes, expression.}
}

@ARTICLE{Zijlstra2011,
  author = {Zijlstra, Wobbe P. and van der Ark, L. Andries and Sijtsma, Klaas},
  title = {Outliers in Questionnaire Data: Can They Be Detected and Should They
	Be Removed?},
  journal = {Journal of Educational and Behavioral Statistics},
  year = {2011},
  volume = {36},
  pages = {186-212},
  number = {2},
  abstract = {Outliers in questionnaire data are unusual observations, which may
	bias statistical results, and outlier statistics may be used to detect
	such outliers. The authors investigated the effect outliers have
	on the specificity and the sensitivity of each of six different outlier
	statistics. The Mahalanobis distance and the item-pair based outlier
	statistics were found to have the best combination of specificity
	and sensitivity. Next, it was investigated how outliers influenced
	the bias in the percentile rank score, Cronbach’s alpha, and the
	validity coefficient. Outliers due to random responding and faking
	produced considerable bias, and outliers due to extreme responding
	produced little bias. Finally, the influence of removing discordant
	observations on bias was studied. Removing observations due to random
	responding identified by means of the Mahalanobis distance, the local
	outlier factor, and the item-pair based outlier statistic reduced
	bias.},
  doi = {10.3102/1076998610366263},
  eprint = {http://jeb.sagepub.com/content/36/2/186.full.pdf+html},
  url = {http://jeb.sagepub.com/content/36/2/186.abstract}
}

@BOOK{Headland1990,
  title = {Emics and Etics: the insider/outsider debate},
  publisher = {Sage Publications},
  year = {1990},
  editor = {Thomas N. Headland and Kenneth L. Pike and Marvin Harris},
  address = {Newbury Park, CA},
  abstract = {The concepts of emics and etics have spread throughout the social
	sciences and humanities since their invention by linguist Kenneth
	Pike in the 1950s. Referring to insider versus outsider, subjective
	versus objective views of the world, these concepts are vital for
	researchers dealing with cultures other than their own.
	
	
	Pike uses this volume as a forum to explain the development of emics
	and etics and their usage today. He is joined in the debate by renowned
	anthropologist Marvin Harris, founder of the school of cultural materialism.
	Eight other scholars add to the scholarly discourse and demonstrate
	applications of the concepts in a variety of disciplines.},
  keywords = {culture, translation, cross-cultural},
  owner = {tim},
  timestamp = {2011.07.08}
}

@BOOK{Poyatos1997,
  title = {Nonverbal Communication and Translation},
  publisher = {Amsterdam/Philadelphia, John Benjamins},
  year = {1997},
  editor = {Poyatos, Fernando},
  abstract = {This is the first book, within the interdisciplinary field of Nonverbal
	Communication Studies, dealing with the specific tasks and problems
	involved in the translation of literary works as well as film and
	television texts, and in the live experience of simultaneous and
	consecutive interpretation. The theoretical and methodological ideas
	and models it contains should merit the interest not only of students
	of literature, professional translators and translatologists, interpreters,
	and those engaged in film and television dubbing, but also to literary
	readers, film and theatergoers, linguists and psycholinguists, semioticians,
	communicologists, and crosscultural anthropologists. Its sixteen
	contributions by translation scholars and professional interpreters
	from fifteen countries, deal with discourse in translation, intercultural
	problems, narrative literature, theater, poetry, interpretation,
	and film and television dubbing.},
  keywords = {translation nvc cross-cultural},
  owner = {tim},
  timestamp = {2011.07.07}
}

@OTHER{amiproject,
  note = {http://corpus.amiproject.org/},
  owner = {tim},
  timestamp = {2013.06.02},
  title = {{AMI} project website},
  url = {http://corpus.amiproject.org/}
}

@MANUAL{EmotionML,
  title = {Emotion Markup Language ({EmotionML}) 1.0},
  owner = {tim},
  timestamp = {2011.07.07},
  url = {http://www.w3.org/TR/emotionml/}
}

@OTHER{merriamwebster,
  note = {http://www.merriam-webster.com},
  owner = {tim},
  timestamp = {2013.06.02},
  title = {{Merriam} {Webster} {Dictionary}},
  url = {http://www.merriam-webster.com}
}

@MISC{Noproblem,
  title = {Translating the 17th of May into English and other horror stories},
  keywords = {translation cross-cultural instruments},
  owner = {tim},
  timestamp = {2011.07.07},
  url = {http://www.noproblem.no/translate.html}
}

@ARTICLE{Asimo2013,
  title = {Human gestures perplex {Asimo}, {Honda} museum robot guide},
  journal = {BBC},
  year = {2013},
  month = {5th July},
  note = {http://www.bbc.co.uk/news/technology-23196867},
  owner = {tim},
  quality = {1},
  timestamp = {2013.07.07}
}

@ARTICLE{Economist2001,
  title = {The triumph of {English}},
  journal = {The Economist},
  year = {2001},
  month = {20th Dec},
  keywords = {translation instruments},
  owner = {ts00051},
  timestamp = {2012.01.13}
}

