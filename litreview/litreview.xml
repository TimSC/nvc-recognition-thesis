<chapter title="Research Context of This Work" label="ChapterLiteratureReview" label2="BackgroundFramingClassificationProblem" label3="BackgroundHowIsNvcExpressed" label4="BackgroundCompareContrastEmotionWithNvc">
<quote>
To effectively communicate, we must realize that we are all different in the way we perceive the world and use this understanding as a guide to our communication with others.<br/>
Tony Robbins
</quote>

<p>One of the central themes of this thesis is <ac full='yes'>NVC</ac> and, for clarity, it may be useful to define this term and compare it to the related concept of emotion. <ac>NVC</ac> is the process of communicating by means other than words <cite ref='Knapp2009'/>. However, Knapp and Hall <cite ref='Knapp2009'/> argued that it is difficult to separate verbal from non-verbal behaviour and to determine if the expression “by means other than words” refers to the way <ac>NVC</ac> was expressed, or the interpretation that is assigned by an observer. They did say that the broad but inexact definition will serve, as long as these issues are understood and appreciated. For the purposes of this thesis, communication is considered as an intentional <cite ref='Jandt2004, Pearce1987'/> action of exchanging thoughts, opinions or information (which contrasts with the view that communication can be expressed with or without intent). This perspective of <ac>NVC</ac> is similar to Ekman and Friesen's definition of “communicative non-verbal behaviour” which they define as “those acts which are clearly and consciously intended by the sender to transmit a specifiable message to the receiver” <cite ref='Ekman1969'/>. The exact manner in which an <ac>NVC</ac> message is communicated can be carefully planned, or performed without conscious awareness <cite ref='Knapp2009, Lehtonen1981'/>. This means that some <ac>NVC</ac> signals are intentionally expressed while the expresser is unconscious of the exact mechanism by which they are expressed. <ac>NVC</ac> can also be defined in terms of behavioural preferences of the communicators themselves, which relates to the use of gestures, posture, touching behaviour (haptically), facial expressions, eye behaviour and vocal behaviour <cite ref='Knapp2009'/>.</p>

<p><ac>NVC</ac> is important for understanding the meaning of communication, because the verbal component is often complemented or augmented by <ac>NVC</ac> <cite ref='Archer1977, Knapp1978'/>. Non-verbal signals are often used to regulate a conversation, including turn taking <cite ref='Kendon1967, Avons1989'/>. It may be helpful to clarify the terms “vocal” and “verbal”. Verbal means to be or pertain to words, while vocal means to be or to pertain to voice. There can therefore be spoken <ac>NVC</ac> signals, which includes prosody and various non-word utterances such as gasping, groaning, laughing and sighing. The non-verbal component of the voice is also referred to as “paralanguage”. The audible part of <ac>NVC</ac> is not explored in this thesis, with the focus being on visual, and specifically facial <ac>NVC</ac> signals.</p>

<figure label="FigureEkmanNvb" short="The relationships between various types of non-verbal behaviours" title="A set diagram showing the relationships between various types of non-verbal behaviour, adapted from Ekman and Friesen &lt;cite ref='Ekman1969'/&gt;, p. 57. This thesis aims to address communicative non-verbal behaviour, however an annotation's perception of communicative non-verbal behaviour is limited to subsets d and f.">
<graphic>litreview/EkmanNvb.pdf</graphic>
</figure>

<p>This thesis treats non-verbal communication and non-verbal behaviour as distinct concepts (similar to <cite ref='Krauss1996, Hall2012'/>), although these terms are sometimes used interchangeably <cite ref='Negi2009'/>. Non-verbal behaviours that convey information, called “informative nonverbal behaviours” by Ekman and Friesen <cite ref='Ekman1969'/>, are defined as acts that “elicit similar interpretations among some set of observers”. Informative nonverbal behaviour includes both intentional and unintentional transfer of information. A separate issue is if a particular behaviour is communicative, in the sense that the behaviour was intended to be expressed by the sender to convey a particular message. As previously mentioned, Ekman and Friesen defined “communicative nonverbal behaviour” as “those acts which are clearly and consciously intended by the sender to transmit a specifiable message to the receiver” <cite ref='Ekman1969'/>. The terms <ac>NVC</ac> and communicative non-verbal behaviour are treated interchangeably in this thesis and are characterised by their intentional expression <cite ref='Hall2012, Ekman1969, Lehtonen1981'/> and the use of a shared decoded meaning <cite ref='Hall2012, Ekman1969, Lehtonen1981, Wiener1972'/>. Similarly, Burgoon <macro v='etal'/> <cite ref='Burgoon1996'/> limited the scope of <ac>NVC</ac> to behaviours that “are typically sent with intent, are used with regularity among members of a social community, are typically interpreted as intentional, and have consensually recognized interpretations”. An implication of this definition is that some non-verbal behaviours are not communicative <cite ref='Ekman1969, Engle1998'/>. Ekman and Friesen define a third type of non-verbal behaviour: interactive non-verbal behaviour which “clearly modify or influence the interactive behaviour of the other person(s)” <cite ref='Ekman1969'/>. These definitions of non-verbal behaviour are overlapping concepts; the relationship between them is shown in Figure <ref label='FigureEkmanNvb'/>.</p>

<p>In contrast to the definition of <ac>NVC</ac> above, it is popular to define <ac>NVC</ac> in a much broader sense. This view considers communication to encompass all forms of information transfer, including unintentionally expressed behaviour and informative non-verbal behaviour. The broader definition was described by Fiske <cite ref='Fiske2010'/> as the “semiotics school” and the narrower <ac>NVC</ac> definition (in the previous paragraph) as the “process school”, with both approaches being necessary for appreciation of the field (see also <cite ref='Rozelle2006'/> for a broad literature review). Motley argued that, irrespective of the controversy of definitions, intentional and unintentional behaviour are distinct <cite ref='Motley1986'/>. The broad definition excludes the possibility of non-communicative non-verbal behaviour because any behaviour can be potentially interpreted, therefore “one cannot not communicate” as claimed by Paul Watzlawick <cite ref='Andersen1991'/>. However, this definition was considered to be over-broad by MacKay <cite ref='MacKay1972'/>, leading him to joke that by this broader definition, the study of <ac>NVC</ac> would “cover every interaction in the universe except the use of words!”</p>

<p>Mental state is a broad term for describing temporary mental conditions that have characteristic properties (see <cite ref='Perner1999'/> for a review). Mental states include emotion, attitudes, moods and cognitions <cite ref='Martin1990'/>. Many mental states manifest themselves as an outward expression and this expression can be intentionally modified to form a partly impulsive and partly controlled expression <cite ref='Frith2009'/>. This means mental state displays have both voluntary and involuntary aspects. However, some mental states are not necessarily externally observable, which distinguishes them from <ac>NVC</ac> which is always externally observable. Emotions are a group of mental states that have connections to particular behaviours, have particular physiological manifestations (such as facial expression) and are also subjective experiences <cite ref='Frijda1986'/>. They tend to be triggered by stimuli, which are evaluated by a person, and spontaneously results in an emotion. Emotions are often expressed by facial expressions, vocal utterances, behavioural changes, and physical responses. The facial area is particularly important in emotion perception <cite ref='Ekman2009'/>. A facial gesture is a motion executed with the facial muscles and may be associated with mental states, emotion or <ac>NVC</ac>. Emotion is an ill defined concept <cite ref='BeckerAsano2011, Shaver1987, Fehr1984'/>.
However, this does not imply that there is no such thing as emotion, nor that it is not a subject worthy of investigation.</p>

<p><ac>NVC</ac> is a communication act that is only expressed in social situations. While some hold the view that emotions are also limited to social situations, others have observed that some forms of emotion can be expressed in non-social situations such as reading <cite ref='Mar2008'/> or dreaming <cite ref='Nielsen1991'/>. In a similar way to <ac>NVC</ac> which typically lasting from a few seconds <cite ref='Lee1998'/> up to hours or longer, emotions can last from a fraction of a second (e.g. surprise) to hours or even longer (such as with empathy) <cite ref='Aaron1997, Verduyn2009'/>. The choice of clothing is an example of <ac>NVC</ac> that has a long duration. A person's internal emotional state is sometimes manifested by facial expression, but the extent to which this is a direct relationship is controversial <cite ref='Azar2000'/>. At one stage Ekman claimed that six emotions (Anger, Disgust, Fear, Happiness, Sadness and Surprise) occurred across cultures <cite ref='Ekman1972'/> and called them “basic emotions”, but later appended additional emotions to this list <cite ref='Ekman99'/>. He holds the position that different emotions are physiologically discrete and separate. This contrasts with approaches that describe emotions using continuous value labels <cite ref='Cowie2005'/>.</p>

<p>A social signal is `a communicative or informative signal which, either directly or indirectly, provides information about “social facts”, that is, about social interactions, social attitudes, social relations and social emotions' <cite ref='Poggi2011'/>. Social signals include `interest, determination, friendliness, boredom, and other “attitudes” toward a social situation' <cite ref='Pentland2007'/>. The term social signals is used in animal behaviour<cite ref='Lin2005'/> and for human non-verbal behaviour <cite ref='Pentland2007'/>. The definition does not specify if the signals are necessarily verbal or non-verbal and many authors use the term “non-verbal social signal” for clarity <cite ref='Okwechime2011, Vinciarelli2008'/>. They are distinct from “social behaviours” which “convey information about feelings, mental state, personality, and other traits of people” and includes expressions of politeness and empathy <cite ref='Vinciarelli2008'/>. The next section discusses the factors on which <ac>NVC</ac> expression depends.</p>

<section title="What Factors Influence &lt;ac&gt;NVC&lt;/ac&gt; Expression?" label="BackgroundWhatFactorsInfluenceNvc">

<p>There are many factors that influence human expression and perception, and specifically <ac>NVC</ac>. If an automatic system is intended to be trained and deployed in a single environment, this will be of little concern. Although encoding based on motion is not sensitive to context, a system that is to recognise communicative intention for multiple people, or in multiple social and cultural situations, needs to account for contextual factors that can change how <ac>NVC</ac> messages are interpreted<footnote>“From this example, it is obvious that in order to determine the communicative intention conveyed by an observed behavioural cue, one must know the context in which the observed signal has been displayed” <cite ref='Pantic2008'/>.</footnote> <ac>NVC</ac> is largely determined by the social situation in which it is used and therefore it is important to study <ac>NVC</ac> in natural social situations <cite ref='Bavelas97'/>. Social context is also a factor that is used by human observers to interpret the behaviour of other humans, and humans are not reliable observers when this context is removed <cite ref='Hoque2009'/>. Social context, also referred to as social environment, “encompass[es] the immediate physical surroundings, social relationships, and cultural milieus within which defined groups of people function and interact” <cite ref='Barnett2001'/>. A waving gesture can be a greeting or a sign of distress depending on the context in which it occurs. Although cultural differences in expression exist for many <ac>NVC</ac> signals, some signals have a similar appearance across cultures and social situations. There has been little research of the automatic recognition of <ac>NVC</ac> messages that are specific to contexts, with most existing approaches only considering a single context or seek to generalise <ac>NVC</ac> across different contexts.</p>

<p>Factors which influence the expression and interpretation of human behaviour include:</p>

<ul>
<li>culture (this is discussed in depth in Section <ref label='BackgroundCrossCulture'/>),</li>
<li>gender,</li>
<li>personal style,</li>
<li>personality and</li>
<li>social situation.</li>
</ul>

<p>People naturally vary in expressiveness; some individuals being animated while others being comparatively inexpressive <cite ref='Afzal2009'/>. Buck claimed that encoding and perceiving accuracy for <ac>NVC</ac> was dependent on gender, personality and age <cite ref='Buck1979'/>. However, these studies assume that there can be “correct” and “incorrect” interpretations of emotion and <ac>NVC</ac>. This view seems questionable because the use of posed behaviour does not necessarily imply that the samples are directly associated with an objective, exemplar basis of human behaviour. However, the study does highlight the differences in the interpretation. Gender difference in expression style was investigated by Morrison <macro v='etal'/> <cite ref='Morrison2007'/>, who found there were specific facial movement styles that could be used by humans to identify gender. Some medical conditions, such as schizophrenia spectrum disorder, can change how <ac>NVC</ac> is expressed <cite ref='Brune2009'/>.
All these studies provide evidence that there are individual variations in how <ac>NVC</ac> or emotion is expressed.</p>

<p>When humans experience emotion, the emotion often manifests itself in body language and facial expression. Emotion expression is based on many factors. The mapping from emotional state to emotional expression can be thought of as a set of rules, according to Ekman and Friesen <cite ref='Ekman1975'/> with each culture having a specific set of encoding rules. Other researchers have extended this idea to specific social situations having distinct display rules <cite ref='Feldman1991, Brinton2007'/> that encode the internal state. For this reason, a person expressing an emotion is sometimes referred to as an “encoder”.
However, cultural differences in gaze aversion was described in terms of cultural rules by McCarthy <macro v='etal'/> <cite ref='McCarthy2006'/>. 
Research related to the effect of culture is described in more depth in Section <ref label='BackgroundCrossCulture'/>.
Studying the mapping from emotion to expression across cultures can be challenging. To demonstrate that cultural display rule differences or similarities exist, the underlying emotions used must be shown to be equivalent across cultures. <ac>NVC</ac> expression may also have encoding rules which are analogous to encoding and display rules.
Just as emotions have shared social norms and expectations that are used when managing and modifying emotional displays in different social circumstances, <ac>NVC</ac> expression is also dependent on social circumstances.</p>

<p>Social situation is also a factor in <ac>NVC</ac> and emotion expression. The effect of social situation itself shows cultural, gender and personality differences <cite ref='Argyle1994'/> making these factors interdependent. Social situation is significant for capturing a corpus of <ac>NVC</ac> behaviours, because the situation in which the recording takes place affects the type and frequency of observed behaviours. It is often convenient to record posed data, because behaviours directly correspond to pre-determined labels and little time is wasted on recording uninteresting behaviour<footnote>Even if acted data is considered as having no social context, the absence of a social context is a factor in <ac>NVC</ac> expression.</footnote>. All human behaviour occurs in a situational context, although acted behaviour may be a special case in that it has a context in terms of social interaction with the audience. Unfortunately, human behaviour is significantly different in posed situations when compared to spontaneous behaviour. Cowie <cite ref='Cowie2005'/> argues that the use of posed data cannot be completely excluded but posed data should not be used uncritically. There has been a recent shift in the automatic human behaviour recognition community to use natural data, rather than posed. However there is a wide range of approaches to collect so called “natural data” to the point that the word can be misleading. The definition of natural language proposed by Stubbs <cite ref='Stubbs1983'/> can be adapted to suit <ac>NVC</ac>: that natural <ac>NVC</ac> occurs “without any intervention from the linguist [or experimenter]” and “is spontaneous in the sense of unplanned, and which is composed in real time in response to immediate situational demands”. Applying this to <ac>NVC</ac>, this definition excludes posed examples of <ac>NVC</ac>, as well as <ac>NVC</ac> based on artificial tasks, stages scenarios, role play tasks or experimenter controlled stimuli (such as a “Wizard of Oz” apparatus). The issue of social situation is discussed in more depth in Section <ref label='BackgroundSocialContextUsedInTwoTalk'/>. Just as the expression of <ac>NVC</ac> depends on many factors, the perception of <ac>NVC</ac> is also dependent on multiple factors. These will be discussed in the next section.</p>

</section>
<section title="Perception of &lt;ac&gt;NVC&lt;/ac&gt;" label="BackgroundWhatFactorsInfluenceNvcPerception" label2="BackgroundPeopleVaryInExpression">

<p>The interpretation of emotion is based on various contextual factors, such as culture, social situation, age, etc. Building on the idea of encoding rules, Argyle <cite ref='Argyle1994'/> suggested there exists a mapping from observable behaviour to a meaningful interpretation and termed this mapping as “decoding rules”. Several studies have found context and conditions that affect perception of human behaviour, including <ac>NVC</ac> and emotion. Terracciano <macro v='etal'/> <cite ref='Terracciano2003'/> found there is a relationship between personality and emotion perception. Matsumoto <macro v='etal'/> <cite ref='Matsumoto2000'/> claimed that accuracy in emotion recognition was correlated with personality measures such as Openness and Conscientiousness. Personality differences of experimental participants, such as neuroticism, were associated with different subject gaze patterns being observed during an emotion recognition task <cite ref='Perlman2009'/>. However, little work has been conducted in the association between personality differences and <ac>NVC</ac> perception. Behaviour mirroring, which is the tendency of people to adopt the behaviour of another person in a conversation, is more pronounced in people who had tested highly on an empathic personality measure <cite ref='Chartrand1999'/>. Lanzetta and Kleck <cite ref='Lanzetta1970'/> found that a person's <ac>NVC</ac> perception ability was related to personality and that highly able subjects were themselves difficult subjects for others to read. All these findings provide evidence that both personality and individual style have a role in the perception of emotions and <ac>NVC</ac>.</p>

<p>Another factor in behaviour perception is gender. Vassallo <macro v='etal'/> <cite ref='Vassallo2009'/> found that gaze patterns during evaluation of emotion were different between genders and that females arrived at a judgement faster than males. Gender based differences in annotator judgements have also been observed in several studies <cite ref='Buck1979, Terracciano2003, Abrilian2006'/>. Context can be a factor in perception, but this may still be specific to certain cultures and personalities. The way face images are viewed can affect perception. The presence and expression of surrounding faces changes the judgement of a central face in some cultures but not others <cite ref='Masuda2008'/>. Goren and Wilson <cite ref='Goren2006'/> found that intense, posed emotion is easier to categorise than weak, posed emotion, as well as finding that emotions are harder to rate accurately if viewed by peripheral vision. Perception of emotion was significantly affected by the level of familiarity with the person being observed <cite ref='Hoque2009'/>. El Kaliouby <macro v='etal'/> <cite ref='ElKaliouby03'/> found that while basic emotions (Ekman 6 and contempt) were almost instantaneously recognized by humans, temporal context improved human recognition for complex emotions, such as interest, boredom, confusion, etc. All these factors make annotation a challenging task, because these variables cannot be easily controlled while maintaining the naturalness of the data. Using multiple annotators and finding a consensus is one common approach. However, personality, relationship and familiarity of people in a social situation are important contextual factors in the expression of <ac>NVC</ac> and using a consensus score discards this aspect of context.</p>

</section>
<section title="Supervised Learning for &lt;ac&gt;NVC&lt;/ac&gt; Recognition in Video Sequences">

<p>One of the aims of this thesis is to create an automatic system to perform facial, intentional <ac>NVC</ac> recognition, based on previously observed examples of behaviour. As well as directly relevant studies in the field of automatic behaviour recognition, this review discusses research from other fields if they are particularly relevant to this thesis. Video recording of <ac>NVC</ac> has been previously been used in the behavioural sciences <cite ref='Matsumoto1991'/>, as well as studies into facial biometrics <cite ref='Goswami2010'/>, automatic lip reading <cite ref='Ong2011'/>, character animation <cite ref='Bickel2008'/> and affective computing systems <cite ref='Pantic2008, Bartlett2010, Zeng2009'/>.</p>

<p>Based on video of facial behaviour, training data and manually annotated labels are used to create a model; this process is known as supervised learning. Based on the model, labels for unseen samples can be automatically predicted. The input data and labels are digitised to allow computers to process the input data. Supervised learning is often divided into <macro v='featureGeneration,'/> which provides a set of higher level features, followed by a classification technique. 
Given an ideal training set with total coverage of the feature space, the use of <macro v='featureGeneration'/> and a sophisticated classifier would not be necessary; nearest neighbour classification <cite ref='Cover1967'/> would be sufficient. However, the available data sets contain a limited number of samples and also contain noise. To improve the performance of a classifier on limited training data, visual changes that are not relevant to the task at hand should be separated or removed from the input data. This improves the robustness of an automatic system and is usually achieved by manual design of the system or by feature selection. This is often accomplished by <macro v='featureGenerationComma'/> which aims to improve robustness and reduce the quantity of training data required. Almost every application of supervised learning uses <macro v='featureGeneration'/> which transforms the data into a “higher level” representation; common approaches are described in Section <ref label='BackgroundEncodeFacialInfo'/>. The <macro v='featureGeneration'/> technique used in an automatic system is usually selected manually. There are also many supervised classification methods (described in Section <ref label='BackgroundSupervisedClassification'/>). The type of classifier used may dictate which <macro v='featureGeneration'/> approach is optimal and visa-versa, so these issues are closely interrelated.</p>

<figure short="The common approaches to using a classifier with video sequences." title="The common approaches to using a classifier with video sequences. &lt;macro v='FeatureGeneration'/&gt; and machine learning have a different role in each case." label="FigureVideoRecognition">
<graphic>nvcclass/VideoRecognition.pdf</graphic>
</figure>

<p>Once <macro v='featureGeneration'/> has been performed on a sequence of video frames, the variable length of examples and temporal nature of a video must be considered to achieve effective automatic recognition performance. This section only considers the processing of visual information and does not consider the issues of fusing audio and video, which is beyond the scope of this thesis. A temporal <ac>NVC</ac> signal can be recognized using data from multiple observations by using <macro v='dataFusion'/> which encodes temporal variations early in the recognition process. Alternatively, a model based recognition system may be used, which deals with temporal variation at a higher level.
<macro v='DataFusion'/> of multiple observations that were acquired at different times is referred to as “temporal fusion” <cite ref='Varshney1997, Ko2008'/>. 
These <macro v='dataFusion'/> approaches will now be described in more detail, in the context of temporal, uni-modal video based recognition (see also Figure <ref label='FigureVideoRecognition'/>).</p>

<ul>
<li>Sensor level feature extraction consolidates raw video image images into a combined raw feature. This is rarely used in the context of human behaviour recognition.</li>
<li>In one form of temporal feature extraction, all frames can be concatenated and directly used by a standard classifier. This can be directly applied to fixed length examples. (Figure <ref label='FigureVideoRecognition'/> a) Frame concatenation is a special case of feature-level encoding. If different modalities are sampled at different frequencies or if samples are of varying length, re-sampling can enable feature concatenation.</li>
<li>Decision level fusion: Each frame can be classified individually using standard machine learning, then the predictions combined and used in a second classifier step to provide an overall overall clip prediction. Decision level fusion may use multiple samples from one or more types of sensor <cite ref='Vakayallapati2011'/>. (Figure <ref label='FigureVideoRecognition'/> b). Variants of decision level fusion include rank level fusion and match score fusion. Rank level fusion: multiple recognisers rank possible hypotheses and these rankings are combined to form a final label. Match score fusion: prediction labels are provided by multiple recognizers based on multiple, individual observations and these labels are then combined to generate information for decision making.</li>
<li>Model level recognition: individually encoded frames can be directly used by some types of machine learning methods. The order of the frames in the clip can be used (as typically done with <ac>HMM</ac>, <ac>CRF</ac>) or ignored (e.g. as Nearest Neighbour or <ac>MIL</ac>). (Figure <ref label='FigureVideoRecognition'/> c)</li>
<li>Feature extraction methods encode how low level observations vary over time in a way that can be interpreted by a classifier. Frames can be summarised or combined into a fixed length vector, then classified using a standard supervised learning technique. This approach is used in Sections <ref label='SectionTemporalFeatures'/> and <ref label='SectionClipFeatureExtraction'/>. (Figure <ref label='FigureVideoRecognition'/> d). One simple approach is to concatenate frames into a single vector (Figure <ref label='FigureVideoRecognition'/> a).</li>
</ul>

<p>Concatenating frame based shape or appearance features before classification is rarely used because videos of varying lengths cannot be directly compared and the approach tends to be sensitive to the speed of activity in the video. For varying length videos, one approach is to re-sample the frame features to produce a fixed length vector, as done by Pfister <macro v='etal'/> <cite ref='Pfister2011'/>. However, the feature vector can be sensitive to whether an event of interest occurs at the start of a clip, or at the end; this may not be desirable if the occurrence of the event at any time is of significance. This is not an issue for approaches that use unsegmented videos <cite ref='Oikonomopoulos2011'/>.</p>

<p>Feature extraction is a popular group of approaches that combine one or more frames into a feature vector, usually of fixed length. Multimodal feature extraction by up-sampling video features and concatenating them with audio features was performed by Petridis and Pantic <cite ref='Petridis2008, Petridis2009, Petridis2011'/> and others <cite ref='Potamianos2001, McCowan2005, Nicolaou2011b'/>. This can increase the dimensionality of the data, which may reduce the performance of the final system. Other feature-level extraction approaches try to encode temporal variations <cite ref='Dietrich2001'/>. Valstar <macro v='etal'/> <cite ref='Valstar2006'/> calculated the mean and maximum change in the displacement and velocity of feature points during facial deformations, which is a form of temporal feature extraction at the feature level. This reduces the dimensionality of the data while hopefully retaining temporal information about the face. In a similar way, face deformation based on heuristic features was encoded by taking the variance of each feature by Datcu and Rothkrantz <cite ref='Datcu2007'/>. As well as using the mean and standard deviation of features in a clip, Petridis and Pantic <cite ref='Petridis2008'/> fitted a quadratic curve to each feature component to encode temporal variation. Most of these approaches use geometric deformation based features, however temporal encoding has also been used for texture and audio. Aligned facial textures were compared using the mean shape and appearance features by Fang and Costen <cite ref='Fang2009'/>, however, using only the mean does not encode temporal variation information. For audio features, Grimm <macro v='etal'/> <cite ref='Grimm2007'/> used seven simple statistical measures on the features, as well as the feature's first and second derivatives, which may be useful in encoding the rate of feature variation.</p>

<p>Fusion after matching combines the predictions from multiple classifiers to form a final decision. This is also known as match score fusion and is a form of decision level fusion. This may be used to combine different types of features, different modalities, predictions from independent video frames or the predictions of different classification methods. The most popular decision level fusion methods are the “sum rule” and rank based voting <cite ref='Kittler2003'/>. The sum rule takes the class dependent average of the predicted probabilities and selects the class label with the highest score. Rank based fusion has each classifier rank each label and these are combined, using one of several approaches, to form a final prediction. One ranking fusion method is majority voting, which takes the highest probability label from the classifiers and the label with the highest proportion is taken as the fused prediction. Audio and visual decision level fusion has been used in many studies <cite ref='Vakayallapati2011, Poh2010, Petridis2008, Petridis2010, Kanluan2008'/>. Pfister <macro v='etal'/> <cite ref='Pfister2011b'/> used majority voting to combine predictions of three types of classifier for visual features. Three different rank based fusion methods were compared by Akak{<macro v='i}'/>n and Sankur <cite ref='Akakin2011'/> to combine different types of visual features, although there were no appreciable performance differences between the ranking approaches. Audio and video modalities were fused at the decision-level by the sum rule by Petridis and Pantic <cite ref='Petridis2008'/>. The sum rule allows the decisions to be weighted to change the emphasis on different modalities, as done manually by Kanluan <macro v='etal'/> <cite ref='Kanluan2008'/>, or automatically by <ac>LLR</ac> fusion as performed by Lucey <macro v='etal'/> <cite ref='Lucey2009'/>. Oikonomopoulos <macro v='etal'/> <cite ref='Oikonomopoulos2011'/> used voting to combine information from different human action models. Ko <macro v='etal'/> <cite ref='Ko2008'/> showed that dynamic time warping can result in a higher performance than a <ac>HMM</ac> approach for recognizing hand gestures based on multi-sensor fused data. Based on these studies, combining multiple modalities with decision-level fusion often results in a large increase in performance compared to a single modality (although exceptions exist to this pattern <cite ref='Nicolaou2011b'/>) and often has recognition performance advantages over feature level fusion (although some studies have shown that the performance using feature fusion is comparable to other approaches in some cases <cite ref='McCowan2005, Petridis2008'/>).</p>

<p>Rather than combining features or decisions, feature vectors from multiple frames may be considered directly by a machine learning algorithm. There are two general classes of algorithms: sequential temporal model classifiers (e.g. <ac>HMM</ac>, <ac>CRF</ac>) that consider an ordered group of items, and “unordered set” classifiers (e.g. <ac>MIL</ac>) which considers a set of items in which the ordering is not considered. These are among the most popular approaches in the recognition of human behaviour. They are discussed in more depth in Section <ref label='BackgroundSupervisedClassification'/>.</p>

<p>Petridis <macro v='etal'/> <cite ref='Petridis2010'/> used a different approach to combining multimodal information by using the audio mode to predict the expected visual features and visa-versa. A positive prediction is made if the predicted features match the observed features and this approach exceeded feature level fusion performance for laughter vs. speech discrimination.</p>

</section>
<section title="Encoding Facial Information" label="BackgroundEncodeFacialInfo">

<p>Facial features encode shape information by shape or appearance changes, or by combining both types of information. The common approaches will now be described and some of their strengths and weaknesses will be discussed.</p>

<subsection title="Encoding Behaviour Using Appearance Based Features">

<p>Changes in facial expression often cause changes in facial appearance due to wrinkles, deformations that change the visibility of the inside of the mouth, etc. However, to make effective use of limited training data, images are typically aligned to a canonical head position or to a canonical head shape. This ensures correspondence between local areas of different face images is maintained and the effect of head pose changes and translation is reduced, making the method much more effective. An alternative is to use changes in an overall image without using alignment which would be effective in encoding head movements, as done by Escalera <macro v='etal'/> <cite ref='Escalera2009'/>, but this cannot accurately encode subtle facial deformations. Some papers on emotion recognition deliberately ignore the image alignment problem and instead focus on the robustness of later steps in the automatic process <cite ref='Shan2009'/>.</p>

<p>Some approaches to face alignment begin with interest point detection; the face can be subsequently re-scaled or affine transformed to a canonical alignment <cite ref='Zhu2009b'/>. Yang <macro v='etal'/> <cite ref='Yang2009b'/> used SIFT interest points and a similarity transform. Yang and Bhanu <cite ref='Yang2011'/> found point correspondences between images using the SIFT-flow technique and removed shape information in the alignment process. This would help to reduce the effect of identity in emotion recognition. Zhu <macro v='etal'/> used Lucas-Kanade based local correspondences to perform a non-rigid transform <cite ref='Zhu2009b'/>.</p>

<p>Other approaches for face alignment use whole face detection or a more general model for the overall face shape or colour. The most popular basis for face alignment is the Viola and Jones object detector <cite ref='Viola2002'/>, with several studies using this as a basis for image alignment <cite ref='Kanluan2008, Bartlett2006, Moore07'/>. Skin colour based detection was used by Feng <macro v='etal'/> <cite ref='Feng2005'/> to align images; this was robust to illumination changes and, to a limited extent, to pose changes. Some models attempt to fit an expression model (which is often inspired or validated using <ac>FACS</ac> <cite ref='Cosker2010, Ahlberg2001'/>) or a shape and appearance model to the observed face, or be limited to variations of shape, head position and pose. Shape based approaches will be discussed below in more depth but there are a few studies that use face models for the purposes of alignment pre-processing before extracting appearance based features. Pfister <macro v='etal'/> <cite ref='Pfister2011'/> used an active shape model to achieve alignment. <ac>AAM</ac>s may also be used for normalisation <cite ref='Lan2009'/> but it common to use the model's parameterisation directly for recognition, rather than to the normalised face as a preprocessing step. Dornaika and Davoine <cite ref='Dornaika2005'/> used a 3{D} head model to not only normalise the face translation and pose, but also to remove shape information from the features. Other whole face based approaches avoid shape models and use image information more directly to determine an appropriate alignment transform. Visser <macro v='etal'/> <cite ref='Visser1999'/> used <ac>PCA</ac> on training images to perform lip localisation, however this was computationally expensive. The alignment transform may be directly found by the affine transformation of an input image on to a canonical image, by maximising pixel intensity correlation <cite ref='Tzimiropoulos2011'/>. Rudovic and Pantic <cite ref='Rudovic2011'/> used a shape constrained Gaussian process to fit a facial point based model while retaining an anatomically plausible shape. Dhall <cite ref='Dhall2011'/> <macro v='etal'/> used <ac>CLM</ac> to normalise head positions for emotion recognition, which fits a parameterised model to landmarked positions based on an ensemble of local feature detectors.</p>

<p>Image alignment is intended to remove appearance variations caused by face translation and pose. Often a simple affine or re-scaling transformation is employed. However, natural conversations contain extreme head poses. Until recently, alignment of images based on detection of facial features over a wide range of head poses was an unsolved problem <cite ref='Zhu2012'/>. An affine transform from an input image to a canonical face is challenging because of self-occlusions <cite ref='Yang2011'/>. For these reasons, using an approach that depends on face alignment to frontal view may fail at extreme poses.</p>

<p>Once the input image has been aligned, appearance features may be extracted that correspond across multiple faces. Two broad approaches are employed: holistically encoding pixel information or part based methods. These types of appearance based features will be discussed.</p>

<ul>
<li>Holistic approaches use dimensionality reduction techniques to encode changes in overall pixel intensity. Visser <macro v='etal'/> <cite ref='Visser1999'/> and Seguier <macro v='etal'/> <cite ref='Seguier2002'/> performed <ac>PCA</ac> on the lip region of interest. Chew <macro v='etal'/> <cite ref='Chew2012'/> used shape normalised textures to recognise <ac>AU</ac>s and compared this method to local texture approaches under the effect of noise, on four public datasets. Texture descriptors may be applied to every pixel in the image as the basis for classification <cite ref='Bartlett2006, Moore07'/>. If a transform is applied on a per pixel basis, the feature vector may become excessively large. Using histograms of texture reduces the representation to a manageable size and also reduces spatial sensitivity, which may be advantageous in improving robustness to insignificant differences in face shape. Kanluan <macro v='etal'/> <cite ref='Kanluan2008'/> performed <ac>DCT</ac> on lip and eye pixels. He <macro v='etal'/> <cite ref='He2005'/> used <ac>ICA</ac> on facial images to find independent modes of variation. These methods are useful for finding low frequency spatial information, which is often more relevant than using individual pixels or high frequency components, which correspond to very small areas or very small changes that are unlikely to be significant on their own. However, changes in individual components of these methods typically correspond to a global change in the region of interest and would involve multiple muscle movements. It is likely that features that do not isolate changes based on individual facial muscles would be sub-optimal. %This is due to the inter-person correspondence between muscles for facial expression, as encoded by <ac>FACS</ac>, is closer in similarity than for the face's visual appearance (due to interpersonal differences e.g. face shape, wrinkles, facial hair, etc).</li>

<li>Rather than encode an overall image, a feature can attempt to encode local texture information near points of interest. This makes localised changes in the face affect only some feature components. As mentioned, this decoupling of the different parts of the face may be useful because it is expected that local changes in the face are useful for recognition. Texture descriptors may be applied to limited regions and encoded using histograms, which provides a more compact representation <cite ref='Feng2005, Shan2009, Pfister2011, Moore2009, Yang2011, Tingfan2012'/>. There are several popular texture descriptors used in facial recognition, particularly Gabor filters (<cite ref='Bartlett2006, Wang2006, Rose2006'/>) and <ac>LBP</ac> features (<cite ref='Feng2005, Shan2009, Yang2011, Moore2009'/>). Recent work has considered layers of texture descriptors. Senechal <macro v='etal'/> <cite ref='Senechal2012'/> used <ac>LGBP</ac> histograms for <ac>AU</ac> recognition, which are <ac>LBP</ac> operators applied to Gabor filtered images. Tingfan <cite ref='Tingfan2012'/> compared using a single layer (Gabor energy filters and <ac>LBP</ac>s) with double layer texture filtering, finding that double layer is more effective in multiple data sets.
<ac>LBP</ac>s were extended to encode temporal variation, and named <ac>LBP</ac>-<ac>TOP</ac>, by Pfister <macro v='etal'/> <cite ref='Pfister2011'/>, however considering changes on a short time scale, such as consecutive frames, may not be optimal for all facial gestures. <ac>LBP</ac> features are also noted for being relatively robust to illumination changes, and in some forms are also robust to rotation. Other approaches include edge based texture descriptors, such as Moore and Bowden <cite ref='Moore07'/> using Canny edge detection to form chamfer images as the feature extraction step. Yang and Bhanu <cite ref='Yang2011'/> used <ac>LPQ</ac> features that exceeded <ac>LBP</ac>s performance for emotion recognition on acted data. Jiang <cite ref='Jiang2011'/> extended the <ac>LPQ</ac> to consider temporal variation (<ac>LPQ</ac>-<ac>TOP</ac>) and found it exceeded performance of <ac>LBP</ac>-<ac>TOP</ac>.</li>
</ul>

<p>Donato <macro v='etal'/> <cite ref='Donato1999'/> compared many appearance based approaches and found Gabor features with <ac>ICA</ac> dimensionality reduction was the most effective for facial expression recognition.</p>

<p>As previously mentioned, images are usually aligned before appearance features are extracted. While these features have been successfully employed for many constrained datasets, appearance based features are only as good as the face alignment process, which may be problematic for extreme head pose. Appearance based features encode information about the face that can be used as the basis of classification, but also includes information that is not necessarily relevant. Depending on the behaviour under consideration, these irrelevant appearance differences include facial hair, wrinkles, skin colour, glasses, etc. Although some emotions are associated with colour change in the face, colour information is rarely considered because differences in skin colour make interpersonal facial colour comparisons problematic. Various approaches can be employed to remove personal differences in features (Section <ref label='SectionPostProcessingFacialFeatures'/>). Appearance features also may be non-optimal if significant facial movements occur that do not cause a detectable change in local appearance.
Facial deformations are expected to be important for automatic <ac>NVC</ac> recognition. For this reason, many approaches attempt to encode face shape directly. The next section will consider these types of features.</p>

</subsection>
<subsection title="Encoding Behaviour Using Shape Based Features" label="BackgroundTemplateTracking">

<p>Shape based features are used to encode information about facial deformation and head pose. Because many human <ac>NVC</ac> signals are based on body and face gestures, encoding the shape captures this type of information. Shape based features usually involve fitting a model to an observation image. The body part selected for feature extraction and sophistication of the model depend on the task that is being attempted. Various approaches have used simple head models that can encode head pose changes, or both the pose and facial expression. Yang <macro v='etal'/> <cite ref='Yang2009'/> used a simple 2{D} model to encode the face. Chen and Ji <cite ref='Chen2011'/> apply <ac long='yes'>DBN</ac>s to combine facial tracking with expression recognition in a hierarchical framework. Zhu and Ramanan <cite ref='Zhu2012'/> used mixtures of trees with a shared pool of parts to localise faces, estimate pose, as well as locate points of interest in unconstrained photographs. Petridis <macro v='etal'/> <cite ref='Petridis2009'/> used a 3{D} cylindrical model to track the face, with six degrees of freedom corresponding to translation and rotation. This would not encode facial expression but should be sufficient for gestures based on overall head movement. Dornaika and Davoine <cite ref='Dornaika2005'/> used a more sophisticated model including deformation to fit facial expression. Zeng <macro v='etal'/> <cite ref='Zeng2006'/> used a {3D} model based visual tracker for expression recognition. However, complex models are computationally demanding, have higher training requirements and usually require advanced methods to fit the model reliably and robustly. Other approaches use 3{D} shape based on structured light capture systems for emotion recognition <cite ref='Sandbach2011, Fanelli2010b, Chen2011'/>. The face is of most interest for <ac>NVC</ac> recognition, but may be complemented by other parts of the body. Occasionally other parts of the body are used for model fitting and recognition, including the shoulders <cite ref='Petridis2009'/> or the overall body pose <cite ref='Bernhardt07'/>.</p>

<p>Some studies use a hybrid approach to extract a model, such as <ac>AAM</ac>s, and then use only the components corresponding to the shape for recognition <cite ref='Girard2011'/>. This has been done by heuristic feature extraction from <ac>AAM</ac> features <cite ref='Datcu2007'/>, as well as distance ratios of points of interest <cite ref='Tang2007'/>. Other approaches use both shape and appearance, which will be discussed in the next section.</p>

<p>Another approach is to treat the body as independently moving parts, localise the position of each part in the frames of the video sequence and use this information directly, or fit a model based on the independent parts. This is conceptually simpler than fitting a complex model with inter-dependent parts. However, position and motion of areas of the body can provide information for the movement of nearby body areas and this is not used by tracking independent points of interest. Tracking attempts to localise a feature of interest in a series of video frames. Tracking uses the assumption that a point of interest has a locally constant appearance. This separates motion from other non-shape changes in the video. However, large changes in appearance caused by pose changes or occlusions make tracking spontaneous videos difficult. Lucas and Kanade  <cite ref='Lucas1981'/> proposed a method based on minimising the least squares difference of a training patch to a test image by gradient descent. However, it can suffer from local minima, noise and occlusions which can lead to tracker drift. An alternative is to use a particle filter based approach, which is robust to noise by using multiple hypotheses of tracker position <cite ref='Isard1998'/>. However, this approach does not scale well to high dimensionality. This was addressed by partitioning the model space components into groups by Patras and Pantic <cite ref='Patras2004'/>. This tracking method is limited to small head pose changes of approximately 20 degrees <cite ref='Valstar2012'/>. This is because of the change of appearance caused by head rotation breaks the assumption of an unchanging local appearance near to a tracked point of interest. Chew <macro v='etal'/> <cite ref='Chew2012'/> used a <ac>CLM</ac> to detect <ac>AU</ac>s, which uses detections of facial points of interest as the basis to fit a shape model. <ac>CLM</ac>s <cite ref='Cristinacce2008'/> are similar to <ac>AAM</ac>s but generate likely feature templates that correlate with the target image, rather than trying directly fitting a model to pixel intensities. Baltrusaitis <macro v='etal'/> <cite ref='Baltrusaitis2012'/> extended <ac>CLM</ac>s to {3D} and applied it to head pose tracking. Liwicki <macro v='etal'/> <cite ref='Liwicki2012, Liwicki2012b'/> proposed a kernel <ac>PCA</ac> method, which retains desirable properties of <ac>PCA</ac> while being robust to outliers; this encodes behaviour as shape information. They applied the method to different computer vision problems, including visual tracking and found it was better than 4 other state-of-the-art tracking methods in most video sequences.</p>

<p>The approach used in this thesis is the pre-existing tracking technique proposed by Ong <macro v='etal'/> <cite ref='Ong2009'/> called <ac full='yes'>LP</ac> tracking; this method encodes behaviour as face shape changes. This method learns a linear mapping from intensity changes in a sparse template to a tracker positional offset. The tracker is trained on multiple training frames which improves its robustness to appearance change, including an amount of head rotation. The maximum tolerated head rotation is not known, but it is suitable for tracking spontaneous human behaviour. The tracker does not automatically recover from occlusions, so the tracker must be manually re-initialised when a feature becomes visible. This could be addressed by incorporating an automatic feature detector to re-initialise tracking positions. This tracking process results in the facial behaviour being encoded as a series of shape deformation frames. The technical details of <ac>LP</ac> tracking are discussed in Appendix <ref label='BackgroundLpTracking'/>.</p>

<p>Tracking data encodes facial deformation changes and can be used directly for behaviour recognition. Tracking is not effective in areas which contain relatively little local texture, such as can be seen in puffed cheeks. Tracking has often been applied to estimate optical flow and this approach encodes the overall face deformation. This method was used to recognise emotions by Rosenblum <macro v='etal'/> <cite ref='Rosenblum1996'/>. Overall head translation information is usually unrelated to facial expression and is often separated from facial deformation information. However, differences in face shape can prevent direct comparison of optical flow features generated on two different people. Also, optical flow based approaches tend to be sensitive to head rotations and require a constant frontal view of the face. Other approaches use tracking of points that corresponds to known positions on the face. This correspondence makes inter-person comparison of deformations easier. Tracker movements are caused by both head movement and expression changes, but this is not ideal for existing machine learning techniques. Head motion was separated from deformation changes using <ac>PCA</ac> by Petridis and Pantic <cite ref='Petridis2008'/>. Another way to improve person independent recognition is to remove the effect of face shape by mean shape subtraction <cite ref='Tax2011'/>.</p>

</subsection>
<subsection title="Encoding Behaviour Using Both Shape and Appearance">

<p>The previous sections have discussed features that encode either the shape or the appearance of human <ac>NVC</ac> and emotion. To guide future work, it may be useful to know which approach is most effective, or if these approaches can be combined effectively. Some papers compare shape features to appearance feature approaches and this provides insight into the best approach for encoding behaviour. However, comparisons of specific techniques do not rule out the creation of superior <macro v='featureGeneration'/> techniques. Lien <cite ref='Lien1998a'/> compared three methods (feature point tracking, dense flow and texture descriptors) for emotion recognition of posed examples and found dense optical flow is most effective. Both Fanelli <macro v='etal'/> <cite ref='Fanelli2010'/> and Akak{<macro v='i}'/>n and Sankur <cite ref='Akakin2011'/> had similar findings, with optical flow or tracking being effective but optical flow or tracking combined with appearance and shape demonstrating a higher performance. However, optical flow techniques are susceptible to head rotation, which commonly occurs in spontaneous behaviour. Some model based methods extract both shape and appearance information which can be used for recognition. A popular approach for facial analysis is an <ac full='yes'>AAM</ac> <cite ref='Cootes1998'/>. This has been used for emotion recognition and studies have found that while shape is more important than appearance, use of combined shape and appearance has a higher performance <cite ref='Ashraf2007, Lucey2009'/>. However, <ac>AAM</ac>s have difficulty fitting to faces under a wide range of poses and expressions, and require additional specialised models to account for this variability <cite ref='Peyras2008, Lee2009'/>. This increases the training requirements of <ac>AAM</ac>s, making their application to new subjects very time consuming. Koelstra <macro v='etal'/> <cite ref='Koelstra2010'/> used motion both history images (which encodes appearance information) and a motion field (which encodes face shape changes) to form local histograms, as the basis for classification of facial actions with motion deformation features being the more effective approach.</p>

<p>The FERA2011 Challenge on emotion and <ac>AU</ac>s attempted to benchmark different approaches to provide a clear ranking for current approaches <cite ref='Valstar2011'/>. Despite the apparent dominance of shape based features in other comparisons, the best performing approach in this challenge used appearance features on aligned faces <cite ref='Yang2011'/>. Several other approaches used fusion of both appearance and shape <cite ref='Valstar2011slides, Valstar2012b'/>. Senechal <macro v='etal'/> <cite ref='Senechal2012'/> used an <ac>AAM</ac> to recognise <ac>AU</ac>s but this was found to be less effective than <ac>LGBP</ac> histograms. Tariq <macro v='etal'/> <cite ref='Tariq2011'/> fused local optical flow, <ac>SIFT</ac> histograms and Hierarchical Gaussianization achieved the best person-specific emotion recognition performance.</p>

<p>Only a single paper used shape <macro v='featureGeneration'/> exclusively but did not have a competitive performance. All the published comparisons of different classes of features have used posed data. The relative importance of shape and appearance is still under debate for this problem, but combining both types of features usually results in improved performance.</p>

</subsection>
<subsection title="Encoding Behaviour Using Audio Features">

<p>Paralangauage, which is a form of non-verbal audible communication, provides information that is not necessarily available if only visual facial behaviour is considered. Many studies have employed audible signals exclusively or combined both audio and visual signals (see <cite ref='Schuller2011'/> for a review). Audio feature extraction methods include fundamental frequency (F0) <cite ref='Liscombe2003, Grimm2007'/>, intensity, Mel Frequency Cepstral Coefficients <cite ref='Grimm2007'/>, distribution energy in spectrum <cite ref='Truong2009'/>, speech rate <cite ref='Truong2009'/> and manually extracted features e.g. spectral tilt <cite ref='Liscombe2003'/>. Automatic systems have been created that recognise arousal and valence labelled data <cite ref='Truong2009'/>, categorical emotional labels <cite ref='Liscombe2003, Hassan2009'/> and affective states <cite ref='SobolShikler2010'/>. Hassan and Damper <cite ref='Hassan2009'/> showed that finding an appropriate subset of features, using <ac>SFS</ac>, can enable a simple classifier (k-nearest neighbour) can provide a better performance than <ac>SVM</ac> classification without feature selection. Speaker normalisation is sometimes used to improve generalisation to new subjects <cite ref='SobolShikler2010'/> or languages not contained in the training data <cite ref='SobolShikler2010, Hassan2012'/>.</p>

<p>Many studies use both visual and audio features to recognise behaviour and emotion <cite ref='Petridis2011, Kanluan2008, Haq2009'/>. There is a close association between facial behaviour and the non-verbal channel of communication <cite ref='Busso2007'/>. The next section describes further processing that can be applied to features, in an attempt to improve robustness.</p>

</subsection>
<subsection title="Post-processing of Facial Features" label="SectionPostProcessingFacialFeatures">

<p>As discussed in the previous section, <macro v='featureGeneration'/> attempts to extract relevant information from raw observations while ignoring variations that are irrelevant for the intended task. Many <macro v='featureGeneration'/> approaches consider individual frames and do not consider the overall video sequence. However, facial expressions can evolve over many frames, rather than being restricted to one or two frames. Using the information for multiple frames can be useful to generate better features and to make the final system more robust. Face shape is associated with a person's identity but this is not of interest for automatic <ac>NVC</ac> recognition. Also, individual feature components can be processed by dimensionality reduction algorithms to attempt to isolate the information of interest. This makes the features more suitable for machine learning, particularly when combined with feature selection.</p>

<p>Using the set of feature values over a long time period, various normalisations may be applied. Face shape for neutral expression cannot easily be inferred from a single frame of video because of the possible presence of expression, but over many frames the neutral face shape is relatively easy to determine, particularly in naturalistic behaviour which often contains long periods of neutral face expressions. The effect of face shape can then be subtracted from the features which improves a system's robustness to identity. Other inter-personal differences, such as facial hair, skin colour and glasses can affect appearance based features but can be managed in a similar way to face shape. There have been several different feature normalisation methods proposed including mean feature subtraction <cite ref='Jeni2011, Tax2011'/> and scaling features to a standard variance <cite ref='Yang2009'/> (also referred to as “whitening”). Six audio feature normalisation methods were compared by Wöllmer <macro v='etal'/> <cite ref='Wollmer2008'/> who found that normalising features to a range of -1 to +1 was advantageous in some cases and was thought to remove identity based differences. Such normalisations are suitable for recorded data but cannot be immediately used for prediction based on a previously unseen face.</p>

<p>Features often encode both information of interest as well as irrelevant information. It can be beneficial to use dimensionality reduction in an attempt to separate relevant information into specific feature components in an unsupervised way. Lien <macro v='etal'/> <cite ref='Lien1998a'/> used <ac>PCA</ac> on dense optical flow features of the face from multiple frames to produce “eigenflows”. Fang and Costen <cite ref='Fang2009'/> used <ac>PCA</ac> to encode facial feature position motions and Akak{<macro v='i}'/>n and Sankur <cite ref='Akakin2011'/> applied <ac>ICA</ac>, <ac>NMF</ac> and <ac>DCT</ac> on sequences of facial feature positions. Individual components of features in the eigenvector based methods correspond to deformations of part or all of the face. These deformations are found in an unsupervised fashion and are not necessarily optimal for recognition. Also, the order of events that are encoded in the sequences are either not encoded at all, or encoded in a way that is not appropriate for machine learning.</p>

<p>Another important feature processing stage is feature selection, which is discussed in Chapter <ref label='ChapterFeatureSelection'/>. The next section describes techniques for supervised classification based on features.</p>

</subsection>
</section>
<section title="Classification" label="BackgroundSupervisedClassification">

<figure caption="Supervised learning uses labelled training data to create a model that can make label predictions for unseen data." label="FigureSupervisedClassification">
<graphic>nvcclass/supervisedSystem.pdf</graphic>
</figure>

<p>One of the aims of this thesis is to create an effective automatic system for <ac>NVC</ac> recognition. After feature extraction and processing, the final step in an automatic system is often classification, which is the process of automatic prediction of a label based on a test sample observation. Many different classifiers have been applied to emotion, speech reading and gesture problems. This section provides an overview of the significant existing approaches to classification.</p>

<p>A supervised classifier is used to create a model based on labelled training data. This model can be used to predict labels for unseen test samples. The samples' features are usually the result of a <macro v='featureGeneration'/> process from raw observations such as video frames. In this case, a test or training sample is a video clip of one or more frames. Given a set of training and test data, the performance of a classifier can be evaluated by comparing its predictions for unseen samples to the test data labels, also known as “ground truth”. An illustration of this process is shown in Figure <ref label='FigureSupervisedClassification'/>. There is a vast number of classification techniques in the published literature. Specific disciplines often have a set of preferred techniques that have been found to be effective. This is because each classifier has different requirements and makes different assumptions about the problem's characteristics. Also, each <macro v='featureGeneration'/> technique has different properties, so features vary in suitability based on the problem and the specific classifier used. Rosenblum suggested that expressions with greater motion are easier to classify <cite ref='Rosenblum1996'/>. This section will focus on the use of classifiers in various facial classification applications and will discuss some of the classifier's properties in each context.</p>

<p>There are a few major families of classifiers, each requiring different formats for the input features and input labels. For this reason, it is not possible to apply every classification technique to every problem. The main families of classification techniques will now be discussed, along with the problems to which they have been applied.</p>

<subsection title="Single Instance Classifiers">

<p>Single instance classifiers operate on samples that are represented by a single feature vector and a label value. This reliance on a fixed length feature vector makes classification of variable length clips problematic, but this can be overcome by <macro v='featureGeneration'/> or decision level fusion. An early classifier was based on finding the $k$ nearest neighbours for a test sample <cite ref='Cover1967'/>. This approach is conceptually simple but it has high computer memory requirements. It also considers all feature components as equally significant, which can be problematic if some of the components of the feature vector are irrelevant. This may be the case for <ac>NVC</ac> and emotion recognition, because typically only part of the face is relevant in determining the label. Donato <macro v='etal'/> <cite ref='Donato1999'/> applied the nearest neighbour technique to facial expression classification. Nearest neighbour is appropriate for problems that have a large quantity of training data. Most classification techniques use a statistical model to approximate the decision boundary between different classes in feature space. One of the earliest classification techniques is <ac>FLD</ac>, which attempts to use a linear manifold to separate two classes. Linear discrimination was applied to facial expression by Rose <cite ref='Rose2006'/>. 
Unfortunately many problems in facial analysis require the use of non-linear decision boundaries to achieve acceptable performance.</p>

<p>A neural network is a machine learning technique based on an interconnected group of artificial neurons. Neural networks have high training requirements, and high computational and memory requirements. They also generate an internal model that is hard for humans to interpret and therefore only provide limited scientific insight. Their partitioning of feature space is also hard to grasp intuitively. However, neural networks have been used in a wide range of applications including speech reading <cite ref='Visser1999, Seguier2002'/>, emotion recognition <cite ref='Rosenblum1996, Petridis2009, Wollmer2009'/>, classifying laughter or no laughter <cite ref='Petridis2010'/> and many other non-facial analysis applications.</p>

<p>Boosting classifiers are a family of machine learning techniques that combine a set of weak learners into a single strong classifier. This is usually done iteratively and greedily, with weak learners being weighted and added to a bank of weighted weak learners. Many boosting approaches limited the weak learners to produce Boolean feature vectors and also require Boolean class labels. This makes some boosting methods less attractive if facial deformation features and <ac>NVC</ac> labels can be considered as continuous values. One popular boosting method is Adaboost <cite ref='Freund1996'/>, which is a binary classifier known to be sensitive to outliers <cite ref='Natsuki2008'/>. Adaboost was applied to emotion classification by Moore and Bowden <cite ref='Moore07'/> and He <macro v='etal'/> <cite ref='He2005'/>. Adaboost has been extended to temporal sequences and this is discussed in the later section on temporal model classifiers. The technical details of Adaboost are discussed in more depth in Section <ref label='SectionAdaboost'/>. Boosting methods often explicitly select a subset of features which is easy to interpret and this can be useful in finding the types of face deformations that are responsible for determining the label of a test sample.</p>

<p>A <ac full='yes'>SVM</ac> is a kernel based learning technique that attempts to learn a decision boundary that provides the maximal separation between positive and negative samples (Vapnick <cite ref='Vapnik1998'/>). In the input feature space, the boundary is non-linear and allows many complex problems to be addressed but at the risk of over-fitting, resulting in lower performance. The boundary model is based on adding weighted kernels centred at particular training samples. A test sample's distance from the boundary provides a mapping to a SVM space in which the problem is linearly separable. <ac>SVM</ac> was originally formulated for binary classification and have since been extended in various ways, including for regression, which is used later in Chapter <ref label='ChapterNvcRegression'/>.
The <ac>SVM</ac> method is popular and has been applied to facial analysis many times. It has been shown to be effective in emotion classification <cite ref='Goecke2006, Datcu2007, Seppi2008, Moore2009, Truong2009, Tariq2011, Jeni2011, Yang2011'/>, classifying pain or no pain <cite ref='Ashraf2007'/> and on <ac>AU</ac> based expression recognition <cite ref='Taheri2011, Girard2011'/>. However, <ac>SVM</ac>s do not provide an intuitive way to interpret their internal model, or to easily determine which features are most relevant. The technical details of the <ac>SVM</ac> is discussed in more depth in Section <ref label='SectionSupportVectorMachines'/>. A <ac full='yes'>RVM</ac> is a kernel technique that is closely related to <ac>SVM</ac> but uses <ac>EM</ac> to find a model to provide probabilistic label prediction. However <ac>EM</ac> does not necessarily find a globally optimal model but still is an effective classifier that results in a sparser model and is often seen to result in a higher performance when compared to <ac>SVM</ac> <cite ref='Nicolaou2011b, Xiangmin2007'/>. <ac>RVM</ac> was applied to classifying brow action samples as posed or spontaneous by Valstar <macro v='etal'/> <cite ref='Valstar2006'/>. <ac full='yes'>SVDD</ac> is another kernel method but in this case addresses the problem of modelling a single class's distribution. This was used by Zeng <macro v='etal'/> <cite ref='Zeng2006'/> to model samples in one class labelled as “emotional” and this was used to classify test samples as either emotional or unemotional. This one class approach is useful if it avoids the problem of attempting to model a class which has a  complex feature space occupancy.</p>

<p>A decision tree classifier learns a graph of simple rules that recursively divide the feature space along axis parallel planes <cite ref='Breiman1984'/>. Decision trees are fast to train and apply, they provide a simple way for human inspection of the internal model and allow feature relevance to be determined. Decision trees were applied by Hillard and Ostendorf <cite ref='Hillard03'/> to agreement and disagreement classification in audio. Decision trees have been extended to random forests <cite ref='Breiman2001'/> in an attempt to improve performance. Random forests use an ensemble of decision trees with each tree trained on a different subset of features, using the concept of bagging. Random forests produce a verbose model that is difficult to interpret, in comparison to decision trees, but relevant features can still be evaluated. Random forests have been applied to emotion classification on posed data <cite ref='Fanelli2010'/>.</p>

<p>While some approaches to facial analysis have used human designed <macro v='featureGeneration,'/> it is uncommon to attempt to manually create a classification model. However manual classification rules were created by Pantic and Patras <cite ref='Pantic2006'/> to classify <ac>AU</ac> based facial expression. This allows human technical and intuitive abilities to be applied to the task to produce a tailored model, but this might not be appropriate for complex feature space distributions or in cases where the relevant features are unknown.</p>

<p>The classifier methods in this section have been concerned with samples having a single feature vector. The following section considers classifiers that specialise in modelling temporal and sequential problems.</p>

</subsection>
<subsection title="Temporal, Model Level Classifiers">

<p>Temporal model classifiers are trained on samples that have been encoded as features that are an set of vectors. An ordered sequence of vectors is often based on sequential audio features, video frames or gestures in a clip of limited duration. This discussion will focus primarily on video classification, which is the topic of this thesis. Sequential temporal model classifiers attempt to model the temporal variation of features for each class. This model can then be used to predict a label for test samples. The most popular sequential classifier is the <ac full='yes'>HMM</ac>, which assumes a process can be characterised by transitions in a hidden state model. The transitions between hidden states are assumed to have a Markov property, which means the transition to the next state depends on the current hidden state but does not depend on previous hidden states. Each hidden state is associated with an emission model, which maps a hidden state onto a distribution of observable states. A separate <ac>HMM</ac> is trained for each class, including a class specific hidden state transition model and an emission model. For gesture recognition, the hidden states are usually discrete labels, and the emission model is typically a Gaussian distribution. If features that encode facial expressions are used, each hidden state corresponds with a distribution of facial expressions. A particular class would be characterised by the transitions from one distribution of facial expressions to the next.</p>

<p><ac>HMM</ac> is an elegant theory which has been successfully used in many applications. However, there are a number of disadvantages arising from model used and assumptions that are made. The hidden state model usually allows for self transitions to remain in a state. Because of the Markov property, the probability of remaining in one state reduces exponentially in time, and it is difficult to say if this would be appropriate for <ac>NVC</ac>, or not. The emission model is assumed to be a Gaussian distribution which, for facial features, makes certain assumptions as to the properties of facial expression. Many of these issues can be addressed by increasing the number of hidden states in the model, but this quickly increases the number of unknown parameters in the transition model and the emission model. <ac>HMM</ac>s typically operate best when there is a large quantity of training data available. The number of hidden states and the topology of the transitions are difficult to determine without manual adjustment, which makes the application of <ac>HMM</ac>s rather time consuming and heuristic. Also, <ac>HMM</ac>s use discrete class specific models and these cannot directly be used to predict continuous value labels.</p>

<p><ac>HMM</ac>s have been successful in the other form of human communication: verbal language recognition. Speech recognition often considers only the audible component but some studies have combined audio and visual facial analysis with <ac>HMM</ac>s to improve recognition <cite ref='Potamianos2001, Nefian2002'/>. Language recognition can even be attempted with visual only features, although previous studies have been restricted to a limited grammar <cite ref='Luettin1996, Gray1997, Matthews1998, Saitoh2005'/>. Another quasi-verbal mode of human communication is sign language, to which a <ac>HMM</ac> classifier has also been applied <cite ref='Starner1995'/>. Although almost entirely visual, sign language is generally not regarded as <ac>NVC</ac> because multiple gestures are combined to form complex meanings. The difference between sign language and <ac>NVC</ac> is that sign language has a grammar while <ac>NVC</ac> does not. Emotion is also thought not to have a grammatical structure <cite ref='Cohen2000'/>. This lack of grammar makes it unlikely that <ac>HMM</ac>s will be particularly effective for <ac>NVC</ac> recognition. On more constrained problems, such as posed emotion or posed facial expression that begins and ends on neutral expression, a clear pattern of expression onset (appearance), peak and offset (disappearance) <cite ref='Ekman1984'/> can be seen. This may serve as a grammar of sorts, which can be used by a <ac>HMM</ac> in facial expression classification <cite ref='Lien1998a'/> or emotion classification <cite ref='Cohen2000, Mower2009'/>. However, naturalistic emotion and <ac>NVC</ac> does not necessarily begin and end with neutral expression and this reduces the consistency on which sequential temporal model classifiers depend. el Kaliouby and Robinson <cite ref='Kaliouby2005'/> use <ac>DBN</ac>s, which are a generalisation of <ac>HMM</ac>s to classifying mental states. Their approach integrates modelling of different temporal and spatial scales into a single classifier.</p>

<p>Adaboost was extended to consider temporal sequences as the TemporalBoost classifier and was tested on facial gestures by Smith <macro v='etal'/> <cite ref='Smith2005'/>. This technique considers frames in a window of variable length, ending with the last frame of the sample. For each boosting iteration, the binary input features in the variable window have a logical “AND” and “OR” operator calculated, which are the values used for boosting. The variable window size is optimised to minimise the prediction error by gradient descent. As with most boosting methods, the trained model provides informative feature relevance information. Although this classifier considers multiple frames, it does not model how features vary in time in any direct sense. In testing, the order of frames in the window is not considered. Also the classifier is limited in that consecutive frames in a window must be considered, and the window end must correspond to the end of the clip, which makes it inappropriate if significant events occur at the start of a clip. As with Adaboost, TemporalBoost is limited to binary inputs and binary labels which makes it less relevant to continuous value problems.</p>

<p>A <ac full='yes'>CRF</ac> is a class of machine learning tools that, unlike conventional classification, which considers test samples independently, the label assignment of each sample considers the labels of “nearby” samples, which when applied to temporal data corresponds to nearby in time. In classification of video, this can enable a video frame to be classified based on adjacent frame labels. Hidden <ac>CRF</ac> extends this with the addition of a latent, unobserved state. This method, like a <ac>HMM</ac>, has discrete state transitions that are usually considered over short time periods. This may not be suitable for gradually varying human emotion and <ac>NVC</ac> signals. Morency used <ac>CRF</ac> to predict and recognise the behaviour of a human listener <cite ref='Morency2011'/>. Bousmalis <macro v='etal'/> <cite ref='Bousmalis2011'/> used <ac>HCRF</ac> to classify agreement and disagreement in political debates. This was later extended to <ac>iHCRF</ac> <cite ref='Bousmalis2012'/>, which is capable of automatically finding the optimal number of hidden states. Rudovic <macro v='etal'/> <cite ref='Rudovic2012'/> extended the <ac>CRF</ac>-like <ac>CORF</ac> method <cite ref='Kim2010'/> to take account of the ordinal relations between onset, apex and offset for <ac>AU</ac> recognition.</p>

<p>Sequential classifiers consider samples that contain a set of ordered video frames. However, the following section will consider classifiers that specialise in classification of unordered sets, which may be appropriate for “grammar free” <ac>NVC</ac>.</p>

</subsection>
<subsection title="Multiple Instance and Trajectory Classifiers">

<p><ac full='yes'>MIL</ac> considers classification of a set or “bag” containing one or more vectors or “instances” <cite ref='Maron1998'/>. This may be applicable to <ac>NVC</ac> recognition, because it is possible that the presence of a single face configuration to be important in determination of the predicted label. If a particular area of feature space is critical to determine the label, it is known as a “concept”, which corresponds to a specific face configuration. However, <ac>MIL</ac> places minimal constraints or assumptions on the nature of the data, so there is no common <ac>MIL</ac> approach. There are rather many different <ac>MIL</ac> algorithms that have been proposed for domain specific problems. Also, it is unclear whether a single exemplar or concept is sufficient to encompass a range of naturally occurring <ac>NVC</ac> signals. Tax <macro v='etal'/> <cite ref='Tax2011'/> applied <ac>MIL</ac> to emotion to find concepts that correspond to each class and applied the concept model to emotion classification. The face configurations corresponding to these concepts were not shown in the paper.</p>

<p>Time varying facial expression can be represented as a trajectory in feature space. Instead of attempting to model a class's trajectories' temporal pattern, Akak{<macro v='i}'/>n and Sankur <cite ref='Akakin2011'/> directly compared trajectories using a modified nearest neighbour approach. The mean and median distance from a test trajectory to a class's trajectories was summed to produce a similarity score. As with conventional nearest neighbour classification, this method can be sensitive to irrelevant features, so this approach may benefit from feature selection.</p>

<p>The discussion so far has focused on papers that apply a classification method to facial analysis. The next section discusses papers that compare different classification methods for a specific application.</p>

</subsection>
<subsection title="Published Comparisons of Classifier Methods">

<p>Various classification methods have been discussed, as well as their properties with respect to facial analysis and <ac>NVC</ac>. However, it is difficult to discuss their relative performance without studies that directly compare classifiers on the same task. The best classifier for a problem is task specific, so these comparisons on emotion recognition provide circumstantial evidence that certain classifiers will be effective for <ac>NVC</ac> recognition. However, this expectation is only by analogy and needs to be confirmed experimentally. Emotion and <ac>NVC</ac> recognition are often interrelated but they are distinct problems.</p>
 
<p>Sun <macro v='etal'/> <cite ref='Sun2004'/> compared many classifiers for emotion recognition based on shape based, model fitting feature extraction. Their comparison included various Bayesian classifiers, variants of decision trees, <ac>kNN</ac>, <ac>SVM</ac>, etc. The authors were surprised that k-nearest neighbour classifier was most effective for this task. This suggests that the decision boundary between emotion classes is non-linear. Bagging and boosting, as an additional measure, was found to be beneficial to statistical model based classifiers. The large quantity of training data available for this application also makes <ac>kNN</ac> more suitable in this case. Shan <macro v='etal'/> <cite ref='Shan2009'/> used various classifiers for emotion recognition based on appearance based <ac>LBP</ac> and Gabor features. The classifiers used were nearest neighbour, <ac>SVM</ac> and Adaboost. In this case, <ac>SVM</ac> was the most effective method, with nearest neighbour being the worse in performance. As well as providing another example of how different classifiers are effective for different tasks, the authors stress that effective <macro v='featureGeneration'/> is critical to achieving good performance. Various <ac>SVM</ac> kernels were compared and the <ac>RBF</ac> kernel was found to have the best performance and generalisation. W<macro v='öllmer'/> <macro v='etal'/> <cite ref='Wollmer2008'/> used audio features to classify emotional activation and valence. <ac>CRF</ac> performance exceeded <ac>SVM</ac> performance for classification. However, a regression approach to this task may be more appropriate than the use of classification.</p>

<p>Other comparisons of classifiers for facial analysis have been conducted that do not use emotion labels. Petridis <macro v='etal'/> <cite ref='Petridis2009'/> compares static and temporal model classifiers for smile classification based on shape features. When considering frame based features, there was no significant difference between sequential and non-sequential classifiers. For window based features, static classification exceeds the performance of sequential classification. This suggests that <macro v='featureGeneration'/> that encodes temporal information is more important than temporal modelling by a classifier for this task. Escalera <macro v='etal'/> <cite ref='Escalera2009'/> used appearance and detection based features on conversation dominance labelled data. The compared classifiers were Discrete Adaboost and <ac>SVM</ac> (with <ac>RBF</ac> and linear kernels) with Adaboost having the best performance and generalisation. Tax <macro v='etal'/> <cite ref='Tax2011'/> used tracking based features to classify facial expression. Several classifiers were used including <ac>LDA</ac>, diverse density <cite ref='Maron1998'/> (a <ac>MIL</ac> method), a <ac>MIL</ac> clustering based method, <ac>HMM</ac> and <ac>CRF</ac>. The performance for several different <ac>AU</ac>s is shown but there is no single classifier that is optimal for all.  In some cases a sequential classifier is useful, sometimes a <ac>MIL</ac> classifier is optimal and in other cases, a non-sequential classifier is best. A similar mixed result of performances was found by Akak{<macro v='i}'/>n and Sankur <cite ref='Akakin2011'/> in use of various features to recognise head gestures. Their results are discussed in more depth in Section <ref label='SectionHmm'/>, because their paper is partly a response to results in Chapter <ref label='ChapterClassification'/>. There was no clear winner between sequential and <ac>MIL</ac>-like approaches. However, they found that better performance could be gained by decision level fusion of different classifiers. Pfister <macro v='etal'/> <cite ref='Pfister2011, Pfister2011b'/> used a temporal appearance based feature to attempt to classify masked emotion from microexpressions. In this study, <ac>MKL</ac> provided a performance advantage over <ac>SVM</ac> in classifying emotion into present or not-present. However, random forests were best at detecting the presence of micro-expressions from a background of neutral expression. The experimental conditions of this study were different from natural social situations, so it is hard to draw firm conclusions on the best approach to <ac>NVC</ac> recognition. Rose <cite ref='Rose2006'/> compared multi-class classification to single class recognisers for expression recognition and found that linear discrimination and kernel based multiclass classifiers was most effective. Rudovic <macro v='etal'/> <cite ref='Rudovic2010'/> compared <ac>RVM</ac>s, <ac>SVM</ac>s, linear regression and Gaussian Process Regression on the CMU Multi-PIE facial expression database and found that <ac>RVM</ac>s combined with pose normalisation was most effective. Buciu <macro v='etal'/> <cite ref='Buciu2009'/> compared the cosine similarity matrix with <ac>SVM</ac>s for expression recognition, based on various <ac>ICA</ac> approaches. They found that different <ac>ICA</ac> methods reduced mutual energy in basis images, which was negatively correlated with recognition performance.</p>

</subsection>
</section>
<section title="Conclusion">

<p>There are a few conclusions that can drawn from the literature. There is no single approach, be it a <macro v='featureGeneration'/> method, temporal analysis method or classifier that is suitable for all problems. There are certainly differences in popularity of classifiers, with <ac>HMM</ac> and <ac>SVM</ac> being dominant in facial analysis, while decision tree based methods are relatively rare. It is also frequently observed that for a set of behaviours, a subset is easier to automatically recognise than others. The next chapter describes the collection of a corpus of informal conversations that is suitable for training an automatic system.</p>

</section>
</chapter>

