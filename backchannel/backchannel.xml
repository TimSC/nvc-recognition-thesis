<?xml version="1.0" encoding="UTF-8"?>
<chapter title="Interpersonal Coordination in NVC" label="ChapterBackchannel" label2="SectionBackchannelIntro">
<quote>
The meeting of two personalities is like the contact of two chemical substances: if there is any reaction, both are transformed.<br/>
Carl Jung
</quote>

<p>The previous chapter considered automatic <ac>NVC</ac> recognition based on visual information of the sender or “encoder” of an <ac>NVC</ac> signal (encoder in this case does not refer to the annotators or automatic encoding but rather to the observed subject exhibiting a behaviour <cite ref='Lanzetta1970'/>). This chapter considers the behavioural associations between an <ac>NVC</ac> encoder and the <ac>NVC</ac> perceiver, and describes a study of recognising <ac>NVC</ac> signals based on the behaviour of the <ac>NVC</ac> perceiver. In natural two person conversations, the behaviour of one participant influences the other participant. 
Usually, people take turns to speak in a conversation. In a dyadic (two person) conversation, the information flow is bi-directional. In a particular speech turn, the speaker's communication is referred to as the “forward” channel. The person listening to the speaker also responds to the speaker, primarily using non-verbal communication, and this communication is known as the “backchannel”. Backchannel communication allows a listener to influence a conversation without verbally interrupting or taking a conversation turn. The forward and backchannel communications occur simultaneously and can vary in style across cultures. For this reason, both subjects in a dyadic conversation can be thought of as both an encoder and perceiver of <ac>NVC</ac> simultaneously and this is bi-directional flow of information exploited in this chapter.</p>

<p>In natural conversations, participants often mimic or mirror behaviours being expressed by the other person. Mimicry or “behaviour matching” occurs when two or more people show similar body configurations <cite ref='Bernieri1991'/>. For example, if one person touches their face during a conversation, the other person is more likely to do so. 
Another class of behaviours is performed in a common rhythm, with the starting and ending of movement being simultaneous. This is known as “synchrony”, which comprises of corresponding rhythms, simultaneous movement and the smooth meshing of interaction <cite ref='Bernieri1991'/>. Together, synchrony and behaviour matching are considered as “interpersonal coordination”. The phenomena of behaviour mirroring and synchrony are not disjunct and can occur at the same time <cite ref='Delaherche2012'/>. These effects raises the possibility that a perceiver's behaviour in response to an <ac>NVC</ac> signal may be useful in <ac>NVC</ac> recognition. Based on this idea, backchannel information for automatic <ac>NVC</ac> recognition is studied and evaluated.</p>

<p>The main contributions of this chapter are:</p>

<ul>
<li>A method for automatic identification of certain types of interpersonal coordination in the deformation of the face during casual conversation. This method is applied to recorded videos and the results are analysed.</li>
<li>A study of <macro v='featureGeneration'/> methods for <ac>NVC</ac> classification that identifies <ac>NVC</ac> signals observed in the forward channel, based solely on backchannel feature data.</li>
</ul>

<p>The next section outlines previous studies that are related to this work. Section <ref label='SectionMirrorBehaviour'/> investigates the inter-person coupling of face deformations in conversation. A automatic system to classify <ac>NVC</ac> based on backchannel cues is described in Section <ref label='SectionClassificationBackchannel'/>.</p>

<section title="Related Research" label="BackgroundNvcIsCoupled">

<p>Backchannel is a form of human communication which has been relatively little studied. The first mention of it is probably by Yngve <cite ref='Yngve1970'/> where he observed that the backchannel allows a listener to express non-verbal or brief verbal speech to influence a conversation, without taking a conversational turn. Even prior to this, gaze was thought to be used in conversation regulation by Kendon <cite ref='Kendon1967'/>. Backchannel signals provide a way for a listener to signal agreement or disagreement, as noted by Bousmalis <macro v='etal'/> <cite ref='Bousmalis2009'/>. Backchannels differ across cultures, with some cultures having a higher frequency of backchannel signals <cite ref='White1989'/>.</p>

<p>Human behaviour may also be coupled during conversations, besides consciously performed communication acts. Various studies have tried to quantify this behaviour (see Reidsma <macro v='etal'/> <cite ref='Reidsma2010'/> and Delaherche <cite ref='Delaherche2012'/> for a review). Chartrand and Bargh <cite ref='Chartrand1999'/> found that people in conversation tend to adopt the same posture, mannerisms and expression. Similar observations have been found for limb movement, and many aspects of speech, but these effects are dependent on social context <cite ref='Frith2009'/>. 
Several studies have used automatic recognition of synchrony (see <cite ref='Delaherche2012, Sun2012'/> for reviews). There is a many different feature extraction techniques and measures of synchrony. Richardson <macro v='etal'/> <cite ref='Richardson2007'/> studied gaze and noted that when two subjects are observing the same scene, their gaze motion is coupled. They found that behaviour was most strongly coupled between synchronised inter-person movements, but also found evidence of gaze coupling occurring after a delay of up to about 3 seconds. The social situation has an impact on the types of mimicry that occur <cite ref='Bourgeois2008'/>. Coupled human behaviour may be useful for automatic recognition. Until recently, there have been no approaches that use backchannel communication. Morency <cite ref='Morency2011'/> studied automatic recognition based on backchannel signals and attempted to predict listener responses to a speaker. Okwechime <macro v='etal'/> <cite ref='Okwechime2011'/> used a-priori data mining to find behaviour patterns in conversation and considered both audio and visual modalities. This study was performed in controlled social situations to elicit interested or disinterested behaviours by the listeners and social context based behaviour differences were identified.
Ramseyer and Tschacher <cite ref='Ramseyer2008'/> used correlations in difference images to find cross-correlated patterns with a time offset of $<macro v='pm'/>$5 sec. They used time shuffled windows to produce non-coordinated behaviour data to determine the extent of cross correlation cause by the null hypothesis. No existing studies consider automatic detection of interpersonal coordination in facial behaviour, although Sun <macro v='etal'/> are working towards this goal <cite ref='Sun2012'/>.</p>

<p>The MAHNOB HMI iBUG Mimicry dataset (shortened to MHi-Mimicry-db) <cite ref='Sun2011'/> contains recordings of dyadic conversations in role play and discussion situations for the purposes of analysing mimicry behaviour. This dataset is significantly larger and more complex than the TwoTalk corpus, containing about 12 hours of records of 40 subjects using 15 video cameras and 3 audio channels. The corpus was manually annotated to produce labels to investigate how mimicry is expressed, if it is intentional or unintentional and the social signal being expressed. Intentional signals were also labelled in terms of the goal of the mimicry. The camera synchronised error was “well below” 20 $<macro v='mu'/>$sec. The two social situations contained in the MHi-Mimicry-db were a political discussion with a confederate and a role play concerning renting a room to a potential lodger. These scenarios were selected to test specific existing hypothesises of human behaviour. Both were dyadic conversations were conducted in a laboratory. The MAHNOB Mimicry dataset was not available at the time this part of the study was conducted. The following section focuses on interpersonal coordination that may be automatically identified without the use of annotation.</p>

</section>
<section title="Coupling in Interpersonal Coordination during Informal Conversation" label="SectionMirrorBehaviour">

<p>This section describes an automatic method to analyse coupling in facial behaviour. Because this correlation based method is sensitive to both rhythmic and non-rhythmic behaviours, certain types of both mimicry and synchrony behaviours will be detected. Correlation being used as a measure of mimicry can be understood in terms of it's mirroring property. Two people tend to reflect or mirror each others body positions and behaviours. By analogy, correlation is a way of measuring the  extent of “mirroring” between two variables. This makes the Peason's correlation suitable for finding some types of coupled behaviours. A more detailed justification of using Pearon's correlation is provided in Appendix <ref label='ChapterMimicryColleration'/>.</p>

<p>Correlation of simultaneous frame features is not sensitive to types of mimicry in which the response behaviour is delayed for a longer time than the duration of the trigger behaviour. The class of behaviours that can be detected by the approach described here will be referred to as “coupling”. Automatic methods can provide a more comprehensive analysis than can be achieved by manual methods because automatic approaches can scale to large quantities of video data.
The next section describes the method, based on tracking, feature extraction and Pearson correlation. Section <ref label='SectionCouplingResults'/> shows the results and discusses their significance.</p>

<subsection title="Methodology">

<p>Using the four dyadic conversations recorded as part of the corpus (see Section <ref label='SectionDescriptionOfTwoTalkCorpus'/>), behaviour patterns that are common to both participants can be identified. The cameras used to record the corpus are genlocked to ensure synchronisation of video frames. Genlock is a common technique to synchronise cameras using a generator signal. The genlock signal was provided by a Tektronix TG700 Multi-Format Video Generator. The cameras were all of the same type; this greatly simplifies achieving accurate synchronisation. Based on informal tests performed by the broadcast engineer responsible for maintaining the system, the camera jitter measured to be was less than one microsecond (the level of jitter in the camera synchronisation was smaller than measurable using the available equipment). The features introduced in the previous chapter are used, <i>geometric-a</i>, which are based on distances of pairs of feature trackers (Section <ref label='SectionGenerateAlgorithmic'/>). These features are applied to the face and non-facial behaviour is not considered.
Clearly, some areas of the face will not be coupled e.g. when one person speaks, it is usual for the other person to listen and not to move their mouth in the same way. To identify coupled behaviour, the variation of each feature for both speakers in a conversation are compared using Pearson's correlation $<macro v='correlFunc'/>$ (see Equation <ref label='EqnPearsonsCorrelation'/>). If features are highly correlated, this indicates that they are closely coupled. A low correlation indicates the features vary independently. As the correlation is performed on simultaneous frames, it will only capture mimicry between individuals if the offset between the occurrences is less than its duration. While this approach is insensitive to more delayed mimicry, it should identify shorter term mirroring of behaviour (see Appendix <ref label='ChapterMimicryColleration'/>). The correlation score corresponds to the strength of coupling of the behaviour. The original video records are used because it increases the quantity of data available for analysis. The feature vectors $<macro v='frameFeatureMatrix'/>$ of two participants $A$ and $B$ are compared $A, B <macro v='in'/> \{1008, 1011, 2008, 2011, 3008, 3011, 6008, 6011\}$. For feature component $i <macro v='in'/> \{0...<macro v='numFeatures'/>\}$, the correlation $<macro v='dyadicCorrelation'/>$ is ($<macro v='frameFeatureMatrix'/>^A_i <macro v='in'/> <mathbb>R</mathbb>^{<macro v='numSeqFrames'/>}$, $<macro v='numSeqFrames'/> = 12 <macro v='times'/> 60 <macro v='times'/> 25 frames$, $<macro v='dyadicCorrelation'/>^{A,B}_i <macro v='in'/> <mathbb>R</mathbb>$):</p>

eqn\begin{gather}
eqn\dyadicCorrelation^{A,B}_i = \correlFunc(\frameFeatureMatrix^A_i, \frameFeatureMatrix^B_i)
eqn\end{gather}

<p>If a behaviour is performed as a reaction, there may be a time delay between the behaviours. Each frame of one participant is compared to a simultaneous frame of the other participant taken using a second camera. This makes this approach sensitive to synchronised behaviours but insensitive to mirroring behaviours that have a temporal offset and short duration. For example, if one person adopts an expression for several seconds and the second person quickly adopts the same expression, this will be found using the correlation of features. The correlation coefficient is limited to measuring linear relationships. It is quite possible that reliable, non-linear patterns of human behaviour may exist. Another limitation of this approach is that this considers only variations in shape but it is possible that patterns in human behaviour may be found if speed or acceleration of the face is also considered.</p>

<p>For this test, corresponding features are compared (e.g. position of left eye in both participants) rather than different areas on the face (e.g. left eye for one subject and mouth opening for the second subject). This constraint makes the system focus primarily on mirroring behaviour. Because the strongly coupled areas of the face are most interesting, the highest scoring feature $i^{A,B}_{max}$, with correlation $<macro v='dyadicCorrelation'/>^{A,B}_{max}$, is found:</p>

eqn\begin{gather}
eqni^{A,B}_{max} = \argmax_{i}{\dyadicCorrelation^{A,B}_i}
eqn\end{gather}

<p>This feature corresponds to a specific area on the face in which the behaviour is coupled. However, there is a possibility that features vary and may coincidentally vary together. It can be difficult to distinguish weak causal relationships with coincidental inter-relationships. To examine this possibility, the correlations of features $<macro v='dyadicCorrelation'/>_{max}$ for individuals that were not in the same conversation was also computed. Apparent correlations between unrelated conversations can be regarded as coincidental.</p>

<p>This method is similar to Ramseyer and Tschacher <cite ref='Ramseyer2008'/> but contains some important differences:</p>

<ul>
 <macro v='item'/> This study uses face shape based on tracking rather than difference images of whole people. This makes our approach more effective in localising coupling behaviour to a specific area of the face. However, no other areas of the body are considered in this study.
 <macro v='item'/> This study uses recordings of a different person to test the null hypothesis correlations, rather than a shuffled window approach. However, this should not give rise to a significant difference in measured performance.
 <macro v='item'/> Ramseyer and Tschacher considered time offset signals, which enabled them to see which person is leading and which person is following in behaviour. This is not attempted in this study.
</ul>

<p>The current section only aims to show that interpersonal coordination exists for some areas of the face in dyadic informal conversation. Therefore, this study consider pairs of participant independently but without attempting to find consistent behaviour patterns that occur in all dyads. This approach of considering dyads independent and without finding corresponding behaviours across dyads was also used by Ramseyer and Tschacher <cite ref='Ramseyer2008'/>. 
Although attempting to find consistent behaviours from multiple dyads would be an interesting study, it is beyond the scope of this thesis.</p>

</subsection>
<subsection title="Results and Discussion" label="SectionCouplingResults">

<table caption="Maximum correlation $&lt;macro v='dyadicCorrelation'/&gt;_{max}$ of corresponding facial shape features for different pairs of participants. Pairs that were participating in the same conversation are highlighted." label="TableBackchannelShapePairs">
<tr><td></td><td> 1008 </td><td> 1011 </td><td> 2008 </td><td> 2011 </td><td> 3008 </td><td> 3011 </td><td> 6008 </td><td> 6011 </td></tr>
<tr><td>1008 </td><td highlight="yes">1.00 </td><td highlight="yes">0.25 </td><td> 0.12 </td><td> 0.11 </td><td> 0.15 </td><td> 0.13 </td><td> 0.08 </td><td> 0.10</td></tr>
<tr><td>1011 </td><td highlight="yes"></td><td highlight="yes">1.00</td><td> 0.19</td><td> 0.13</td><td> 0.18</td><td> 0.10</td><td> 0.15</td><td> 0.21</td></tr>
<tr><td>2008 </td><td> </td><td> </td><td highlight="yes">1.00</td><td highlight="yes">0.38</td><td> 0.11</td><td> 0.16</td><td> 0.12</td><td> 0.14</td></tr>
<tr><td>2011 </td><td> </td><td> </td><td highlight="yes"></td><td highlight="yes">1.00 </td><td> 0.14</td><td> 0.18</td><td> 0.13</td><td> 0.12</td></tr>
<tr><td>3008 </td><td> </td><td> </td><td> </td><td> </td><td highlight="yes">1.00 </td><td highlight="yes">0.34</td><td> 0.09</td><td>0.20</td></tr>
<tr><td>3011 </td><td> </td><td> </td><td> </td><td> </td><td highlight="yes"></td><td highlight="yes">1.00</td><td> 0.09</td><td> 0.09</td></tr>
<tr><td>6008 </td><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td><td highlight="yes">1.00</td><td highlight="yes">0.38</td></tr>
<tr><td>6011 </td><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td><td highlight="yes"></td><td highlight="yes">1.00</td></tr>
</table>

<p>The maximum correlation $<macro v='dyadicCorrelation'/>_{max}$ of different pairings of subjects is shown in Table <ref label='TableBackchannelShapePairs'/>. The conversation pairs are shown in Table <ref label='ParticipantDemographics'/>. There is a weak but significant correlation (between 0.25 and 0.38) for pairs involved in the same conversation. This confirms our expectation that there are interpersonal coordinated behaviours for corresponding areas of the face. Comparing this to conversation pairings in which the correlation would be coincidental, the correlation is found to be consistently lower than 0.25. This implies that the coupling of facial behaviour in conversation is above the level of correlation due to coincidental matches. Not all highlighted conversation pairs have the same level of correlation. This may be due to different relationships and communication styles between conversation participants. There may be person specific differences in their tendency to couple behaviour, as found by Chartrand and Bargh <cite ref='Chartrand1999'/>.</p>

<figure short="Histogram of correlation for corresponding algorithmic geometric features that originate from &lt;i&gt;different&lt;/i&gt; conversations." caption="Histogram of correlation for corresponding algorithmic geometric features that originate from &lt;i&gt;different&lt;/i&gt; conversations. The standard deviation is 0.048 and the variance is 0.002." label="FigureCorrelationHist">
<graphic width="0.8">backchannel/correlationHist.pdf</graphic>
</figure>

<p>The statistical significance of the best correlated features can be confirmed by characterising the distribution of correlation performance scores for the null hypothesis and use it to calculate the p-value for the hypothesis of interest. The possibility of a null hypothesis can be discounted if this p-value is lower than the desired significance level $<macro v='alpha'/>$ and can then be considered as statistically significant (see Hayes <cite ref='Hayes2005'/>). Because multiple hypothesises are tested, $<macro v='alpha'/>$ is adjusted using the Bonferroni correction to account for the greater chance of finding coincidental patterns. Any correlations between feature pairs that originate from separate conversations are due to chance. The distribution of correlation scores for null hypothesis pairings is shown in Figure <ref label='FigureCorrelationHist'/>. The standard deviation of correlation scores from null hypothesis pairings is $<macro v='sigma'/>=0.048$ and the variance is $<macro v='sigma'/>^2=0.002$. The chance correlation distribution is zero centred $<bar><macro v='correlFunc'/></bar>=0$. The p-value of a correlation score $<macro v='correlFunc'/>$ is calculated by a z-test. A z-test is used because the standard deviation of all possible null hypothesis is known. For a single observation compared to a zero centred population $<bar><macro v='correlFunc'/></bar>=0$, the z-test score is defined as:</p>

eqn\begin{gather}
eqnz = \frac{\correlFunc}{\sigma}
eqn\end{gather}

<p>For the highest correlation by chance, $<macro v='correlFunc'/>=0.21$, the corresponding z-score is $z=-0.21/0.048=−4.4$ and p-value is $2<macro v='times'/><mathcal>N</mathcal>(−4.4)=1.2<macro v='times'/>10^{-5}$ where $<mathcal>N</mathcal>$ is the cumulative normal distribution. The double tailed score is used because the modulus of the correlation is used to determine the largest magnitude correlation. The standard significance level $<macro v='alpha'/>$ is usually chosen to be either $0.01$ or $0.05$. The more stringent level $<macro v='alpha'/>=0.01$ is used in this chapter. Each of the correlation scores use the maximum value, based on 1035 comparisons (see Section <ref label='SectionGenerateAlgorithmic'/>). The Bonferroni corrected $<macro v='alpha'/>$ is calculated as $<macro v='alpha'/>_{adjusted} = #frac{0.01}{1035}=9.7<macro v='times'/>10^{-6}$. The p-value of $1.2<macro v='times'/>10^{-5}$, observed in the validation experiments, is within this threshold and cannot be considered as a statistically significant finding, as should be expected.</p>

<p>For pairings in which are engaged in the same conversation, patterns that are clearly statistically significant are expected. The correlation scores for these conversations vary from $<macro v='correlFunc'/>=0.25$ to $<macro v='correlFunc'/>=0.38$. This corresponds to p-values between $2<macro v='times'/><mathcal>N</mathcal>(-0.25/0.048)=1.9<macro v='times'/>10^{-7}$ and $2<macro v='times'/><mathcal>N</mathcal>(-0.38/0.048)=2.4<macro v='times'/>10^{-15}$, respectively. These are well below the Bonferroni corrected significance level of $9.7<macro v='times'/>10^{-6}$. The null hypothesis may therefore be rejected for these tests and they are therefore statistically significant.</p>

<figure short="Corresponding facial distances found to be most coupled in natural conversation for two of the conversations, marked in green." caption="Corresponding facial distances found to be most coupled in natural conversation for two of the conversations, marked in green. The top row is conversation 1008-1011. The bottom row is conversation 3008-3011." label="BackChannelCoupledPose">
<graphic width="0.49">backchannel/1008.pdf</graphic>
<graphic width="0.49">backchannel/1011.pdf</graphic><br/>
<graphic width="0.49">backchannel/3008.pdf</graphic>
<graphic width="0.49">backchannel/3011.pdf</graphic>
</figure>

<figure short="Corresponding facial distances found to be most coupled in natural conversation for two of the conversations, marked in green." caption="Corresponding facial distances found to be most coupled in natural conversation for two of the conversations, marked in green. The top row is conversation 2008-2011. The bottom row is conversation 6008-6011." label="BackChannelCoupledMouth">
<graphic width="0.49">backchannel/2008.pdf</graphic>
<graphic width="0.49">backchannel/2011.pdf</graphic><br/>
<graphic width="0.49">backchannel/6008.pdf</graphic>
<graphic width="0.49">backchannel/6011.pdf</graphic>
</figure>

<p>The two areas of the face found to be coupled in natural conversation are shown in Figures <ref label='BackChannelCoupledPose'/> and <ref label='BackChannelCoupledMouth'/>. The areas shown in Figure <ref label='BackChannelCoupledPose'/> seem to involve vertical distances which are generally rigid and likely encode head pitch. In contrast, the second group's relevant areas, shown in Figure <ref label='BackChannelCoupledMouth'/>, are more related to the mouth and perhaps relate to mutual smiling or some other mouth related expression.
The next section attempts to find patterns in facial behaviour that are not necessarily related to corresponding areas of the face.</p>

</subsection>
<subsection title="Coupling of Shape for Non-corresponding Facial Features">

<table short="Maximum correlation $&lt;macro v='dyadicCorrelation'/&gt;_{max}$ of &lt;b&gt;facial shape features&lt;/b&gt; for different pairs of participants (including corresponding and non-corresponding features)." caption="Maximum correlation $&lt;macro v='dyadicCorrelation'/&gt;_{max}$ of &lt;b&gt;facial shape features&lt;/b&gt; for different pairs of participants (including corresponding and non-corresponding features). Pairs that were participating in the same conversation are highlighted.">
<tr><td></td><td> 1008 </td><td> 1011 </td><td> 2008 </td><td> 2011 </td><td> 3008 </td><td> 3011 </td><td> 6008 </td><td> 6011 </td></tr>
<tr><td>1008 </td><td highlight="yes">1.00 </td><td highlight="yes">0.30 </td><td> 0.18 </td><td> 0.15 </td><td> 0.23 </td><td> 0.22 </td><td> 0.15 </td><td> 0.18</td></tr>
<tr><td>1011 </td><td highlight="yes"></td><td highlight="yes">1.00</td><td> 0.25</td><td> 0.22</td><td> 0.29</td><td> 0.23</td><td> 0.21</td><td> 0.23</td></tr>
<tr><td>2008 </td><td> </td><td> </td><td highlight="yes">1.00</td><td highlight="yes">0.43</td><td> 0.17</td><td> 0.23</td><td> 0.17</td><td> 0.18</td></tr>
<tr><td>2011 </td><td> </td><td> </td><td highlight="yes"></td><td highlight="yes">1.00 </td><td> 0.22</td><td> 0.19</td><td>0.18 </td><td>0.19 </td></tr>
<tr><td>3008 </td><td> </td><td> </td><td> </td><td> </td><td highlight="yes">1.00 </td><td highlight="yes">0.49</td><td> 0.18</td><td> 0.29</td></tr>
<tr><td>3011 </td><td> </td><td> </td><td> </td><td> </td><td highlight="yes"></td><td highlight="yes">1.00</td><td> 0.21</td><td> 0.16</td></tr>
<tr><td>6008 </td><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td><td highlight="yes">1.00</td><td highlight="yes">0.42</td></tr>
<tr><td>6011 </td><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td><td highlight="yes"></td><td highlight="yes">1.00</td></tr>
</table>

<p>The previous section considered the interpersonal coordination of facial behaviour for corresponding areas of the face. However, facial behaviour coupling between people may be expressed in different facial areas. This falls within the definition of synchrony, for which “the important element is the timing, rather than the nature of the behaviours” <cite ref='Delaherche2012'/>. Synchrony can involve coordinated times of different forms of behaviour, e.g. beginning to speak when another person stops speaking. This phenomena has been explored to some extent by Ramseyer and Tschacher <cite ref='Ramseyer2008'/>, who did not use corresponding parts of the body but instead searched entire images for any movement synchrony. To find correlations for this broader problem, the feature component $i$ for subject $A$ is compared to feature component $j$ for subject $B$. Because each feature component corresponds to a different area of the face, this searches for relationships in shape between non-corresponding facial areas. The correlation between facial components $i$ and $j$ ($i <macro v='in'/> \{0...<macro v='numFeatures'/>\}$ and $j <macro v='in'/> \{0...<macro v='numFeatures'/>\}$) defined as $<macro v='dyadicCorrelation'/>^{A,B}_{i,j} <macro v='in'/> <mathbb>R</mathbb>$ for subject $A$ and $B$:</p>

eqn\begin{gather}
eqn\dyadicCorrelation^{A,B}_{i,j} = \correlFunc(\frameFeatureMatrix^A_i, \frameFeatureMatrix^B_j) \\
eqn(i_{max}, j_{max}) = \argmax_{i,j}{\dyadicCorrelation^{A,B}_{i,j}}
eqn\end{gather}

<p>Again, the components $i_{max}, j_{max}$ with the highest correlation $<macro v='dyadicCorrelation'/>^{A,B}_{i,j}$ are determined. The result of this process is shown in Table <ref label='TableBackchannelShape'/>. The correlation for pairs in the same conversation is consistently above the correlation of coincidence matches. The standard deviation of correlation scores from null hypothesis pairings is $0.047$ and the variance is $<macro v='sigma'/>^2=0.002$, which is similar to the case of chance correlations if only corresponding features are considered (see Figure <ref label='FigureCorrelationHist'/>). More comparisons are performed to obtain the maximum correlation for each pair of subjects. This results in a more stringent Bonferroni adjusted $<macro v='alpha'/>$, which is calculated as $<macro v='alpha'/>_{adjusted} = #frac{0.01}{T_{1035-1}}=1.87<macro v='times'/>10^{-8}$ where $T_i$ is the $i$th triangular number. For pairs of subjects in engaged in conversation, the maximum correlation for each pair is 0.30, 0.43, 0.49 and 0.42. Based on the z-test, these correlations have a p-value of $1.7<macro v='times'/>10^{-10}$, $5.75<macro v='times'/>10^{-20}$, $1.90<macro v='times'/>10^{-25}$ and $4.03<macro v='times'/>10^{-19}$ respectively. The first is relatively near the significance boundary, while the latter three results are clearly statistically significant. There may be because the broader range of facial areas considered includes a wider range of human expression, so instead of just co-occurrence behaviours, other strongly coupled communication relationships can be detected. Because a greater number of feature components are compared, there is an increased possibility for coincidental matches. This is evident in the table because the off axis pairings show an increase in correlation compared to Table <ref label='TableBackchannelShapePairs'/>. These results indicate that some facial behaviour is coupled and this coupling is stronger if non-corresponding facial areas are also considered. Although the correlation scores are above chance occurrence, they are not perfectly consistent patterns of human behaviour. This does not make this patterns uninteresting or insignificant because most inter-personal behaviours are only general patterns; identification of deterministic patterns in human behaviour are not the norm.</p>

</subsection>
<subsection title="Coupling of Facial Shape Activity for Non-corresponding Facial Features">

<table label="TableBackchannelVar" short="Maximum correlation $&lt;macro v='dyadicCorrelation'/&gt;_{max}$ of sliding window &lt;b&gt;variance of facial shape&lt;/b&gt;, for various pairs of participants (including corresponding and non-corresponding features)." caption="Maximum correlation $&lt;macro v='dyadicCorrelation'/&gt;_{max}$ of sliding window &lt;b&gt;variance of facial shape&lt;/b&gt;, for various pairs of participants (including corresponding and non-corresponding features). Pairs that were participating in the same conversation are highlighted.">
<tr><td></td><td> 1008 </td><td> 1011 </td><td> 2008 </td><td> 2011 </td><td> 3008 </td><td> 3011 </td><td> 6008 </td><td> 6011 </td></tr>
<tr><td>1008 </td><td highlight="yes">1.00</td><td highlight="yes">0.32</td><td> 0.13</td><td> 0.13</td><td> 0.29</td><td> 0.38</td><td> 0.16</td><td> 0.27</td></tr>
<tr><td>1011 </td><td highlight="yes"></td><td highlight="yes">1.00</td><td> 0.33</td><td> 0.21</td><td> 0.19</td><td> 0.16</td><td> 0.21</td><td> 0.18</td></tr>
<tr><td>2008 </td><td> </td><td> </td><td highlight="yes">1.00</td><td highlight="yes">0.34</td><td> 0.11</td><td> 0.23</td><td>0.16 </td><td>0.21 </td></tr>
<tr><td>2011 </td><td> </td><td> </td><td highlight="yes"></td><td highlight="yes">1.00 </td><td> 0.12</td><td> 0.14</td><td> 0.26</td><td> 0.29</td></tr>
<tr><td>3008 </td><td> </td><td> </td><td> </td><td> </td><td highlight="yes">1.00</td><td highlight="yes">0.41</td><td> 0.47</td><td> 0.14</td></tr>
<tr><td>3011 </td><td> </td><td> </td><td> </td><td> </td><td highlight="yes"></td><td highlight="yes">1.00</td><td> 0.27</td><td> 0.25</td></tr>
<tr><td>6008 </td><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td><td highlight="yes">1.00</td><td highlight="yes">0.15</td></tr>
<tr><td>6011 </td><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td><td highlight="yes"></td><td highlight="yes">1.00</td></tr>
</table>

<p>The previous sections have examined coupling of face shape in natural, dyadic conversation. However, it may be that facial motion contains reliable patterns of human behaviour. For instance, head pitch activity might encode nodding in agreement and this may be related to mouth activity caused by talking. The variance of each feature component in a sliding window was calculated. The window was selected to be 1 second in duration (25 frames), because this is sensitive to <ac>NVC</ac> signals of a relatively short duration. However, different <ac>NVC</ac> signals may occur on other time scales which would not be detected. At sliding window position $<macro v='timeOffset'/>$, the variance $<macro v='slidingWindowVar'/>$ of a feature component in a sliding window of 25 frames duration is:</p>

eqn\begin{gather}
eqn\label{EqnVarianceSlidingWindow}
eqn\slidingWindowVar^A_{\timeOffset,i} = \frac{1}{25} \displaystyle\sum\limits^{12}_{a=-12} {\frameFeatureMatrix^A_{\timeOffset+a,i}}^2 - \left( \frac{1}{25}\displaystyle\sum\limits^{12}_{a=-12} {\frameFeatureMatrix^A_{\timeOffset+a,i}} \right)^2\\
eqn\dyadicCorrelation^{A,B}_{i,j} = \correlFunc(\slidingWindowVar^A_i, \slidingWindowVar^B_j)
eqn\end{gather}

<p>The results of this method are shown in Figure <ref label='TableBackchannelVar'/>. In this case, the correlations caused by coincidence (shown in the off axis pairings) are often higher than the correlations that might be expected to have coupling. The variance of correlations from null hypothesis pairings has a standard deviation of $0.034$. For subject pairs engaged in conversation, the maximum correlations were 0.32, 0.34, 0.41 and 0.15. Based on the z-test, the p-values for these results are $4.88<macro v='times'/>10^{-21}$, $1.52<macro v='times'/>10^{-23}$, $1.74<macro v='times'/>10^{-33}$ and $1.03<macro v='times'/>10^{-05}$ respectively. The first three pairings are well below the significance threshold $<macro v='alpha'/>_{adjusted} = #frac{0.01}{T_{1035-1}}=1.87<macro v='times'/>10^{-8}$ but the final pair is above the threshold and is more likely to be due to a null hypothesis. The highest correlation is for conversation 3008-6008 which has $<macro v='dyadicCorrelation'/>=0.47$ and is caused by coincidental variation of the features (p-value $1.84<macro v='times'/>10^{-43}$). This coincidental match is strangely below the significance threshold and may be due to the distribution of correlations being non-Gaussian. Since coupling cause by patterns in human behaviour cannot be distinguished from those cause by coincidence, more data is required to obtain results that can be confidently considered as statistically significant. An analysis of facial shape activity based on corresponding features resulted in a similar, non-significant result and has been omitted for brevity. This system was not compared to human performance for identifying interpersonal coordination because the resources required to collect such an annotation data set would be very resource intensive and this is beyond the scope of this thesis.
Facial shape in natural conversations exhibits interpersonal coordination. Given the relationship between people's behaviours, the next section uses backchannel information for automatic <ac>NVC</ac> recognition. However, the following section considers all feature components rather than the components identified in earlier in the chapter. This allows features to be used in the classifier model that do not necessarily have a linear coupling with the forward channel behaviour.</p>

</subsection>
</section>
<section title="Classification Based on Backchannel Information" label="SectionClassificationBackchannel">

<p>The previous chapter has investigated automatic methods to classify <ac>NVC</ac> based on visual facial information. However, the corpus has two subjects interacting. The annotated subject is designated as the “sender” and the other conversation participant as the “receiver”. 
The sender expresses in the forward channel and perceives the backchannel. The receiver expresses in the backchannel and perceives the forward channel. 
The previous section has discussed coupling of facial behaviour in natural conversation. This raises the possibility that a sender's <ac>NVC</ac> signals may be inferred based on the receiver's visual information. Figure <ref label='BackChannelFigure'/> shows the experimental arrangement to detect <ac>NVC</ac> using backchannel signals. The videos of <ac>NVC</ac> receiver participants are tracked and features extracted using <i>geometric-a</i> features, as described in Section <ref label='SectionGenerateAlgorithmic'/>. A classifier is trained on the sender <ac>NVC</ac> labels and the receiver facial feature data. Testing is performed on a person independent basis, with only clear <ac>NVC</ac> clips used in the test $<macro v='clearClipSet'/>^{clear}_{<macro v='nvcCategory'/>}$ (see Section <ref label='SectionClearExamples'/>) based on annotation ratings of the <ac>NVC</ac> sender. %Test features are used to train an <ac>SVM</ac> classifier.</p>

<figure label="BackChannelFigure" caption="Automatic &lt;ac&gt;NVC&lt;/ac&gt; classification can operate on either the forward or backchannel information.">
<graphic width="0.4">backchannel/figureforwardbackchannel.pdf</graphic>
</figure>

<p>The experimental arrangement is almost identical to that presented in the previous chapter. The target classes and annotation data is described in Chapter <ref label='ChapterCorpus'/>. <macro v='FeatureGeneration'/> is performed as discussed in Section <ref label='SectionFeatureGeneration'/>. Classification is performed as described in Section <ref label='SectionClassificationMethods'/>. Eight fold cross validation is used on clear examples of <ac>NVC</ac> (Section <ref label='SectionClearExamples'/>) and performance is evaluated using <ac>AUC</ac>, as described in Section <ref label='SectionClassificationPerformance'/>. The only difference is the shape features used for training and testing are taken from the other person in the dyad (i.e. the person conversing with the subject that was shown to the annotators).</p>

<subsection title="Results and Discussion">

<p>The performance of automatic classification based on backchannel <ac>NVC</ac> signals is shown in Table <ref label='TableBackchannel'/>. If the performance for forward channel <ac>NVC</ac> classification (Table <ref label='TableCompareFeaturesAndClassifiers'/>) is compared to backchannel performance, it can be seen that the backchannel performance is equal or lower in every case. This is unsurprising, because backchannel information is only a behavioural response to forward channel communication. The results suggest that, for some methods of <macro v='featureGenerationComma'/> the classifier is consistently performing at above chance level of 0.5. However, additional tests are required to establish if this result is statistically significant. If the performance for each of the <macro v='featureGeneration'/> methods is compared, it can be seen that <i>geometric-a</i> features are again the most effective. The majority of other feature sets have performance levels at or near chance level. Person independent testing is more challenging than multi-person testing in all cases. 
These observations have the same general pattern as the forward channel performance results.</p>

<table label="TableBackchannel" caption="&lt;ac&gt;AUC&lt;/ac&gt; &lt;ac&gt;ROC&lt;/ac&gt; performance using backchannel features (clip level testing, person independent testing, average score of categories and average standard deviation from Tables &lt;ref label='BoostClipMultipersonReceiverTable'/&gt; to &lt;ref label='SvmClipPersonindepReceiverTable'/&gt; are shown)."> <!--XIV, XII, XV, XIII-->
<tr><td>Test </td><td> Multi-person </td><td></td><td> Person independent </td><td></td></tr>
<tr><td></td><td> SVM </td><td> Adaboost </td><td> SVM </td><td> Adaboost </td></tr>
<tr><td>affine                           </td><td> 0.50 $<macro v='pm'/>$ 0.04 </td><td> 0.53 $<macro v='pm'/>$ 0.05 </td><td> 0.49 $<macro v='pm'/>$ 0.05 </td><td> 0.50 $<macro v='pm'/>$ 0.04 </td></tr>
<tr><td>deform-cubica                    </td><td> 0.51 $<macro v='pm'/>$ 0.01 </td><td> 0.52 $<macro v='pm'/>$ 0.05 </td><td> 0.50 $<macro v='pm'/>$ 0.00 </td><td> 0.49 $<macro v='pm'/>$ 0.06 </td></tr>
<tr><td>deform-fastica                   </td><td> 0.51 $<macro v='pm'/>$ 0.01 </td><td> 0.52 $<macro v='pm'/>$ 0.05 </td><td> 0.50 $<macro v='pm'/>$ 0.00 </td><td> 0.49 $<macro v='pm'/>$ 0.06 </td></tr>
<tr><td>deform-pca                       </td><td> 0.54 $<macro v='pm'/>$ 0.04 </td><td> 0.62 $<macro v='pm'/>$ 0.07 </td><td> 0.50 $<macro v='pm'/>$ 0.00 </td><td> 0.52 $<macro v='pm'/>$ 0.04 </td></tr>
<tr><td>geometric-h                      </td><td> 0.65 $<macro v='pm'/>$ 0.05 </td><td> 0.66 $<macro v='pm'/>$ 0.07 </td><td> 0.54 $<macro v='pm'/>$ 0.04 </td><td> 0.59 $<macro v='pm'/>$ 0.07 </td></tr>
<tr highlight="yes"><td>geometric-a </td><td> 0.67 $<macro v='pm'/>$ 0.03 </td><td> 0.68 $<macro v='pm'/>$ 0.09 </td><td> 0.59 $<macro v='pm'/>$ 0.08 </td><td> 0.59 $<macro v='pm'/>$ 0.09 </td></tr>
<tr><td>lbp                              </td><td> 0.54 $<macro v='pm'/>$ 0.05 </td><td> 0.58 $<macro v='pm'/>$ 0.05 </td><td> 0.49 $<macro v='pm'/>$ 0.07 </td><td> 0.48 $<macro v='pm'/>$ 0.10 </td></tr>
<tr><td>lm                               </td><td> 0.55 $<macro v='pm'/>$ 0.03 </td><td> 0.56 $<macro v='pm'/>$ 0.06 </td><td> 0.48 $<macro v='pm'/>$ 0.02 </td><td> 0.50 $<macro v='pm'/>$ 0.05 </td></tr>
</table>

<p>One difference between backchannel and forward channel is that <ac>SVM</ac> classification has equal to or worse performance when compared to Adaboost. This may be due to over fitting of the SVM, which might be addressed by parameter tuning.</p>

</subsection>
</section>
<section title="Conclusion">

<p>This chapter has identified coupled behaviour in facial expression and described a study of backchannel features to classify forward channel <ac>NVC</ac>. This was based on previous research that found many human behaviours are inter-personally coordinated in two person conversations. Coupled facial deformations were identified for both corresponding regions of the face, as well as non-corresponding regions. The corresponding areas are situated in the mouth area or were thought to relate to head pitch. Classification was also possible using backchannel features, although the performance is significantly lower if compared to forward channel features. </p>

<p>This area of research is still at an early stage. There are many potential improvements and extensions to the work presented in this chapter. For instance, coupled behaviours were only examined in conversation pairs, which may lead to personal differences in behavioural patterns. It would be possible to modify the method to look for behavioural patterns that consistently occur across a larger group of conversation participants. Also, only instantaneous face deformations were considered. If an effective way of encoding face deformation or gesture was found, it may be possible to check for responses to stimuli that occur after a time delay, in a similar fashion to Richardson <macro v='etal'/> <cite ref='Richardson2007'/>. Pearson's correlation coefficient is only sensitive to linear correlations between variables, but it is quite possible that non-linear behavioural patters may exist. Only facial deformation is considered in the features and it would be interesting to expand the behavioural encoding to verbal communication, as well as arm position and body pose. This would enable a much broader range of social phenomena to be studied.</p>

<p>There are also possible extensions to classification based on backchannel features. It should be possible to combine backchannel and forward channel features, either by feature or decision fusion, to improve performance. Also, many of the limitations of the previous chapter apply here: <ac>NVC</ac> is not temporally modelled, and the social context and cultural background of the participants and annotators could be better controlled. To address the cultural differences of the annotators, the next section describes the collection of a <macro v='culturallySpecific'/> annotation set for the TwoTalk corpus.</p>

</section>
</chapter>

